**Run Experiments to Test Your Plan**

**Recap and Introduction to Systematic Testing**

**00:00**
Hi, this is Ash with Spark59. As a quick recap, by now you should have documented your initial Plan A and identified a handful of top risk items to tackle.

**00:09**
This brings us to the final and most important meta-principle: using experiments to systematically test your plan.

**00:17**
Running experiments is the key activity in a lean startup, but running effective experiments takes discipline and is actually quite hard to do. In this video, I’m going to walk you through some ground rules for doing just that.

**Defining the Startup Experiment**

**00:30**
But first, let’s start with the definition. If you are already familiar with the Lean Startup methodology, you have no doubt seen this picture before. This is the Build-Measure-Learn loop Eric Ries codified about three years ago. A cycle around this loop is what we'd call an experiment.

**00:46**
It starts at the very top with a set of ideas or hypotheses. You then turn these hypotheses into some aspect of a product. This doesn't have to be the final product, but some appropriate proxy of it, which you put in front of customers for the purpose of measuring their reaction, either through qualitative and/or quantitative means. 

**01:06**
The data you collect here leads to learning, which then fuels the next set of ideas or experiments.

**01:12**
As an example, I might have a set of hypotheses that my early adopters will be internet marketers and that they would pay $100 a month for my product.

**01:22**
I might decide instead of building out the full product, which would take too long, to instead put up a demo or screenshot of my product on a landing page and then go show it to these internet marketers.

**01:33**
At the end of the demo, I would present my $100 a month pricing model and measure how many of them convert to the next stage.

**01:41**
Based on the reactions to my offer, I might have to refine my hypotheses and start a new experiment, or if my hypotheses were validated, I would move on to testing other high-risk hypotheses in follow-up experiments.

**The Three Critical Attributes of an Experiment**

**01:56**
When designing an experiment, there are three attributes that are critical.

**02:01**
The first is speed, as measured by the cycle time around this loop. You'll remember from earlier that the real challenge in building a successful product is going from your initial Plan A to a plan that works before running out of resources.

**02:15**
To do this, you have to learn as quickly as possible. It is important to obsessively reduce the scope of what you have to build without compromising on the hypothesis you’re trying to test and then getting it out in front of customers as quickly as possible.

**02:31**
Every experiment needs to end in customer learning. An experiment is only done after you’ve measured your customer's reaction and reached some actionable learning or decision point.

**02:41**
And something I talk a lot about is focus. At any given point in time, there are dozens of things that could possibly be tested, but only a handful of them will have that 10x impact on your progress. You need to only focus on those key actions or top risks and ignore the rest.

**Consequences of Lacking Key Attributes**

**03:00**
Here's a visual way of illustrating what happens when you don't have all three of these attributes.

**03:06**
If you're going fast and focused, but not learning from customers, the image of a dog chasing its tail comes to mind. You're probably executing on some plan, but remember that most initial plans don't work by themselves. Without the customer feedback loop, you risk building too much or going astray and building the wrong product.

**03:27**
If you're focused on the right things and learning from customers but are going too slow, you stand the risk of running out of resources or being outpaced by a fast follower.

**03:37**
And finally, if you're going fast and learning, but not focused on the right things, this is one of the more common traps of premature optimization. 

**03:46**
This is where as a technical founder, you might obsess over scalability and endlessly tune your code or servers instead of launching. The reality is that you have zero customers today and will probably have zero or very few customers when you actually launch. 

**04:01**
If your customers do end up crashing your servers, that is actually a great problem to have because it shows demand for your product. 

**04:08**
You can almost always come back and solve these, at first using more hardware and then following up with additional tuning and optimizations. Twitter, YouTube, Facebook all went through this and didn't architect for massive scale from day one.

**04:24**
I don't want to just pick on the technical folks. If you are a marketer, you might fall into the trap of endlessly optimizing your landing pages. 

**04:31**
At some point, you reach a point of diminishing returns. Squeezing an additional half a percent in conversion on a site that only gets 100 visitors a day might not be the best use of your effort at this stage of your product cycle.

**04:45**
So the key point I want to drive home is that in order to build an optimal learning loop, you need to design your experiments to maximize on all three things: speed, learning, and focus.

**Ground Rules for Effective Experiments**

**04:55**
Next, I’m going to cover some ground rules for running effective experiments.

**04:59**
First, there’s a natural tension on the one hand between keeping your experiment small and fast and on the other hand expecting them to uncover big breakthrough insights. 

**05:10**
You’ll probably never run a single experiment that completely de-risks your entire business model. Experiments shouldn’t be treated as standalone monolithic projects but should rather be strung together so the learning is additive.

**05:23**
The three stages here help illustrate this additive process. When you first get hit by an idea, you start by validating if you have a problem worth solving. 

**05:32**
You then move into the solution value testing phase and then finally into growth or scale. Each of these stages are additive and build on learning from the previous stages.

**05:43**
Next, the expected outcomes of your experiments need to be declared upfront. 

**05:49**
This is very important because if you simply plan on seeing what happens, you will always succeed because something will happen. 

**05:56**
It’s then very easy to rationalize whatever happens, good or bad, to fit into your mental model or reality distortion field and convince yourself that you are on the right track.

**Declaring Falsifiable Hypotheses**

**06:07**
Simply stating your expected outcome, however, is not enough. You have to state it as a falsifiable hypothesis, which is a concept taken right out of the scientific method. A falsifiable hypothesis is a statement that can be proven false.

**06:21**
This is a necessary condition when setting up an experiment. Otherwise, it's too easy to fall into the inductivist trap and always convince yourself you're on the right track.

**06:31**
Let me illustrate this with an example. Consider the statement above: "Being known as an expert will drive early adopters." 

**06:36**
Here I believe that because I am an expert, I'll be able to drive early adopters to my product. I might do a number of things to test this belief, such as write a blog post, speak, or send out a Twitter update announcing my product. 

**06:51**
But at what point is this hypothesis validated? Is it when I get 10 people to sign up, 100 people, or 1,000 people? Because I don't explicitly state this, I can declare success at any point provided I get at least one sign up.

**07:06**
Here is the same belief turned into a more specific and testable falsifiable hypothesis. 

**07:12**
This time I'm going to take a specific action, which in this case is writing a blog post, and as a result of that action, I expect to see at least 100 signups. This time I clearly know how to both measure and declare success.

**07:26**
Here’s a simple formula I use for crafting falsifiable hypotheses:
*   Specific Repeatable Action (will lead to) Expected Measurable Action.
*   Example: Blog post will drive > 100 early signups.

**The Fear of Being Wrong and Lack of Information**

**07:39**
As simple as this step sounds, I often find entrepreneurs reluctant to make such predictions. There are generally two reasons for this.

**07:46**
First, they hate to be proven wrong. This one is inherent in our nature. We often have personal egos attached to our products. 

**07:54**
I know it's hard to offer an easy solution here. All I can say is that running lean is fundamentally about being less wasteful. The irony is that you can't really appreciate waste unless you have been somewhat wasteful before. 

**08:07**
At some point in the entrepreneur life cycle, you start to detach your personal ego from your product and realize that there's a bigger goal that is outside your product preconceptions. You start to realize that it's more effective to be empirical and objective than to run on blind faith alone.

**08:23**
The second reason entrepreneurs are uncomfortable making these kinds of predictions is that they feel they don't have the right information to make them. 

**08:31**
Welcome to the world of building products under conditions of extreme uncertainty. Yes, when you first start out, your judgment is going to be all off. You're going to repeatedly make overly optimistic predictions and be completely wrong, and that's okay. 

**08:46**
The good news is that over time, you'll begin to understand your business better and your judgment will improve.

**Time-Boxing and Scarcity**

**08:52**
But what happens when you don't yet have enough data to clearly decide either way? Say in my last case with the blog post, what do I do if after the first week I get exactly 60 signups? 

**09:04**
The natural tendency is to want to wait just another week longer. Pretty soon we start rationalizing extending the experiment even longer, hoping that the data will make a turn for the better.

**09:15**
Pretty soon weeks can turn into months, and this is a pretty slippery slope. Remember that in a startup, time is the scarcest resource. Other resources like money and people can fluctuate up and down, but time only moves in one direction.

**09:30**
To avoid this trap of long-running experiments, you need to clearly timebox your experiment alongside the falsifiable hypothesis. 

**09:39**
If I had put a timebox of two weeks on my blog post experiment, then after two weeks I would review the data and clearly declare whether the hypothesis was validated or invalidated. 

**09:50**
Again, the timebox may not be perfect and you may actually need to add more time. That again is okay. The important thing is that after two weeks, you pause to reflect on the status of the experiment, and if you decide that more time is needed, you make that decision within reason versus wishful thinking.

**Finding Breakthroughs in Failure**

**10:08**
Which brings us to the last point on experiments, which is a big one. 

**10:12**
When experiments fail, many entrepreneurs run away from that failure and are quick to change direction drastically, justifying it as a pivot. Unfortunately, the word pivot has become an overly abused lean startup term.

**10:24**
A pivot represents a change in strategy and it needs to be grounded in learning. Otherwise, it's just a disguised "see what sticks" strategy, which is not the most optimal way for finding a plan that works.

**10:37**
There's also a reason that the hockey stick curve is largely flat at the beginning. It's not because the founders were too dumb or not working hard enough, but before you can find a business model that works, you have to go through a lot of stuff that doesn't.

**10:51**
The answer isn't running away from failed experiments, but rather digging deeper. When an experiment fails because either you had an unexpected outcome or hit your timebox deadline, taking the time to uncover the root cause for this failure is where you'll find your breakthrough insights.

**Putting It All Together: The "Running Lean" Case Study**

**11:09**
So that was a high-level overview of the running lean process. Let's put it all together now.

**11:15**
You start with your best guess of an initial business model or your Plan A. You then identify what's riskiest on the plan through conversations with your team and external advisors. 

**11:26**
Finally, you craft a set of small fast experiments to test those risky assumptions using the principles we outlined today. 

**11:34**
After each experiment, there is a learning feedback loop that helps you update your risks and your business model. And then the cycle repeats itself.

**11:44**
Before we leave, I'd like to share a concrete case study that will help you solidify a number of the principles and tactics we have covered in this series so far. 

**11:53**
I usually speak to a diverse audience of entrepreneurs and can't always rely on my high-tech or software/hardware case studies, so I often use a product that is much simpler to understand and doesn't require any prior context to understand it.

**12:07**
I'm going to walk you through a case study on how I wrote my book. Yes, I was testing and refining a lot of these principles while I was writing my book and applied every one of these techniques to the writing process.

**12:19**
Even though you may not have written a book, you can probably appreciate that writing a book is a big project. In fact, you'll see shortly it's very similar to building any other type of product.

**Iterating the Book Creation Process**

**12:31**
Going back in time, I was busy running my previous company, WiredReach. Along the way, I realized that I was averaging about two years between my products, which was way too long. 

**12:42**
I was also constantly getting hit with new ideas to build, so I started a search for better faster ways for vetting these ideas. 

**12:50**
In 2009, I ran into the early works of Steve Blank and Eric Ries, and a lot of what they were saying resonated with me, and so I decided to test them out in my next product.

**13:00**
But as I started applying these principles, I ended up with more questions than answers and decided I would turn this product into a public testbed and openly blog about it. 

**13:10**
Week after week I shared my learning and slowly my readership grew. Then one day I got an email asking if I'd consider turning my blog posts into a book.

**13:18**
At first, I was flattered but politely refused because I was just too busy running my company. I'd never considered writing a book before and didn't have the time anyway. 

**13:28**
But after the 12th request or so, I decided to explore a little further.

**Understanding Reader Needs Through Problem Interviews**

**13:32**
I did this by setting up problem interviews with these readers to understand who they were. I assumed they were entrepreneurs, but I needed to be sure. 

**13:40**
More importantly, I wanted to know why they wanted me to write this book. There was no shortage of content available online. I wanted to understand what top problem they were expecting the book to solve for them.

**13:52**
So through these interviews, I learned that most of them were not just entrepreneurs, but were also technical founders like me. They too had been struggling with taking lean principles to practice and found my writing to be an actionable how-to guide.

**Testing the Value with a Teaser and Solution Interviews**

**14:05**
Armed with this knowledge, I decided to spend a half day creating a demo for the book, which I did in the form of a teaser page that looked something like this.

**14:14**
You'll notice this is not the most aesthetic page, but it's not bad for a half day's work. I quickly picked a placeholder title and a stock image for the book. 

**14:23**
I didn't bother getting these things right at this stage because they weren't what was riskiest. While the title and book cover are important once the book is in the bookstore, I had more risky things to tackle first, such as the table of contents you see down below.

**14:38**
If this book wasn't going to be any good, the best title or book cover in the world wasn't going to save it.

**14:44**
So I set up another round of solution interviews and this time I asked if people would be willing to buy the book if it had this table of contents. 

**14:52**
Most business books are priced in the $20 to $30 range, so I didn't even bother putting that on the page. Again, the riskier thing here was getting people to want to buy the book, not optimizing for the exact price point. 

**15:04**
These conversations were immensely helpful in refining the content of the book, and after a few updates, I got all 12 people to say they would buy the book.

**Validation Through Workshops and Pre-Orders**

**15:13**
While this was good small-scale validation, selling the book to just 12 people wouldn't justify the time and effort that would go into writing this book, and so I turned the teaser page into a much larger smoke test.

**15:26**
I made three changes here. First, I changed the title. During my problem interviews, a number of people thought the title "Getting Lean" was not active enough since lean is about doing. I suggested "Running Lean" and that stuck. 

**15:39**
Second, I added a "coming this summer" launch date. This was back in March of 2010, so that gave me a two to five month window depending on how far you stretch the summer.

**15:50**
And finally, I added a form to collect email addresses from people interested in the book.

**15:56**
I'd estimated the book writing process taking two to three months since it was largely a repackaging of my blog content. 

**16:03**
Given the $20 to $30 price point for the book, I did a simple back of the envelope calculation and decided that I needed to collect at least 1,000 email addresses before I would take the book project seriously.

**16:16**
So I set up this page, announced it on my blog and Twitter. A few people like Eric Ries helped spread the word, and then I left the page alone and went back to running my company.

**Iterative Development and Small Batches**

**16:26**
By early summer, I had reached my 1,000 email goal, and that was when I decided to take the next step. I tried writing the first chapter, but as I looked back at the table of contents, I realized that a lot of my original thinking had changed. 

**16:39**
While the table of contents was mostly okay, I realized that writing this book would no longer be a cut and paste job from my blog. It would take too long and I needed another way to write this book.

**16:51**
The answer came in the form of a free workshop. I spent a day taking my table of contents and turning it into a slide deck. It was much the same content you're seeing right now, but just not as pretty. It was mostly text and a lot of bullet points.

**17:05**
I announced a free Lean Startup workshop in Austin, Texas, where I live, and got 30 people interested. 

**17:11**
The reason I kept this workshop free is I had no idea what people would pay for something like this. Also, a good test for value is trying to give away something for free. If people don't take it at $0, they sure in hell will not pay for it.

**17:25**
I also decided not to run the workshop with all 30 people at once, but instead broke them up into three groups of ten. 

**17:32**
My thinking here was that if the first workshop was outright terrible, I'd be able to fix any problems with the other groups instead of burning up my list all in one go. This is the lean principle of testing in small batches.

**17:45**
Interestingly enough, the first workshop went real well. People liked the content so much and many said they would have paid for it, and so I conducted some impromptu price testing on the spot and ran two other workshops and a number of them after that as paid workshops.

**Leveraging Early Traction for Success**

**18:03**
Getting people to pay for your product is the first form of validation. I ran these paid workshops all through the summer and kept refining the content, not through writing a manuscript but through refining the slide deck. 

**18:16**
By the end of the summer, I had fully mapped the entire book and was ready to actually start writing it.

**18:23**
But there was a problem. I'd originally promised my book as a summer launch and summer had come and gone. People were getting artsy and started inquiring about the status of the book. 

**18:32**
I wrote back to them and told them that I had grossly underestimated the effort it would take to write this book, but now that I was ready to start writing it, I was going to make them an offer.

**18:43**
I gave them a preview chapter of the book and told them that if they pre-ordered the book right now, I'd deliver two chapters of the book to them every two weeks. 

**18:52**
I would essentially write the book much like we write software. This delivery schedule worked for my book because it was a non-fiction book and it had lots of explicit actionable content like exercises and interview scripts that people could put to use in their startups. 

**19:08**
So while people were busy testing the content from previous chapters in their own startups, I'd be able to stay ahead with the next update.

**19:15**
I went back to the book page and changed the "coming this summer" to "coming soon" because I didn't really have an exact end date this time around. I also added a pre-order option and offered a discount. 

**19:28**
I have since then changed my position on this. I went back and asked people who pre-ordered after the fact if the $25 discount mattered to them, and they told me that it didn't.

**19:39**
The other interesting thing that happened is only about half the people went for this pre-order option. I asked the others why they didn't, and they cited packaging as the biggest reason. 

**19:49**
Many of them did not want to read the book as a PDF and chose to wait for a print or Kindle version. My first inclination was that I can find a way to support them, but then I realized that these people were not really my early adopters. 

**20:02**
An early adopter is someone who wants your product so badly that they're willing to tolerate a less than perfect version and jump through a few hoops if needed. Once again, creating a Kindle or print version of the book wasn't the riskiest thing, but testing the content of the book. 

**20:18**
I chose not to create the extra work for myself and simply treated the people who didn't pre-order as latter-stage customers.

**Achieving the Scale Stage**

**20:25**
These two-week releases were incredibly helpful. I got great feedback every two weeks that ranged from basic typo fixes to content and flow suggestions. 

**20:35**
I can confidently say that because of this continuous feedback loop with customers, I was able to write a much better book, which required very little final editing because it was all being done incrementally along the way. This, by the way, is another example of the principle of small batches.

**20:52**
Then an interesting thing happened. When I was about two-thirds of the way through the book, I got a random email from a major publisher. They had read my partial book and had a book deal on the table.

**21:04**
I got on the phone and my first question was "Why do you want to publish this book?" I told them my book wasn't DRM protected and there were several copies freely floating around on the internet. In fact, I'd even helped in distributing some of those copies myself by placing the book in communities like Hacker News.

**21:21**
They told me that none of that mattered to them. What they saw was a book that had been sufficiently de-risked from their vantage point. 

**21:28**
Because I had managed to sell over 1,000 copies on my own, they felt that with their distribution platform, they would be able to 10x that number, and that made this a compelling book for them. When they put it that way, it all made sense. 

**21:41**
I had demonstrated early traction, which matters most to external stakeholders like investors and in this case to a publisher. 

**21:49**
If I had tried to have this conversation at the beginning of this process back in early 2010, I would have had no leverage and this conversation would have probably gone a very different way.

**21:59**
I told the publisher my commitment was to get the first edition published and after that, I'd entertain the idea of publishing a print version. I went ahead and did finish the book in February of 2011.

**Scaling and Closing**

**22:12**
At that point, I went back to the book page and revamped it to look a lot more appealing as a marketing site. I got a designer to create a book cover, I got a number of prominent people like David Skok, who were early adopters also, to write me a testimonial. 

**22:27**
At this stage, the book project shifted from value creation to scaling. I started testing a number of tactics to promote and sell the book and eventually sold over 15,000 copies of this first edition.

**22:38**
To complete the story, using the software analogy, large software is never finished, but just released. I felt the same way about this book. I felt that I had a lot more to say once the book was published and so I kept on writing on my blog, I created a newsletter, and I kept teaching workshops.

**23:01**
The workshops were only intended as an MVP to the book, but after writing the book, demand for these workshops actually went up and I found myself running these paid workshops all over the world. 

**23:09**
It is through these workshops that I started testing more advanced concepts and launched a couple of additional products. At this point, I was still running my old company, which I decided to sell and start Spark59 around the mission of using tools, content, and coaching to help entrepreneurs succeed.

**23:29**
And yes, I did finally go back and publish a second edition of the book, which looks like this. Hopefully, this case study helps solidify the key principles we have covered throughout the series and sparks you to take that next step with your product.

**23:43**
If you'd like to get more content like this, consider subscribing to my free Running Lean Mastery newsletter. You can find it on the Spark59 homepage. Until next time, take care. Thanks.

***

### **Organized Notes**

**Core Principles of Systematic Testing**
*   **[00:17]** Running experiments is the key activity in a lean startup but requires extreme discipline.
*   **[00:30]** Definition: One cycle through the **Build-Measure-Learn Loop** equals one experiment.
*   **[01:56]** Three Critical Attributes:
    1.  **Speed**: Cycle time around the loop. Focus on reducing scope to learn faster.
    2.  **Learning**: Experiments must result in customer insights or actionable decision points.
    3.  **Focus**: Target the handful of actions that provide 10x impact.

**Traps of Improper Experimentation**
*   **[03:06] Chasing Your Tail**: Fast and Focused but NO Learning. Result: Building the wrong product.
*   **[03:27] Running Out of Resources**: Focused and Learning but NO Speed. Result: Outpaced by followers.
*   **[03:37] Premature Optimization**: Fast and Learning but NO Focus. 
    *   **[03:46]** Technical Trap: Scaling code for zero customers.
    *   **[04:24]** Marketing Trap: Endlessly A/B testing landing pages with low traffic.

**Ground Rules for Experiments**
*   **[04:59] Experiments are Additive**: A single experiment won't de-risk a whole business. Success comes from stringing them together (Problem fit -> Solution fit -> Scale).
*   **[05:43] Declare Outcomes Upfront**: Prevents rationalizing results to fit existing mental models.
*   **[06:07] Use Falsifiable Hypotheses**: Statements must be capable of being proven false to be valid.
    *   **[07:26] Formula**: [Specific Repeatable Action] will lead to [Expected Measurable Action].
*   **[09:30] Time-Boxing**: Prevents experiments from running indefinitely without review. Time is an entrepreneur's scarcest resource.
*   **[10:12] Pivot Grounded in Learning**: A pivot is a change in strategy based on insights, not just a random change in direction.

**The Book Creation Case Study**
*   **[13:32] Problem Interviews**: Validated the target audience (technical founders) and their primary struggle (applying lean theory to practice).
*   **[14:05] Teaser Page (Smoke Test)**: A simple landing page used to test interest without optimizing aesthetics (Title/Cover).
*   **[14:44] Solution Interviews**: Tested the table of contents to see if it solved the problem before writing the book.
*   **[16:26] Small Batch Content Testing**: Used free workshops to refine content through direct interaction instead of just writing.
*   **[17:55] Price Testing**: Moved to paid workshops to validate willingness to pay.
*   **[18:43] Iterative Release**: Delivered the book in chapters every two weeks to get continuous feedback and reduce final editing effort.
*   **[21:21] Traction as Leverage**: Pre-selling 1,000 copies provided the leverage needed to negotiate a favorable deal with a major publisher.