**Innovation Accounting: A Spark59 Case Study**

**Introduction to Innovation Accounting**

**00:00** 
Hi, this is Ash with Spark59. If you’ve been following Lean Startup for a while, you’ve probably heard of actionable metrics, cohorts, and even innovation accounting.

**00:10** 
Today I’m going to provide a high-level overview of what innovation accounting means to us and how we put it to practice at Spark59.

**00:18** 
This talk will be organized around three topics, and I’ll be using a case study from one of our products, Lean Canvas, to illustrate the underlying principles:
1. Measuring Progress
2. Running Experiments
3. Communicating Progress

**00:28** 
Just for context, Lean Canvas is an online tool that helps entrepreneurs capture and communicate their business model on a single page or canvas.

**Measuring Progress in Startups**

**00:39** 
So let’s get started with how we measure progress. When you first launch a product, lots of things can and do go wrong. The common tendency is to want to collect as much data as possible.

**00:50** 
But in today’s world, where we can measure almost anything, we often end up drowning in a sea of information. This is what we expect from our metrics dashboard, but this is what reality really looks like.

**01:04** 
First, we don't really need more numbers, but rather a standard measure of progress. Much like we have standard financial statements to measure established companies, we need a set of standard metrics to measure startups.

**01:19** 
At Spark59, we standardize our measure of progress around five macro metrics first described by Dave McClure as Pirate Metrics. At its core, it captures the complete customer lifecycle going from acquiring an unaware visitor all the way to creating passionate customers.

**01:36** 
While the goal is to get customers completely through the funnel, we focus our experiments around a single metric at a time. The initial focus is on delivering value, and there are two key metrics that particularly stand out: first, Activation and then Retention.

**01:54** 
At the earliest stages, you don't need lots of users to test your value metrics because the learning tends to be more qualitative. Being able to count our early customers on our fingertips is actually a gift. 

**02:02** 
Instead of using a fancy analytics tool, we visualize our customer lifecycle on a Trello board much like this.

**02:14** 
Once we demonstrate our basic value metrics, the focus then shifts toward growth. On the customer lifecycle, sustainable growth can come from one of these three key engines of growth:
1. Paid
2. Sticky
3. Viral 

**02:28** 
The learning now shifts to being more quantitative. 

**02:32** 
But as you add more customers, measuring progress actually becomes harder, not easier. This is because the product is constantly changing. We are continually adding more customers, more features, and running more marketing campaigns. All of this makes it harder to correlate cause from effect.

**The Role of Cohort Analysis**

**02:49** 
This is where cohorts help. The reason correlation gets harder is that the further down the funnel you go, the more interesting customer events like revenue get time-shifted. 

**03:01** 
You can see this here with the blue dots. The people who sign up today don't upgrade for several weeks or months into the future, which makes it hard to accurately track them over time.

**03:12** 
Cohorts essentially help you group your customers and keep the blue dots together. By being able to isolate the complete lifecycle as a cohort, you can more rigorously measure if you’re improving your product on a daily, weekly, and monthly basis.

**Running Experiments: Case Study**

**03:26** 
Next, I’m going to walk through an actual experiment we recently ran. This is a screenshot of our dashboard from earlier this year. Based on these numbers, we decided to focus on first improving our activation rate.

**03:41** 
In Lean Canvas, we define activation as a user completing at least six out of nine of the boxes on the canvas. 

**03:49** 
We started by mapping out our activation sub-funnel which, as you can see here, is a long multi-step process. All our initial experiments were based around optimizing this flow through usability improvements. 

**04:02** 
But no matter what we tried, we couldn’t make a significant dent on the activation rate.

**Understanding the "Why" Behind Metrics**

**04:09** 
The main reason for this is that while metrics can tell you what’s going wrong, they can’t tell you why. You have to go behind the numbers, which is what we did next. 

**04:19** 
Not only were we interested in seeing who successfully activated and how long they took, but also those that got stuck.

**04:28** 
We now had a way for asking people why they got stuck, but doing this every day can get tedious, so we automated the process. 

**04:37** 
We used an email autoresponder that looked something like this. This single email alone generated 20 to 30 email responses a day.

**04:48** 
These responses helped us identify the top three reasons people didn't activate:
1. Too busy.
2. Need more information.
3. Just kicking tires. 

**04:52** 
You'll notice that none of these were usability issues where all our previous efforts were directed.

**Experimentation and Qualitative Feedback**

**04:58** 
We decided to tackle the second item on this list by building a seven-day Lean Canvas course. We set it up as a split test so that half the people signing up saw the course and the other half didn't. 

**05:10** 
The course itself was just a set of seven short videos that I recorded over a few afternoons. 

**05:16** 
So what do you think the results were? We surely thought the course would make a dent on the activation rate, but after several weeks of testing, we measured no significant impact on activation. In fact, there was even a slight dip in the activation rate.

**05:30** 
Based on this data, the right thing would have been to kill this initiative. Except...

**05:35** 
We left a way for people to leave comments below the course videos, and we got a lot of comments, and they were largely positive. This qualitative feedback is what kept us from killing this experiment and forced us to revisit our third principle.

**Monitoring the Full Customer Lifecycle**

**05:49** 
The customer lifecycle is a complex system. While we still focus our work around a single key metric, we now always monitor the entire customer lifecycle even when running a split test. 

**06:02** 
This is what our modified experiments dashboard now looks like.

**06:11** 
Fast forwarding to today, instead of killing the seven-day course, we ended up building a much longer sequence that now runs several weeks. 

**06:20** 
Since starting this campaign, we have been able to verify significant increases in both retention and revenue.

**Communicating Progress**

**06:26** 
Let’s switch gears now and talk about how we communicate our learning from experiments. While running experiments is a key activity for Lean Startups, running them effectively requires a lot of discipline.

**06:39** 
One of the things we do is establish a daily, weekly, and monthly reporting cadence. Our daily standups are mainly for task-level discussions. 

**06:49** 
We get together weekly to review running experiments. This is also where we review and propose new experiments.

**06:57** 
And finally, we also get together monthly to measure our cumulative progress and learning against our desired success metrics. This is where we adjust strategy if needed. 

**07:07** 
The learning from these monthly meetings is also what we share with our external advisors.

**Learning from Failure and the Pivot**

**07:13** 
The final point I’d like to make is a big one. When experiments fail, many entrepreneurs are quick to change direction drastically, justifying it as a pivot, which unfortunately has become an overly abused term.

**07:25** 
A pivot represents a change in strategy and needs to be grounded in learning. Otherwise, it’s just a disguised "see what sticks" strategy.

**07:35** 
There’s a reason that the hockey stick curve is largely flat at the beginning. It’s not because the founders were too dumb or not working hard enough. 

**07:43** 
But before you can find a business model that works, you have to go through a lot of stuff that doesn't. 

**07:49** 
The answer isn't running away from failed experiments, but rather digging deeper. Taking the time to uncover the root cause of these failures is where you’ll find the real breakthrough insights.

**The Innovation Accounting Trinity**

**08:00** 
In summary, the key takeaway I want to drive home again is that metrics by themselves are not enough. Not more numbers, but actionable learning.

**08:07** 
We find our learning is most effective when we combine a number of tactics together into something we call the Innovation Accounting Trinity:
1. Cohort Metrics
2. Split Testing
3. Lifecycle Messaging

**08:16** 
It starts with us baselining our product using cohorts as the standard measure of progress. We then build a continuous feedback loop with customers using automated lifecycle messages. 

**08:26** 
These conversations both help us understand who our customers are and why they behave the way they do. 

**08:33** 
In essence, they help us formulate rapid hypotheses about our customers, which we then rigorously test using split test experiments.

**Conclusion**

**08:42** 
So that was a quick overview of how we put innovation accounting to practice at Spark59. Next time I’ll cover how to build a company-wide dashboard your whole company will not only understand, but they’ll want to use. Until then, take care. Thanks.

***

**Organized Notes**

**Core Principles of Innovation Accounting (00:10)**
*   **Definition:** Innovation accounting is the process of defining, measuring, and communicating progress in a startup environment.
*   **The Problem (00:50):** Startups often collect too much data and "drown" in information rather than finding actionable insights.
*   **Standardization (01:04):** Startups need standard metrics similar to financial statements for established companies.

**Measuring Progress via Pirate Metrics (01:19)**
*   **AARRR Model:** Acquisition, Activation, Retention, Revenue, Referral.
*   **Initial Focus (01:36):** Prioritize Activation (the first great experience) and Retention (coming back).
*   **Engines of Growth (02:14):** Once value is proven, focus shifts to Paid, Sticky, or Viral growth engines.
*   **The Power of Cohorts (02:49):** Cohorts allow a team to group users by their signup date to isolate cause and effect, especially when lower-funnel events (like revenue) occur much later than signups.

**The Activation Case Study: Lean Canvas (03:26)**
*   **Defining Activation:** For Lean Canvas, a user is "activated" if they fill 6 out of 9 boxes.
*   **Usability vs. Information (03:49):** Initial attempts to optimize the UI failed to improve metrics.
*   **The Feedback Loop (04:09):** Automated emails revealed that users weren't failing because of a bad UI, but because they were "too busy" or "needed more information."
*   **The "Failed" Experiment (05:16):** A 7-day video course split test showed a slight dip in quantitative metrics but received massive qualitative praise through comments.
*   **Result (06:11):** By monitoring the full lifecycle (including Retention and Revenue), the team realized the course actually improved long-term value, leading to a permanent campaign expansion.

**Reporting and Cadence (06:39)**
*   **Daily:** Task-level standups.
*   **Weekly:** Review existing experiments and propose new ones.
*   **Monthly:** Measure cumulative progress against success metrics and adjust strategy/share with advisors.

**The True Meaning of a Pivot (07:13)**
*   **Grounded Learning:** A pivot must be based on specific insights from experiments. 
*   **Avoid "See What Sticks":** Changing direction without understanding *why* previous efforts failed is not a pivot; it is a lack of strategy.
*   **The Hockey Stick (07:35):** The early "flat" part of the growth curve is dedicated to working through failures to find a viable model.

**The Innovation Accounting Trinity (08:07)**
The most effective learning occurs at the intersection of:
1.  **Cohort Metrics:** Providing a baseline for progress.
2.  **Lifecycle Messaging:** Creating a continuous feedback loop to understand "why" users behave a certain way.
3.  **Split Testing:** Rigorously testing new hypotheses derived from customer conversations.