
Network Theory
An Overview

Network Theory: An Overview
Joss Colchester 2016

Contents
• Networks Overview
   o Network Paradigm
   o Networks Analysis
• Graph theory
   o Graphs 
   o Connections
   o Centrality
• Network Structure
   o Network Topology
   o Connectivity
   o Diameter
   o Clustering
• Types Of Networks
   o Degree Distribution
   o Random & Decentralized Networks
   o Decentralized & Small World Networks
   o Centralized & Scale Free Networks
• Network Diffusion
   o Network Dynamics
   o Diffusion & Contagion 
   o Robustness

Preface

Network theory is one of the most exciting and dynamic areas of science today with new breakthroughs coming every few years as we piece together a whole new way of looking at the world, a true paradigm shift that is all about connectivity. The study of network theory is a highly interdisciplinary field, which has emerged as a major topic of interest in various disciplines ranging from physics and mathematics, to biology and computer science to almost all areas of social science. From the metabolic networks that fuel the cells in our body, to the social networks that shape our lives, networks are everywhere, we see them in the rise of the Internet, the flow of global air traffic and in the spread of financial crises, learning to model and design these networks is central to 21st Century science and engineering. 

This is an introductory book where we present topics in a non-mathematical and intuitive form that should not require any specific prior knowledge of science as the book is designed to be accessible to anyone with an interest in the subject. During the book, we will explore all the major topics in this area. 

Networks Overview: In this first section to the book, we are going to give an overview to network theory that will also work as an overview to the structure of the book and the content we will be covering. We talk about what we called the network paradigm that is the whole new perspective that network theory offers when we look at the world through the lenses of connectivity. 

Graph Theory: In this second section, we lay down the basics of our language for talking about graphs by giving an introduction to graph theory, we talk about a node’s degree of connectivity and different metrics for analyzing a nodes degree of centrality and significance within a network. 

Network Structure: In the third section, we explore the overall topology to a network by talking about connectivity, which is how connected the whole network is, diameter, density, and clustering all key factors in defining the overall structure to a network. 

Types Of Networks: In this section, we will be looking at different models to networks by starting out with a randomly generated network we will see how most network are in fact not random but have some distinct structure, here we will be talking about a number of different models such as centralized scale free networks and the small world phenomena. 

Network Diffusion: In the last section to the book, we touch upon how networks change overtime, in particular looking at the different parameter effecting the generation of a network, how something spreads or fails to spread across it and finally wrap-up by talking about network robustness and resilience. 

By the end of taking this book, I hope that you will have a solid grasp of the formal language of network theory, the standardized language used to model networks within a wide variety of domains. You should also have a solid conceptual background required to approach a more advanced book in the mathematical analysis of networks. 

Being an introductory book, it has been designed to be accessible to a broad group of people but will be of particular relevance to those in engineering, the natural and social sciences, mathematics or IT. 

The Network Paradigm 

In this first section, we are going to talk about what we might call the network paradigm, all models and theories are like windows onto the world, none of them are perfect they all enable us to see some things but also inhibit us from seeing others and more importantly they all rest upon some set of assumptions. This set of methods and assumptions that support a particular scientific domain is called a paradigm. So, before we start to get into the technical side of network theory, we want to make you aware of some of the key features to the network paradigm, these features will constitute major themes that we will be rediscovering and trying to highlight as we go through the book. 

Firstly, networks are all about connectivity within systems whose components are relatively isolated we can focus our interest on the individual components within the system, by analyzing their properties we can gain an understanding of how the whole system works. For example, say I have put together a financial portfolio of different assets, well if the risk on these different assets is not related in anyway, that is to say they are all from very different sectors of the economy, well I can calculate the overall risk of my portfolio by simply analyzing the properties of each asset and then summing them up to formulate a total value for the overall portfolio. 

But what if many of the assets in my portfolio are correlated? If I have acquired many investments within both say the food processing industry and agriculture well the risk on both is correlated, or assets in both logistic and retail which are again interconnected. Due to these correlations the value of the assets will move together thus the real risk return ratio of my portfolio is no longer defined by that of each asset in isolating but now by these correlations, that is to say what is connected to what and in what way are they connected comes to now define the whole system. 

The point to take away here is that we often spend a lot of time analyzing individual components and then assume that the whole system is simply an additive function of these parts, but when we turn up the connectivity within a system it is increasingly the relations between components that come to define the overall system and this is where networks theory finds its relevance as it is all about connectivity. 

To go back to our finance example, hedge fund managers and other financial institutions do not go out of business because they fail to analyze the characteristics of their assets in isolation, they go bank corrupt and we get financial crisis because we don’t see the networks of connections behind these assets. To see the world through the network paradigm we need to see not so much things but the connections between them. 

This leads us to our second point: this world of connectivity has a very different kind of space to the one we are used to. In fact, we have spent our lives walking around in a three-dimensional space, what we called a Euclidean geometry, which is deeply intuitive to us. However, the geometry of networks is what we call their network topology. This topology stretches and bends our three-dimensional space around it. 

For example, let’s say I live in Istanbul, Turkey. If we pull out a map, we will see that Budapest capital of Hungry is much closer than London to Istanbul, but because London is a major hub in the global air transportation system, while Budapest in only a minor one, it is much easier to make a connection to London instead of Budapest. As a result, if we put something in this network at Istanbul, due to the network’s topology, London is essentially closer to Istanbul than Budapest. Hopefully this will give you some insight into how network operate in a different type of geometry than the one we are used to, and thus they cut across out traditional domains, not just in space as in this example but in all areas making the study of networks a truly interdisciplinary one. 

Thirdly, networks represent a very organic type of structure that often emerges from the bottom up but also has some environmental constrains imposed upon it. Examples of this might be the trading routs that have emerged at different periods in history. During the Middle Ages traders from Asia would exchange goods with merchants in India and the Middle East who would in turn bring goods to Europe and vise versa, thus emerging an almost global network of trade routes out of the local interactions between merchants. 

The same can be seen within an ant colony where individual ants leave a trail of pheromones to food sources. Here again a network emerges from the different ant trails. But none of these connections and the networks that emerge out of them are for free they cost something to maintain and thus many of these networks are the product of an interaction between the local elements that are creating the connections and the environment that is placing some resource constrain on its development. We will be coming back to illustrate this point more fully later on in the book. 

Lastly, complexity and nonlinearity are inherent features to networks. As the number of elements in our network may just grow in a linear fashion as in 1, 2, 3 and so on, the number of connections between them may grow exponentially, so if you take just a small group of people say 10 or 20 there can be literally billions of different types of networks between them, thus nonlinearity will be a reoccurring theme and we can only approach this type of exponential complex with the use of computers. 

When we combine this new way of describing the world that network theory gives us, with the powerful tools of computation and a flood of new data sources that we now have available to us we get a mini revolution and a new approach called network science, that is starting to having a major impact on many areas of science. Networks give us a very intuitive away of visually representing complexity. The model of a network is distinctly visual and this can give us a quick and intuitive overview to a complex system, by just looking at it we can get quick scenes of how connected it is, what are the key nodes and other critical information to understanding the whole system. 

I hope you have got an idea for what we mean by the network paradigm, it is about connectivity, the fact that this connectivity creates its own type of geometry, one that is often defined by how the elements in the network created the connections and the constraints its environment imposed upon it, with this world of networks often being nonlinear and only really approachable through the use of computation. 

To summarize, what we have covered in this section. We started our discussion on networks by looking at what we called the network paradigm, a paradigm is the set of methods and assumptions underlining a particular scientific domain as such it constitutes a whole way seeing the world. 

Firstly, we have noted that within systems whose components are relatively isolated we can focus our interest on the individual components within the system, by analyzing their properties we can gain an understanding of how the whole system works. But when systems become more connected we need to shift our focus to the dynamics and structure of the relations between them and this is what network theory is designed to do. 

Secondly, connectivity has a very different kind of space to the three dimensional Euclidean geometry we are used to which is the geometry of things or objects. The geometry of connectivity is what we call network topology and this topology stretches and bends our three dimensional space around it. 

Thirdly, networks represent a very organic type of structure that often emerges from the bottom up but also has some environmental constrains imposed upon it. Within network structures, elements often have a high degree of autonomy but their environment also places some constrains on the development of connections and thus the system is often a product of this interaction between the elements and the limitations the environment places on the system. 

Lastly, that nonlinearity and complexity are an inherent part of networks. However, as the model of a network is distinctly visual, it can give us a quick and intuitive overview to a complex system. The network theory takes advantage of new developments in computation to be able to tackle very large data sets and present them in a quick, meaningful and engaging way. 

Network Analysis 

In this section, we are going to give an overview of network analysis that will also work as an overview to the structure of this book and the content we will be covering. As the name implies, network analysis is all about the study of networks, we are trying to create models so as to analyze them, in order to be able to do this the first thing we need is some kind of formal language and this formal language is called graph theory. We will be going into the details of graph theory in the next section but it is a relatively new area of mathematics that gives us some kind of standardized language with which to talk about and quantify the structure and properties of networks. 

So, once we have this basic vocabulary, say you give me a network to analyze the question then turns to what are the features and properties of this network that we should be really interested. The first set of questions we might like to ask relate to individual elements within the network. We want to know, what are the nodes within the network? What are the connections between them? What properties are we really interested in? For example, in a computer network it might not be relevant to us who owns the different computers and connections but instead we might just be interested in the speed of the computers and the bandwidth of the connections, so we need to define what it is about our network we are interested in because as with all models, we will be focusing on some information and excluding other. 

There is lots of other information we want to know about these individual elements and the connections, such as asking whether they are weighted or not, meaning can we ascribe a value to them, we can talk about a computer network’s bandwidth in megabits per second, but it might not be so easy to do the same with a social network where the relations are of friendship or kinship. We can also ask if these relations go both ways or are just unidirectional. Other questions we will be asking here is how connected is any individual node or how central is it within the overall network. 

The next major set of questions we will be asking about our network will relate to its overall structure, networks are defined by both what happens on the local level, that is how central or connected you are, but also what happens on the global level, because the dynamics of the network on the global level feeds back to effect the elements on the local level. 

Here are some of the key questions we will be asking about the overall structure to the network: How connected is it? Are there connections between all the parts or are some parts disconnect and separate from others? How dense is this set of connections? If we compare a group of unassociated people waiting at a bus stop with a close knit group of friends we will see the density of the network will vary greatly. What are the patters of clustering within the system? Do we see many small groups or just a few large groups? These are the types of features that will define the overall makeup of the network structure. 

One key question we are interested in answering here is if we change some parameter to one of these properties, that is increase or decrease its value, how will that effect the overall structure to the system and we will be discussing all of this in a later section. 

The possible ways in which we can connect even a few element grows very quickly and there is of course many, many different structures to networks out there, we can’t possibly create a list of all of these, but what we can do is try and identify some fundamentally different types or models to networks. The first type of network that researchers started to explore was what is called a random network. 

By studying randomly generated networks, we get an important insight, which is that most networks aren’t random. Rather, they are created and often defined by the rules under which the elements chose to connect to other elements within the network. Sometimes networks are specifically designed in a top-down fashion, such as the a computer network within a corporations where some systems administrator has specifically designed it in a particular way. However, many of the networks we see around us are not like this. In fact, within many networks, such as commercial market, logistics networks, friendship networks, terrorist networks, food webs, and so on, the overall network emerges out of local level rules and interactions. When we begin to understand these rules, then we can begin to understand the different types of overall network structures that emerge out of them. 

The next set of questions we might want to ask about a network relates to how something will spread out or travel along the network and this is referred to as diffusion. If we are trying to understand the outbreak of a disease in a given area, we will be trying to understand how it is spreading and what network structures will give rise to rapid or delayed spreading, we will also be interested in how changing a given parameters will effect this. There will be times when diffusion is a positive thing and times when it is not where we will be talking about how to contain it to prevent disaster spreading. And this will lead naturally to a discussion on network robustness and fragility, how susceptible is our network to failure both from random and strategic attack. 

Lastly, how networks change over time will present us with another set of questions about the network we are analyzing, how does something like the network to a political regime come to form? What is the mechanism that holds it together? And when does it disintegrate? We will be discussing this lifecycle to networks in our last section on network dynamics. This set of four areas that make up the main sections to this book will provide us with a broad framework within which to understand networks and their dynamics.

Graph Theory 

When we hear the word “Network”, all sorts of things spring to mind like social networks and the Internet in particular. However, the power of network theory is really in its high degree of abstraction, so the first thing for us to do is to try and start back at the beginning by forgetting what we think we know about networks and embrace the abstract language of networks what is called graph theory. In the formal language of mathematics a network is called a graph and graph theory is the area of mathematics that studies these objects called graphs. The first theory of graphs dates back to 1736. The first textbook came about in 1958. However, most of the work within this field is less than a few decades old. 

In its essence a graph is really very simple, it consists of just two parts what are called vertices and edges. Firstly, vertices: a vertex or node is a thing, that is to say it is an entity and we can ascribe some value to it, so a person is an example of a node as is a car, planet, farm, city or molecule. All of these things have static properties that can be quantifies, such as the color of our car, the size of our farm, or the weight of our molecule. Within network science vertices are more often called nodes so we will be typically using this term during the book. 

Edges can be define as a relation of some sort between two or more nodes, this connection may be tangible as in the cables between computers on a network or the roads between cities within a national transportation system or these edges may by be intangible, such as social relations of friendship. Edges may be also called links, ties or relations and we will be often using this latter term during the book. The nodes belonging to an edge are called the ends, endpoints, or end vertices of the edge. 

Within graph theory networks are called graphs and a graph is defined as a set of edges and a set vertices. A simple graph does not contain loops or multiple edges, but a multigraph is a graph with multiple edges between nodes. So where as a simple graph of transpiration system would just tell us if there is a connection between two cities, a multigraph would show all the different connections between the two cities. 

Graphs can be directed or undirected. With an undirected graph, edges have no orientation, for example a diplomatic relation between two nations may be mutual and thus have no direction to the edge between the nodes. These undirected graphs have unordered pairs of nodes, that means we can just switch them round, if Jane and Paul are married, we can say Jane is married to Paul or we can say Paul is married to Jane it makes no difference and thus it is an unordered pair. 

On the other hand, directed graphs are a set of nodes connected by edges, where the edges have a direction associated with them, that is typically denoted with arrows indicating the direction, for example if we were drawing a graph of international trade the graph might have arrows to indicate the direction to the flow of goods and services. So directed graphs have some order to the relations between nodes and this can be quite important. 

A graph is a weighted graph if a number (weight) is assigned to each edge. These numbers quantify the degree of interaction between the nodes or the volume of exchange, so with our trading example earlier, if we wanted to convert this into a weighted graph we would then ascribe a quantitative value to the amount of trade between the different nations. 

So, this is the basic language of graphs that we can extend to graphs that have multiple types of edges and nodes. These are called multiplex networks and add a whole new level of complexity to our representation. They allow us to capture how different networks interrelate and overlap to affect each other but this is beyond the scope of our book, as the basic language we have outline above will be sufficient for our introduction to network theory. 

To summarize, here’s what we’ve covered in this section: we have presented the basics of graph theory, which is a relatively new area of mathematics that studies networks and gives us a formal language with which to describe them. The two basic components to a network are vertices and edges, a vertex or node is a thing, that is to say it is an entity which has properties and we can ascribe some value to it. 

Edges also called links or ties can be defined as a connection of some sort between two or more nodes, these connections represent an exchange or some kind of an interaction. A graph is a combination of a set of nodes and edges that connect them, a multigraph is a graph with multiple edges between nodes. Graphs can be directed or undirected depending if there is some specific direction to the interaction between the nodes.

Lastly, we talked about weighted graphs. A graph is weighted if a number (weight) is assigned to each edge. This weight then represents the degree of interaction between the nodes and may measure such things as how much time two people spend together, the amount of data flowing on a telecommunications network or financial capital flowing between to institutions 

Connections

In this section we are going to continue to expand on our vocabulary but this time focusing on how we model the connectivity of a node or between nodes within a graph. As we mentioned, connectivity is a key concept within the network paradigm, we often try to measure the significance of things in terms of the quantity to one of their properties but within networks how connected an individual node is becomes a key metric of its significant within the network. 

So, how connected is any given node in the network? And this is termed its degree of connectivity. The degree of a node in a network is a measure of the number of connections it has to other nodes. This degree of connectivity can be interpreted in terms of the immediate likelihood of a node catching whatever is flowing through the network. For example, the higher your degree within a given social network the more likely you are to hear about some juicy piece of gossip, because you have many more channels through which to intercept it, of cause this works both ways as it might not be juicy gossip that is spreading on this network, but instead a virus, connectivity is often interpreted as a positive thing but not always. 

If we are analyzing an undirected and unweight network, a node’s degree of connectivity will then simply be a summation of all the links that the node has. If the graph is directed then we can refine our analysis by dividing this into a measure for the amount of in and out links, so returning to our example of nations trading the in degree of any node would be the total number of import relations it has with other nations and the out degree would be the total number of exporting relations it has. If this is a weighted graph we can then of cause refine our model farther by placing quantitative values on each edge. 

If an edge exists between node A and B then we say they are adjacent, so if we take a map of a subway we could say that each station or node is adjacent to any other station that is just one stop away from it. We can then capture all the relations within a network by creating an adjacency matrix. This is a non-mathematical introductory book so we will not be going into matrixes but to just give you a quick insight into how this is represent. We can create a simple 2 by 2 matrix to capture the connections within a network by placing a 1 at the cross section between two nodes if they are adjacent and a 0 if not, with the end result being a table of all the connections within the system and this will give you an idea of what a graph looks like to a computer. 

Let’s take a look at how two nodes in a network are connected, meaning the channel or paths through a network from one node to another and this is called a walk. A walk on a graph is a sequence of adjacent vertices where repetition is allowed, with a walk we are simply going from one node to the next along a sequences of edges. A path is a walk without revisiting any nodes that is a sequence of links from the first node to last without repetition. 

Let’s cite the so-called travelling salesman problem, which involves a salesman that has to visit a number of different cities within a particular region, he of cause wants to avoid walks where he will be retracing the same ground and try and find a path that will not involve any repetition of the cities he has to visit, their are a number of different algorithms for achieving this but we won't go into them here. Our traveling salesman will also be interested in finding the shortest path between each place he has to visit, this shortest path between two nodes on a graph is called the geodesic and it represents the fewest number of links we need to traverse in order to get from any given node to another. 

To summarize we have started a discussion on one of the central concepts within network theory that of connectivity by talking about the number of connections an individual node has. The degree of a node in a network is a measure of the number of connections it has to other nodes, we can refine our analysis of a node’s degree of connectivity by asking how many in-links or out-links there are and ascribe a value to them if it is a weighted graph If an edge exists between node A and B then we say they are adjacent and can create what is called an adjacency matrix to capture in data all the individual connections within the system. 

A walk on a graph is a sequence of adjacent vertices where repetition is allowed. A path is a walk without repeating nodes, a sequence of links from the first node to last with out repetition. Lastly, we talked about the concept of a geodesic which is the shortest path between two nodes on a graph or the fewest number of links we need to traverse in order to get from any given node to another. 

Centrality Measures 

Centrality is really a measure that tells us how influential or significant a node is within the overall network, this concept of significance will have different meanings depending on the type of network we are analyzing, so in some ways centrality indices are answers to the question "What characterizes an important node?" 

From this measurement of centrality we can get some idea of the nodes position within the overall network. The degree of a node’s connectivity that we previously looked at is probably the simples and most basic measure of centrality. We can measure the degree of a node by looking at the number of other nodes it is connected to vs. the total it could possibly be connected to. But this measurement of degree only really captures what is happening locally around that node it don’t really tell us where the node lies in the network, which is needed to get a proper understanding of its degree centrality and influence. 

This concept of centrality is quite a bit more complex than that of degree and may often depend on the context, but we will present some of the most important parameters for trying to capture the significance of any given node within a network. The significance of a node can be thought of in two ways, firstly how much of the networks recourses flow through the node and secondly how critical is the node to that flow, as in can it be replaced, so a bridge within a nations transpiration network may be very significant because it carries a very large percentage of the traffic or because it is the only bridge between two important locations. So this helps us understand significance on a conceptual level but we now need to define some concrete parameters to capture and quantify this intuition. 

We will present four of the most significant metric for doing this here. Firstly as we have already discussed a nodes degree of connectivity is a primary metric that defined its degree of significance within its local environment. Secondly, we have what are called closeness centrality measures that try to capture how close a node is to any other node in the network that is how quickly or easily can the node reach other nodes. Betweenness is a third metric we might use, which is trying to capture the nodes role as a connector or bridge between other groups of nodes. Lastly we have prestige measures that are trying to describe how significant you are based upon how significant the nodes you are connect to are. Again, which one of these works best will be context dependent. 

Let’s focus on closeness centrality. Closeness is defined as the reciprocal of farness where the farness of a given node is defined as the sum of its distances to all other nodes. Thus, the more central a node is the lower its total distance to all other nodes. Closeness can be regarded as a measure of how long it will take to spread something such as information from the node of interest to all other nodes sequentially; we can understand how this correlates to the node’s significance in that it is a measurement of the nodes capacity to effect all the other elements in the network. 

Betweenesss centrality, as mentioned, is really taking about how critical a node is to a network in its functioning as a unique bridging point between other nodes in the network. Betweenness centrality quantifies the number of times a node acts as a bridge along the shortest path between two other nodes. In this formulation, vertices that have a high probability of occurring on a randomly chosen shortest path between two vertices have a high betweenness value. 

Our last measure is trying to capture how connected the nodes that a given node is connected to are, so instead of looking at the total amount of connections you have it is more interested in the value of those connections, one way of capturing this is called eigenvector centrality. Eigenvector centrality assigns relative scores to all nodes in the network based on the concept that connections to highly connected nodes contribute more then connections to nodes with lower degrees of connectivity. Eigenvector centrality is one measure used by web search engines to try and rank the relative importance of a website by looking at the importance of the websites that link into it. 

Now that we have a basic understanding of the different metrics for talking about a nodes centrality within a network we can take a look at a graph and see how they each present a different perspective and set of results to this question. Here is the same set of networks with node ranking depicted in colors dark blue for a low rank through to red with the highest ranking. We can note how the color changes around the network for each set of metrics we apply thus indicating the different information they are capturing and the degree of relativity involved in trying to capture a nodes degree of centrality.

To summarize, we have talked about one of the key concepts in network theory, centrality. Centrality gives us some idea of the nodes position within the overall network it is also a measure that tells us how influential or significant a node is within a network although this concept of significance will have different meanings depending on the context. We have presented four of the most significant metrics for measuring centrality; 1. Degree connectivity 2. Closeness centrality 3. Betweenness centrality 4. Eigenvector centrality.

Degree centrality is simply a measure of the number of in and out links a node has and may depend upon the weight of these links. Closeness centrality is a measure of its distance to all other nodes. The closeness of a node can be calculated by reference to its shortest path to all other nodes in the network. 

Betweeness centrality is trying to define how critical a node is to a network in its function as a unique bridging point between other nodes in the network, this can be quantified as the number of times a node acts as a bridge along the shortest path between two other nodes. 

Eigenvector centrality assigns relative scores to all nodes in the network based on the concept that connections to highly connected nodes contribute more than connections to nodes with a low degree of connectivity. 

Networks Topology Overview 

In the coming sections we are going to be looking at global metrics that refer to the whole of a graph. As we have previously noted, networks are a very informal type of structure they often simply develop without any overall top down design. Someone builds a protocol for two computers to exchange information over a network and shares it with a colleague, while other people see the utility of it and connect to this little network and then more people as the network grows, until 25 years later we have a massive network of networks that is the Internet. 

No one planned the Internet , just as no one really planned the global financial networks that have emerged over the past few decades. Traders, investors and institutions set up connections wherever they thought there was a viable return on investment, but now that these networks are here there overall markup feeds back to effect us the users, networks may start out quite random but they often develop into some stable overall structure and understanding the patterns to this overall structure is of central importance in network theory and the focus of this section to the book. 

We can call this overall structure to a network its topology, where topology simply means the way in which constituent parts are interrelated or arranged: Within the context of a network it defines the way different nodes are placed and interconnected with each other and the overall patterns that emerge out of this. 

To illustrate this farther let’s think about a set of simple networks each containing the same amount of nodes but each having a different overall topology owing to the way it is connected. We might have what is called a tree graph with a tree like structure branching out, we may have a ring graph with a circular structure, a start graph with one central node with all others as off shoots from this. 

We can imagine how these different network topologies would in turn have very different features and properties to them. Imagine they were different transportation systems in which you are trying to get form A to B. In the star topology it would only ever take you two hops to reach your destination but in the ring it might take 3, the tree structure possibly 4, and this same influence from the topology would apply if we were trying to rout water through a hydraulic network or electricity on a power grid. 

This network could also represent the flow of information within a national society we can see how the centralized star structure or the tree structure would be much easier for some political regime to control and influence as apposed to the more decentralized fully connected or ring network. Networks have overall structure and this overall topology to the network matters as it feeds back to effect the actions and capabilities of the nodes on the local level. 

So to start digging into the analysis of some of the most important macro features to a network we might begin with connectivity as primary. With connectivity we are really talking about the density of the connections in the system. As we will be discussing shortly, the degree of connectivity will largely be a product of how easy or difficult it is for any two nodes to form a connection. As we reduce the barriers to interaction we will see the network become dense, integrated and increasingly defined by the structure and make of the network, as opposed to the components in isolation. 

Another macro scale property to the system we will be interested in modeling and quantifying is its size. By size we simple mean the number of nodes, this may sound like a trivial factor but scale can matter, as sometimes more is not just more it is can in fact be different. Think about living in a small rural community where everyone knows everyone by just one or two degrees of separation vs. living in a large urban metropolis where the anonymity of much longer path lengths between people creates new types of social dynamics. 

Lastly, in these coming sections we will be talking about a network’s overall pattern of connectedness. The way in which a network is connected plays a large part in how networks are analyzed and interpreted. Due to some common set of properties shared by a subset of the system we often get subsystems forming within networks, these subsystems are called clusters and often have a significant effect on the networks makeup, for example we might think here about the clustering in the different cultural groups around the world, although two culture like that of France and Italy are different, they share a common Greco-Roman heritage that gives them and other European countries a set of common features through which they form a cultural cluster within the network of global cultures. 

We will finish this section on network topology by saying that networks are often created by the nodes within the network, which create or not connections in response to local level conditions. However, once a network has reached a level of maturity, a global level structure will have emerged to it that feeds back to affect the elements in the systems. Once this is the case, we then need to analyze this global structure to the network, what we called its topology in order to understand it. In the coming three sections to this book we will be looking at some of the primary features that shape this macro scale topology to networks. 

Connectivity 

One of the defining features to a network is its overall degree of connectivity, which might qualify as the defining feature. Going from a system with a low degree of connectivity to one with a high degree of connectivity is not just a quantitative change in the number of edges within the network it is also qualitative change as we have previously tried to note, it marks a shift from a component based regime, where we need to firstly think about the components to the system, their properties in isolation and their linear interactions, to a relational based regime where we need to first model how the system is interconnect. 

One way of contextualizing the degree of connectivity to a network is by talking about how easy or difficult it is for a node in the network to make a connection with another. That is because the overall connectivity emerges out of the local actions of the nodes in the network, if we make it difficult for them to interact then there will be a lower overall connectivity, we might call this ease of two nodes interacting or coupling, the coupling parameter to the network. 

Therefore, if we take any network, say a logistics network, we can ask: Under what circumstances are the nodes more likely to interact? In this case, the nodes are producers, distributors and consumers, and they will be more likely to interact as the cost of transportation and trade restrictions are reduced. The development of the global economy over the past few decades could be sited as an example here, through the reduction in trade tariffs and advancements in transportation and communications. The ease of interaction between producers and distributors on a global level has increased, resulting in the increased density of logistics and trade networks as regional and national economies have become intergrade into the global economy. 

In America, the logistic cost of transporting some freight is thought to be around 5% of the cost of the goods where as in china it is thought to be around 12 to 13%, you can then imagine how a business will factor this into their choices as to whether they supply to far off distributors or not and thus whether the network become more dense or more sparse. 

One way of quantifying this concept of the overall connectivity to a network is with reference to its density. The density of a network is defined as a ratio of the number of edges to the number of possible edges and this will also correlate to the average degree of connectivity to the nodes in the network. So when we increase our coupling parameter we are increasing the density of the network and the average degree of connectivity. 

This coupling parameter to a system can of course be defined by many different things, depending on the network we are dealing with it may be economic as in our example above which was measured in terms of the financial cost of transportation, or it may be measuring the climatic condition to an ecosystem, where we could quantify it in terms of the average environmental temperature, as we reduce the temperature the number and density of interactions between creatures reduces as they hibernate in isolation. We could also think about the formality of a social setting as a parameter as we reduce the formality of the setting, say by having an office party, peoples social inhibitions are reduced and they are more likely to interact. 

As we turn this coupling parameter up or down thus requiring the nodes to exert more or less resources in order to create a connection, we would expect the level of integration within the network to increase or decrease. The easier it is for elements to create a connection the more connections and the longer these connections can be, thus working to integrate the entire system. Inversely, the more resistance there is for nodes to create connections the less there will be and the network will disintegrate, with the most costly ones, that is those that are maintained over a greater distance being the first to go. But this does not always change in a linear fashion, as I will now demonstrate. 

The amount of connectivity within the network will be primary defined by how much resources a node has to expend in order to make that connection. The amount of resources that a node will have to expend in order to create a connection will grow in a some what proportional fashion to the length of the relations. For example, if I am walking to the local shop, I will have to exert a certain amount of energy to do this and if the shop is twice as far away I will have to expend twice as much resources in order to create that connection, this is a linear progression. But this simple linear scaling is not always the case. In fact, say I am taking an intercontinental flight, a journey of some 2000 kilometers; well, this does not require 10 times more effort on my behalf than a local flight of 200 kilometers. 

Because of this the degree of connectivity in the system may not always grow in a simple linear fashion, but because of this nonlinearity the level of connectivity can grow or decay in an exponential and rapid fashion resulting in there being tipping points and phase transitions. We will be discussing this farther later on when we look at the dynamics of networks, that is how they change overtime.

Network Diameter & Scale 

The size of a network is important not so much because of the sheer quantity of elements we are dealing with, but more because it sets the context for how close or far on average one node in the network is from another and this is important because it will tell us how quickly something will spread through the network and also how integrated different components in the network are likely to be. 

Making a connection within a network, or travelling from one node to another is rarely free it typically costs some resource, whether this is the cost of fuel to travel in a transportation network, the laying of cables to transport information, or the risk of rejection when you ask some one out on a date. The farther we have to travel along a network to get from A to B the more it will cost and the less likely it will occur, with the result being a lower level of integration to the system. 

For example, the Russian Empire in the 1900s, a vast land mass spanning from Europe to East Asia but without any coherent transportation network connecting it, was continuously under threat of falling to pieces. However, with the building of the Trans-Siberian rail network, information, goods and resources could eventually diffuse to the different parts of the system and it is this interaction between components to the system that gives it cohesion and integration. We may be dealing with a very large system but if there is no diffusion or interaction across the network it ceases to function as an entirety. 

Two important metrics for capturing this overall distance between nodes are: the networks diameter and its average path length. The diameter of a network is simply the longest of all the geodesics in the network, if we remember back a geodesic is a shortest path between two nodes. So when we are asking for the diameter of a network we are looking at all the shortest paths and then choosing the longest one, and this will give us an idea of how far something might have to travel to get all the way across the network. The average path length is calculated by finding the shortest path between all pairs of nodes, adding them up, and then dividing by the total number of pairs. This will shows us the number of steps on average it takes to get from one member of the network to another. 

So we can take a real world network and ask what is its average path length, for example a number of years ago researchers studied the social network of Facebook when it had approximately 721 million users with 69 billion friendship connections between them, the average path length turned out to be just 4.74 intermediary connections. This appears to be an extraordinary low distance between any two members of such a large network. 

We can quote one of Facebook’s spokesperson on this finding when he said, quote, "In these two works, we show how the Facebook social network is at once both global and local, It connects people who are far apart, but also has the dense local structure we see in small communities.” This is what is called the small-world phenomena that we will be discussing in greater depth in a later section, but one thing we wish to stress here is that this question of how close things are in a network is not just a product of its size but it is also a product of the structure to the network as we would expect. 

If we take to different network topologies, such as a ring and tree network and look at the diameter of the ring network we will see it is quite high, in fact it is half the number of nodes in the network. If we look at the tree network that has the same number of nodes we see the diameter is much shorter, due to this branching structure to the network which is a much more efficient way of connecting things. But the thing to note is that with the ring topology the amount of nodes I can reach is only growing in a linear fashion relative to the amount of links I have to travers, so if I travel 1 link away I can reach 2 nodes, if I travel 2 links away I can reach 4 nodes if I go 3 links I can reach 6 and so on this is a linear progression. 

In contrast with our other tree network structure, if we place our selves at the top of the tree, now due to this branching structure we can get an exponential growth in the number of nodes we can reach relative to the distance we have to travel, at one step we reach just 2 nodes but at 2 steps I can reach 6 nodes, at 3 steps the whole network of 14 nodes and it is this same exponential growth that is one of the factors behind the small-world phenomena allowing us to have a surprisingly short average path length even within very large networks like Facebook. 

Thus, we should be able to see how even if we had a much larger network, if it has the right structure then we will be able to reach more nodes in a shorter path length than if the network was small but had a structure that gave it only a linear relation between distance traveled and nodes connected to. And this is important because we often think about and measure size and scale in terms of some static quantity, the number of people in a city or the creatures in an ecosystem. But as networks are all about connectivity, what really matters with respect to scale here is how far you are away from other nodes which can be dramatically altered by simply restructuring the network and thus scale becomes much more subjective and relative to the topology of the network. 

Clustering & Connectedness 

The way in which a network is connected plays a large part in how we will analyze and interpret it. When analyzing connectedness and clustering we are asking how integrated or fractured the overall network system is, how these different major sub systems are distributed out and their local characteristics. A graph can said to be connected if for any node in the graph there is a path to any other node, when the graph is not connected then there will be a number of what we call components to it. A component is a sub-set of nodes and edges within a graph that are fully connected, thus for a node to be part of a component it must be connected to all the other nodes in that component. 

A cluster is simply a subset of the nodes and edges in a graph that possess certain common characteristics, or relate to each other in a particular way forming some domain-specific structure. So where as a component is simply referring to whether a given set of nodes are all connected or not, a cluster is referring to how they are connected and how much they are connected that is the frequency of links between a given subset of nodes.

In order to model the degree of clustering of a subset of nodes we simply take a node and look at how connect a node it links to is to other nodes that it is also connected to. So if this was a social network of friends we would be asking how many of your friends know your other friends, the more your friends are interconnect the more clustered the subset is said to be. This clustering within social networks is also called a clique, a clique is a group of people who interact with each other more regularly and intensely than others in the same setting. 

Within this social context clustering can be correlated to homophily, where homophily describes the phenomenon where people tend to form connections with those similar to themselves, as captured in the famous saying “Birds of a feather flock together”. We might think of clustering coming from the fact that the interaction between nodes with similar attributes will often require less resources than interaction between nodes with different attributes, for example between to cultures there may be a language barrier or between different devices on a network that might have different protocols, or clustering may be due to physical constraints of the resource expenditure required to maintain them over a greater distance, thus resulting in a clustering around a geographic neighborhood. 

Understanding the different local conditions that have created clustering within a network are important for understanding why the network is distributed out into the topology that it has, how you can work to integrate it or disintegrate it and how something will propagate across the network, as each one of these clusters will have its own unique set of properties within the whole making it particularly receptive or resistant to a given phenomena. For example, we might be analyzing a political network, with each cluster in this network representing a different set of ideologies, social values and policy agendas that are receptive to different messages. 

Or as another example by understanding that different clustering groups on a computer network may represent different operating systems we will be able to better understand why a virus has rapidly spread in one part of the network but not in another and also by understanding these local clustering condition we will be able to better approach integrating them into the broader network. 

The clustering coefficient of a node is then a method for measuring the degree of a local cluster, there are a number of such methods for measuring this but they are basically trying to capture the ratio of existing links connecting a nodes neighbors to each other relative to the maximum possible number of such links that could exist between them. A high clustering coefficient for a network is another indication of this small-world phenomenon that we saw previously. 

Degree Distribution 

In this section of the book, we are going to talk about the different types or models to networks that we find in the world, these different models are based upon a key feature to the make-up of a network -that is how centralized or distributed it is- as this will define many properties to the network such as how something will flow through it, which nodes have influence or how quickly can we effect the entire network. 

Whereas in the last section to the book we were really looking at the overall degree of connectedness to a network as a primary parameter, when we turned it up or down it worked to integrate or disintegrate the entire network. The key parameter we will be exploring in this section is a network’s degree distribution; degree distribution tries to capture the difference in the degree of connectivity between nodes in a graph, it is really asking the question, do all the nodes have roughly the same amount of connections or do some have very many while others have very few connection? 

The different network models we will be talking about in this section will lay on this spectrum of degree distribution. Starting from systems with very homogeneous degree distribution, that is all nodes have a relatively similar amount of connections, here we will be talking about random networks and distributed systems where we have a relatively even topology to the network, but as we turn our degree distribution parameter up we will start to see hubs appearing, these types of networks are described as decentralized, implying that unlike our distributed graph where there was no real center these have a number of different central hubs to them, these decentralized networks have the small-world property that we mentioned earlier making them very effective at connecting a large amount of elements within a short average path length. 

Lastly, as we turn up our degree distribution parameter to make a very large disparity between the nodes different degrees of connectivity, we will start to get centralized networks with one or few dominant nodes and many nodes with a relatively low level of connectivity, this type of network is captured within a model called a scale free or power law network that we will be talking about in a later section. 

Why do we get these different networks with fundamentally different degree distributions? The answer to this question should become apparent during the next few lectures, by starting with random networks we will be able to see that most networks are in fact not random at all, birds don’t just chose at random what other creatures they are going to prey on within a food web, people don’t randomly choose their friends and transport authorities don’t just randomly lay down highways between any two locations, these connections are of cause make under specific rules, that govern why and to which other nodes any node will make a connection with. And it is out of the aggregate behavior of these nodes interacting that we get networks that have specific and widely encounter properties, meaning we don’t live in a world of random networks but in fact a world of networks that have a specific structure that has emerged out of these local rules. 

Lastly, Just to emphasize that this degree distribution parameter may just be a quantitative parameter but changing it can have a qualitative effect on the network we are dealing with, if you are inside one of these networks that we have listed above things are very different depending on which one you are in. To illustrate this we might take an example of a political network. Political social networks span from the highly centralized for of dictatorship, where all socio-political connections lead back to one dominant node, to at the other end of the extreme some kind of egalitarianism where responsibility and authority is distributed out in a pluralistic fashion. These different network structures will have a systemic effect and influence on almost all areas of the social and cultural fabric, demonstrating the significance of this degree distribution to a network that will be the topic of our coming set of chapters. 

Random & Distributed Networks 

When presenting the different classes of networks, people often start by presenting the random model to a network first, not because most networks in our world are random, quite the contrary, but it is by first understanding what a network created without any specific rules looks like that we can then compare the other networks we encounter around as to see if they differ from this and if they do then we can ask about the rules that created them. 

A random network is more formally termed the Erdős–Rényi random graph model, so named after two mathematicians who first introduced a set of models for random graphs in the mid 20th century. As the name implies this type of network is generate by simply taking a set of nodes and randomly placing links between them with some given probability. So we just take two nodes in the network and we role a dice to see if there will be a connection between them or not, the higher we set our probability the more likely there will be a connection and thus the more connected the overall graph will become. This is a simple system in that once you have decided how many nodes there will be, it is then really just defined by a single parameter, which is the probability parameter for the likely hood that any two nodes will form a connection. 

If we looked at the degree distribution of this network, it would follow a normal distribution. Since it was randomly generated, there will be some difference in the distribution of degrees of connectivity among the nodes. In fact, some will have one degree, some five, but there will be a well defined normal or average degree. In this distribution, there will be very few nodes with a very large degree and very few with a very low degree, most will tend towards the normal amount of connections. 

Unlike real world networks, there is low clustering in random networks. Therefore, the resulting network very rarely contains highly connected nodes. Consequently, a random network is not a good candidate model for the highly connected architecture that characterizes many of the networks we see around us. Although a useful theoretical exercise, random networks in generally do not represent networks in the real world, they are considered far more random because real world networks are typically create to serve some function and are constrained by some limiting resource, that gives them a more distinct pattern. If we look at some network like the traditional trade routs across the Sahara desert in Africa it may look some what random at first glance but we know that it is not because for the caravans of camels and traders who created these network, setting out to cross the Sahara In any random direction would have of cause been fatal to them. 

This helps illustrate the two key factors to generating any given structure to a network that we have previously only touch upon. Firstly, the context or environmental constrains the network is under, different types of networks are all under different types of environmental constrains. For example, this may be the geological constraints placed upon the travelers in our example above. It may be the physiological constrains placed upon the metabolic network with the human body, or it may be the financial constrains placed upon a logistic network. 

All of these represent resistance to network formation that the environment places upon the network, but inversely we can look at this the other way round asking what methods the nodes in the network use to overcome these constrains. Our travellers in the Sahara were using prior knowledge encoded in maps as to where the water wells were in order to overcome the arid environmental constraint placed upon them. A national airline, because of the limitations on finance, may not be able to run a direct rout between every city within the country but will get around this by creating a hub and spoke network so as to reach all locations. Again, this is a method or strategy for overcoming resource constrains and out of the interaction between these environmental constrains and the methods used by the nodes to overcome them we will get a particular overall structure to the network that makes them distinctly different from our random network. 

The first type of network model we will discuss then is called a distributed network. This type of model is in many ways quite similar to our random model, it is defined by a low degree distribution level, meaning all or most of the nodes have the same degree of connectivity and as there are no dominant nodes to provide global functions for the entire network each node must contribute equally to the networks maintenance. And as there is no real global coordination in a distributed network nodes can have a very high degree of autonomy, as they are largely self-sufficient and independent from nodes outside of their neighborhood. An example of a distributed network might be a community alert group, where each member of the community has equal responsibility and authority to act when there is an event that others should know about, there is no hierarchy and in this example the network is only actualized when needed thus placing very limited constrains on its members. Within the world of computing distributed networks are also called mesh networks. 

Distributed networks have a number of advantages and disadvantage, on the positive side as we will be discussing later they maybe very robust to failure as there is not critical or strategic nodes in the network any node can theoretically be replaced by any other and as we have already noted elements may have a high degree of autonomy, with little network maintenance tax placed upon them. 

But also this type of network can be less efficient in many circumstance, without centralized nodes there can not be any centralized batch processing that leverages economics of scale and diffusion across the network can be slow as there are no central hubs with which to reach many nodes in a single hop, this can also create problems in terms of coordinating the network as a whole. In many ways, a distributed network represents a system in a fine balance and relatively stable state and this is often not what we see when we look at real world networks, in the next section we will be turning up our degree distribution parameter to the point where small local hubs emerge in what is called a decentralized network.

Decentralized & Small World Networks 

In this section, we are going to continuing on with our discussion about how different degree distributions within a network generate different models to networks. We previously looked at what happens when we have a low level of diversity between the different nodes degree of connectivity by exploring distributed networks, that had a very egalitarian degree distribution. Many networks don’t show this pattern though. For as we turn up our degree distribution allowing for nodes with a much higher degree of connectivity than others what we see instead is the overall topology to the network becoming more differentiated as local clusters emerge with some nodes playing a central role, that is defined as a hub. 

We call this network model that has local hubs but still relatively little overall differentiation to it, a decentralized network, there may be some overall center to it but it is still defined largely by what is happening on the local or regional level. To take an example of a decentralized network we could cite the urban network of contemporary Germany, unlike other countries such as Japan or Nigeria whose urban network is dominated by a primary node, Germany’s urban infrastructure and the services that it provides are distributed out into an number of important centers, for example the primary air transportation and financial hub is in Frankford, the political capital in Berlin, with Munich having the strongest economy, each of maybe five or ten centers play a very important role in maintaining the network. There are of cause many more examples we could cite, such as conglomerate corporations, political federations or distributed computer networks. 

So, why do we get these local level hubs and spoke structures emerging? There are a number of reasons for this but many tie back to the fact that the system is under certain environmental resource constrains and it will only be possible for nodes to overcome some of these constrains by combining their resources. This, coupled with batch processing and the economies of scale that it enables, are behind the formation of many hubs, from banks that amass finical resources to be able to fund large projects, to international airports, to the emergence of factories as local hubs in manufacturing networks. 

These hubs then serve the function of connecting nodes locally, but also connecting them globally to other hubs in the network. The result then is local clustering but also some global connections between clusters and this give us the small-world phenomena previously mentioned. A small-world network is a type of graph in which most nodes are not neighbors of one another, but most nodes can be reached from any other by a small number of connections. A certain category of small-world network was identified by Duncan Watts and Steven Strogatz in 1998. Watts and Strogatz measured, that in fact many real-world networks have a small average shortest path length, but also a clustering coefficient significantly higher than expected by random chance. 

In a sense, this is quite counter-intuitive as what it is saying is that even though there is a significant amount of clustering in these networks, meaning that nodes are typically highly connected to other local nodes in their cluster, and if we had quite a large network like this then we would expect there to be quite a long shortest path length, which is not the case here. 

Eventually, they came up with a model that captured this phenomenon. It involved starting with a ring lattice where all the nodes are only locally connected, thus has high clustering. But, then randomly picking some links to rewire, so that they would not connect to their local cluster but somewhere else in the network, they found that you don’t need to randomly rewire very many links before the shortest diameter starts dropping very quickly and from this we are able to capture the small- world phenomena. 

The small worlds phenomena has since gone on to be popularized in the six degrees of separation hypothesis, which is a theory that everyone is just six or fewer steps away from any other person in the world, so that a chain of "a friend of a friend" connections can be made between any two people in a maximum of six steps. Many empirical graphs are well-modeled by small-world networks. Social networks, website links on Internet, wikis such as Wikipedia, and gene networks all exhibit this small-world characteristic. 

We can see the small world phenomena behind our decentralized network model, as it had these local clusters with hubs, with the hubs making global connections allowing for a relatively efficient set of overall connections to the system, without having to expend to much resources on maintaining very many global connections that are likely to be expensive and difficult to maintain.

Our decentralized model represents a certain mid-range of degree distribution; the level of connectivity between a typical node and one of these local hubs is significant but not too large. But again we could turn up our degree distribution farther, all the way up until the system is connected through just a few nodes or even just one dominant node, this would give us what we call a centralized network which we will turn our attention to in the next section. 

Centralized & Power Law Networks 

Centralized networks represent networks with a very high degree distribution, meaning in this type of network structure there will be very many nodes with a very low level of connectivity and very few, or maybe just one node, with an exceptionally high degree of connectivity. Therefore, they are very heterogeneous and unequal in terms of how connected and influential the different nodes in the network are. 

Let’s start by taking an example of a centralized system. If we were to look at a network of global banking activity with nodes representing the size of assets booked in the respective jurisdiction and the edges between them the exchange of financial assets. We would see how a very few core nodes dominate this network, there are approximately 200 countries in the world but the 19 largest jurisdictions in terms of capital together are responsible for over 90% of the assets. This type of centralized structure to a network is surprisingly prevalent in our world and we could cite many other examples of it, such as social networks where a very few people may have millions of people connected to them and the vast majority very few. 

These highly centralized networks are more formally called scale-free or power law networks, which describe a power or exponential relationship between the degree of connectivity a node has and the frequency of its occurrence. These power-law networks are really define by the mathematics that is behind them, the number of nodes with degree x is proportional to 1 over x squared. So, the number of nodes with degree 2 is one fourth of all the nodes. The number of nodes with degree 3 is one ninth of all the nodes. The number of nodes with degree 10 is proportional to one hundredth. The point to take away from this is that this long tail means there can be nodes with a very high degree but there will also be very many with a very low degree of connectivity giving us our centralized network. 

This type of power law graph was first discovered within the degree distribution of websites on the Internet with some websites like Google and Yahoo having very many links into them but there also being very many sites out on the web that have a very few links into them. Since then it has been discovered in many types of very different networks such as in metabolic networks where the essential molecules of ATP and ADP that provide the energy to fuel cells play a central role interacting with a very many different molecules, where as most of the molecules interact with very few others, thus making these two molecules hubs in the metabolic networks fueling the cells in our bodies. 

This power law has also been document in the frequency of citations between academic papers and within the social network between of Hollywood actors. This scale free property to networks is then interesting because it appears regularly and across all forms of networks from the Internet to social groups to biological systems. The power law distribution to a network like the World Wide Web is often explained with reference to what is called preferential attachment. Preferential attachment describes how a resource is distributed among a number of nodes according to how much they already have, so that those who already have a lot receive more than those who have little, in more familiar terms this is called “The rich get richer.”

Within this model if you are say building a website and choosing which other website to link to, then you will be twice as likely to link to a website that has twice as many links as another. To formalize that a bit better, the probability that you will make a link to a site is proportional to the size of the site. If a network was created under these rules, then we should get a power law distribution. But, in reality, this is quite a simplified mode, it should just give you an idea for some of the mechanics behind these power law networks. Why we have these very large centralized nodes in the financial system we discussed earlier is, of course, much more complex than this, as it involves a number of different parameters, most notable among these is the actual quality of the service that the node is providing not just its size. 

Lastly, centralized networks can be very robust or very fragile depending on if we remove nodes randomly or strategically. If we were removing nodes randomly they will be very robust to failure because the vase majority of the nodes have a very low degree of connectivity and thus we would likely be removing one of these insignificant nodes with little effect on the overall network. But inversely if we were to remove a node strategically, that is to say purposefully choosing the node we remove in order to do maximum the damage we are doing, then these centralized networks are very susceptible to failure of this kind because we just have to remove one of the giant hubs that are critical in their role connecting many smaller hubs and the system will be effected greatly by this. 

Network Dynamics 

Traditionally, research in graph theory focuses its attention very much on studying graphs that are static. However, almost all real networks are dynamic in nature and how they have evolved and change overtime is a defining feature to their topology and properties. As network theory is a very new subject much of it is still focused on trying to explore the basics of static graphs, as the study of their dynamics, results in the additional of a whole new sets of parameters to our models and takes us into a new level of complexity, much of which remains unexplored, and is the subject of active research. 

We can start by talking about growing a random network to see what it looks like, when we say growing a network we might mean adding more nodes to it, but also, more interestingly adding links to it that increases the overall connectivity. In our random model links were just placed between nodes at random with some given probability, growing the network here just meant increasing this probability so as to have more links develop over time. One interesting thing we find when we do this is that there are thresholds and phase transitions during the networks development, by thresholds we simply mean that; by gradually increasing our link probability parameter, some property to the network suddenly appears when we pass a critical value. 

For example, our first threshold is when the average degree goes above 1 over the total number of nodes in the network, as at this threshold we start to get our first connection. At degree 1, that is when every node has on average one connection the network stars to appear connected, we see one giant component emerging within the network, that is one dominant cluster and we start to have cycles, which means there are feedback loops in the network. Another threshold occurs when nodes have an average degree of log(n) at this point everything starts to be connected meaning there is typically a path to all other nodes in the network. So this is what we see in random network but as we know most real world networks are not random as they are subject to some resource constraints and they have preferential attachment giving them clusters that we do not see in these random graphs. 

One way of thinking about how real world networks form is through the lens of peculation theory, percolation theory looks at how something filters or peculates through something else like a liquid filtering through some mash structure in a material or we might think about some water running down the side of the hill, as it does the water will find the path of least resistance creating channels and furrows in the side of the hill. This network formation is then the product of the resource constraints that its environment placed upon it, but the constrains are unevenly distributed and the networks topology is then reflecting this as it follows the paths of least resistance avoiding the toughest material. 

In order to demonstrate the general relevance of this we will take some other examples, if say we put on cheap flights from one city to another then people will start using this transportation link because of financial constraints. Or because of phenomena of Homophily within social networks we will get the same peculation dynamics where it will be easier for people to make links with people who are similar to themselves than with others, again creating a particular structure based on the social constraints within the system. 

To incorporate this into our model we would need to add in the attributes or properties of the nodes within the network, where they will form links depending upon these properties, so in a social network these attributes might be age, gender or income and people will have a preferential attachment. We would then create a probability for the likelihood that any node will connect with another of the same kind compared to whether it will make a connection with a node of a different kind. 

The result of including these factors into our representation would be a much more realistic model, where we see local clustering and some distant relations. Behind this relatively abstract model to network development presented here is a much more complex set of questions about the local incentives of the nodes in the network. Another way of looking at network formation within this model is from the perspective of the nodes and the local rules they are operating under, to do this we might use game theory that looks at the incentives of the individuals within the network and their payoff for forming a connection or breaking one. 

Another key driver behind the formation of real world networks is the so-called network effect and Metcalfe's law, which states that the value of a network grows as a square of the number of the nodes in the network. So this goes back to one of our earlier insights that where as the number of nodes in a network may only grow in a linear fashion the number of edges may grow exponential. The network effect arises when users derive value from the presence of other users, with the telephone being a classical example: the more people that join the network, the more valuable it becomes. Thus, there is a positive feedback loop where more people joining feeds back to make the network more valuable that in tern draws more people and so on, in this way we can get the exponential grow that we have seen in the rise of many network organizations like the Twitter and Facebook. 

We might note the development of these networks is nonlinear with distinct tipping points, because it requires a critical mass of users for something like a computer operating system to have any value, but once you have gone beyond that critical mass it then become very valuable because you are able to now interoperate with all these other users. This is why free is a good marketing strategy for some I.T. start ups, because it is all about getting this critical mass, once you have it then the network effect kicks in and it become a “must have”. 

The dynamics of how real world networks are formed, strengthened and eventually dissolved, is complex and still a very open area of research where lots of basic questions remain unanswered. In this section, we have just touched upon some of the main features to this process to highlight the fact that although most of our models are static, real world networks anything but. 

Diffusion & Contagion

How something spreads across a network is a key question we will be interested in asking when analyzing many different networks. The classical example of this being the diffusion of a disease through some population. But, we might be talking about how the loss of one species in an ecosystem has an effect on others, the spread of financial contagion from one institution to another, or the spread of some information within a group of people. More formally, we call this spreading on a network propagation or diffusion, how diffusion happens and how long it takes is defined by a number of different parameters. We will just list the primary factors involved here before looking into them individually. 

The first factor is infectiousness, where we are talking about how infectious the phenomena that is spreading on the network is. A corollary to this is asking how resistant the nodes are to this contagion giving us a resistance parameter. Next, we need to consider the topology to the network, obviously this diffusion is taking place along the connections within the network, meaning different structures to the connections and different degree distributions will be another defining factor when considering diffusion. Lastly, we need to consider if this diffusion is taking place strategically or at random and this ties back to topology because as we have already discussed some network topologies are more susceptible to strategic influence then others. 

By infectiousness we mean something that is likely to spread or influence others in a rapid manner, irrespective of the type of network it is spreading on. If you hear about some important piece of news, you feel driven to tell others and that is infectiousness, it is like an outward force that is pushing the phenomena across local connections and out over the network. We may be able to quantify this in terms of money or how contagious a disease is or a number of other metrics but we also need to ask, how many nodes a given node can infect in any given time interval? A mosquito can only bite one other creature at a time but a person can broadcast a message to possibly millions of other people at any given instance, thus enabling a much more rapid contagion rate.

Inversely, we need to think about how resistant the nodes in the network are to the spreading of this phenomenon. Imagine trying to promote gay mirage in some conservator rural community, no matter how infectious your campaign is it is unlikely to takeoff and this is due not to your failures but to the resistance of the other nodes in the network to this phenomena. We could also add time to our model here, capturing how nodes may be affected for only a brief period of time before recovery, as would be the case with the spread of many diseases or some trend in fashion. 

Next, we need to look at the topology to the network to understand how something is likely to spread across it. The primary factor here is simply the overall degree of connectivity to the network. Obviously, the more connected it is, the faster something should spread across it. However, we would need to look at the average shortest path to get an idea of how many edges any phenomenon would have to travers in order to effect the whole system. 

We also need to analyze the degree distribution to understand how centralized the network is, as major hubs can enable rapid local and global diffusion. For example, modern broadcast media has arisen hand-in-hand with the modern nation state, as it is only through these centralized hubs that uniform information can be rapidly disseminated to a large population and thus a key component in creating a sense of nation culture and cohesion, without these centralized hubs diffusion can be a lot slower and become heterogeneous. 

We also need to ask whether this dissemination is random or strategic. That is whether there is some logic behind the promotion and dissemination aimed at strategically effecting nodes that have a high degree of connectivity and thus enabling a more rapid diffusion. Many forms of diffusion can be modeled as random, a virus has no logic telling it to attack creatures that have lots of physical contact with others, we might say the same of financial contagion, toxic assets don’t themselves choose where to end up in the network and which nodes to effect most, these are factors that are defined by other dynamics. But some diffusion processes are strategic, for example, military strategy is often specifically designed to attack a critical node in the oppositions military or infrastructure network in the hope that this shock to a critical node will then propagate to its many dependent nodes and thus have a greater effect than simply choosing to attach any node at random. 

Lastly, we will just touch upon the topic of complex contagion, the simple contagion model we have been describing so far is essentially binary, meaning either a node is effected or not and within this model all that matters is whether one other node effects it or not. Complex contagion in contrary is the process in which multiple sources of exposure to a phenomena are required before an individual adopts the change in its state. An example of this might be the adoption of some new technology or innovation which is costly, especially for early adopters but less so for those who wait, we can then model this as a form of complex contagion, asking how many other nodes need to adopt the innovation before a given node will do like wise. 

There might also be two competing events propagating across the network. For example, if we tried to model how an individual will vote for two different candidates in an election based upon the social network they are a part of, we would then be defining some variable as to how many of the nodes neighbors need to vote for a particular party before they would cast their vote for the same party. 

These complex models have many interacting parts, thus there will be tipping points as a node will not do anything until a threshold value is met and there is feedback as when the node changes its state it will affect the choices of others around it also. All of this means that this more complex form of contagion is nonlinear with the possibility of exponential cascades forming. 

We have only just begun to touch upon the most salient metrics effecting network diffusion. Real-world diffusion across something like a social network is a complex process that may require multiplex network models, which is allowing our models to have multiple different connections between nodes, in order to capture how different types of connections and networks interact to enable or resist the diffusion of some phenomena. So, within our voting example we might have to take into account economic factors and relations in order to capture the true dynamics at play.

Robustness 

Robustness & resilience are often thought of in terms of a systems capacity to maintain functionality in the face of external perturbations. We see some extraordinary examples of this, ecological networks that persist despite extreme environmental changes. Communication networks like the Internet can often deal with malfunction, errors and attacks, without these local events leading to catastrophic global failures, but we also see the opposite where some small failure in say a financial system can propagate to affect the whole system. Trying to understand how and why this happens is the study of network robustness. 

Robustness can be correlated with connectivity in that connectivity enables system integration, as we previously noted without connectivity parts to the system may become disconnected and disintegrated. If blood stops flowing to some part of the body then it will become atrophied and waste away or if a child stops talking to their parents then the family unit disintegrates through lack of communications. Thus when we are talking about robustness & resilience we are often asking what will happen to the networks overall connectivity and integration if we remove some components or connections and equally how will this failure then spread within the network system. 

We can think about failure either with respect to the networks nodes asking what will happen if we remove a certain amount, this is called node percolation, but we can also talk about failure in terms of the removal of edges, that is edge percolation and another key factor here is whether the attack is random or strategic. When we are talking about robustness with respect to the nodes in the network then a key factor is the degree distribution between the nodes, the higher that degree distribution, meaning there will be more hubs, the more vulnerable it is to strategic attach, but if it is a random attack then the degree distribution is not so important, as these hubs that are part of centralized networks will be particularly vulnerable to strategic attack. 

As we have previously noted distributed networks will be robust to strategic attack but scale free centralize networks are particularly susceptible. A strategic attack on large hubs will drastically reduce the number of connections within the system increase the average path length significantly, which is a key measure of the systems overall integration. This has been confirmed in empirical data from the Internet and World Wide Web which show robustness to random attack but are significantly effected by strategic attract due to the presence of major hubs in the network. 

Next edge percolating; that is when connections fail or are removed, an important factor here is the degree of betweenness to the network, if we remember betweenness is really measuring the number of bridging edges, that represent the critical, irreplaceable connections between one cluster and other. An example of this might be the Malacca Straits, a stretch of sea between the cost of Malaysia and Indonesia that connects maritime transport in Asia with the Middle East and Europe. Approximately 40 percent of the world's trade and 25 percent of all crude petroleum is thought to pass through this critical link in the global logistics network. This is an example of a bridging link that reduces the systems robustness and makes it much more susceptible to strategic attack. 

Robustness does not just depend on if a node or edge will fail, but just as importantly what will happen when it fails, in other words will the failure end there, or will it have a cascading effect as may often be the case. For example, failure in a power grid can result in other power stations becoming overload allowing for the failure to propagate. When we are modeling robustness in terms of some external perturbation that propagates through the system, destroying links and nodes on its way, we want to ask how easily does it spread and what is the resistance to its spreading within the network. One method for preventing failure propagation is through buffers and redundancy. 

When we engineer networks these buffers are often artificially super-imposed upon the network, but when we look at the robustness in ecosystems it is built into these networks in the form of diversity, diversity is both a buffer in that the difference between nodes will present a barrier to complete contagion and it is a form of redundancy as components are also similar, they can to a certain extent just replace other components. 

This diversity in connections often comes in the form of weak ties, where the connections in a network can be divided into strong or weak, which are really a way of defining connections in terms of their frequency of interaction. So. in a social network, a strong tie is someone you interact with on a daily basis thus strong ties are often between members of a cluster or clique. 

A weak tie might be with someone you only talk to once every few months or once a year. Whereas strong ties may dominate a network in terms of quantity, weak ties are important in that they connect nodes from different clusters making them bridges that add some diversity to the network which can be vital to its robustness if some subset of a network becomes infected. We could cite the phenomena of group think here, where a small social network or clique, often quickly converges to an agreed opinion on a subject without full consideration of different opinions. However, by having some external consultant join the group, they would add a weak tie to add a diverse perspective and resist group thing from prevailing. 

We have been talking about static models to robustness and failure propagation, but of course many real world networks are more dynamic than this, where the network can adapt to some external perturbation. For example, the Internet routers have routing tables that keep track of how effective any path through the network is and then update where it sends new packets based upon it. If we remove some set of edges it will dynamically reroute packets to try and maintain network functionality and this would be the same for a logistic network, a criminal network or any other network with some kind of control system that allows it to adapt. 

We will conclude here by making one last point, that is that connectivity is a double edged sword with respect to robustness. Connections enable integration and this is a key source of robustness as it bonds the systems components together, but connectivity can also be a pathway for disaster spreading. The most recent financial crisis might be a good example of this, where unknown linkages between complex financial instruments and institutions lead to rapid contagion. 

Thus, we need to be aware that every link in the network has a cost in terms of robustness, if it is not contributing to the systems integration and robustness then it may be depleting from it as just another pathway for contagion spreading. We should be under no dilution that connectivity is always in some way a good thing, research has shown that within certain settings connectivity can, up to a certain point adds to the systems robustness but beyond this hyper-connectivity can just be adding pathways for failures to propagate and overall fragility.

Conclusion

This book was designed to give an intergrade overview to the area of network theory, through the developing of a “bigger picture“ composed of a set of major themes we have tried to show how the different areas fit together. The hope is that through the development of these major themes you have gained some insight into what we called the network paradigm. Seeing the world through the lenses of connectivity, can give us a new perspective to better understand phenomena that are difficult to approach from a more traditional component based approach.

Network theory is a formal language that supports what is called network science, one of the most active and fastest growing areas of contemporary science. The area of network science then applies the formal language of networks that we have been looking during this book to many different areas that we have not had a change to touch on, for example the study of urban environments, looking at how networks and connectivity can work to connect people and integrated the social fabric, or looking at how peoples social network effect their behavior so as to better understand how something like obesity spreads. 

Today there are lots of open source free software packages for network analyses along with a growing availability of data sets online making the area of network science over more accessible.