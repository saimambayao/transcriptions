
**Complex Adaptive Systems**

Complex Adaptive Systems: An Overview
Joss Colchester 2016

**Contents**
* Overview
    * Complexity Theory
    * Complex Systems
    * Complex Adaptive Systems
* Adaptive Systems
    * Adaptive Systems
    * Control Systems
    * Schemata
* Simple Rules
    * Simple Rules - Complex Behavior
    * Feedback Loops
    * Cellular Automata
    * Competition & Cooperation
* Edge of Chaos
    * Self-Organization
    * Self-Organization Far-From-Equilibrium
    * Robustness
* Evolution
    * Robustness
    * Dynamics of Evolution
    * Fitness Landscapes

**Forward**

Complex adaptive systems are all around us from financial markets, to ecosystems, to the human immune system and even civilization itself. They consist of many agents that are acting and reacting to each other’s behavior, out of this often chaotic set of interactions emerges global patterns of organization in a dynamic world of constant change and evolution where nothing is fixed. In these complex adaptive systems no one is in control, no one has complete information, patterns of order emerge through self-organization between agents.

Individual cells self-organize to form differentiated body organs, ants interact and self-organize to form colonies, and people interact to form social networks. These patterns of global organization emerge out of a dynamic between order and chaos that we are only just beginning to understand but as we do we are finding that these apparently very dissimilar systems share fundamental commonalities.

The aim of this book is to give a comprehensive, clear and accessible outline to the new area of complex adaptive systems that is finding application in many areas. We will be covering all the main topics within this domain, as we start by talking about adaptation itself where we will be discussing cybernetics and looking at how systems regulate themselves to respond to change.

We’ll go on to talk about the dynamics of cooperation and competition, looking at how and why agents work together to create local patterns of organization. Next we will be talking about the process of self-organization and asking the big questions about how do we get and sustain ordered patterns out of randomness and chaos? Lastly we will be looking at the process of evolution as a powerful and relentless force that shapes complex adaptive systems on the macro scale and we will be taking it apart to get a solid grasp of its basic functioning.

This book requires no prior specific knowledge of mathematics or science, as it is designed as an introduction presenting concepts in a nonmathematical and intuitive form that should be accessible to anyone with an interest in the subject.

**Complexity Theory Overview**

In this section, we will be giving an overview to the areas of complexity theory by looking at the major theoretical frameworks that are considered to form part of it and contribute to the study of complex systems. We will be briefly talking about systems theory, network theory, chaos theory and adaptive systems theory, before trying to provide an overview to the context and significance of this area in relation to contemporary science.

Complexity theory is a set of theoretical frameworks used for modeling and analyzing complex systems within a variety of domains. Complexity has proven to be a fundamental feature to our world that is not amenable to our traditional methods of modern science, and thus as researchers have encountered it within many different areas from computer science to ecology to engineering they have had to develop new sets of models and methods for approaching it.

Out of these different frameworks has emerged a core set of commonalities that over the past few decades has come to be recognized as a generic framework for studying complex systems in the abstract. Complexity theory encompasses a very broad and very diverse set of models and methods, as yet there is no proper formulation to structure and give definition to this framework, thus we will present it as a composite of four main areas that encompasses the different major perspective on complex systems and how to best interpret them.

Let’s focus on systems theory firstly. Systems theory is in many ways the mother of complexity theory. Before there was complexity theory, systems theory was dealing with the ideas of complexity, self-organization, adaptation and so on. Almost all interpretations to complexity depend upon the concept of a system. In the same way that modern science can be formalized within the formal language of mathematics, all of complex systems science can be formalized within the language of systems theory but, systems theory is a very abstract and powerful formal language and it is typically too abstract for most people and thus is understood and used relatively little.

Cybernetics is another closely related area of systems theory. It was also part in forming the foundation to complexity theory, cybernetics during the mid to late 20th century studied control systems and provided a lot of the theoretical background to modern computing. Thus, we can see how the interplay between computing and complexity science goes all the way back to its origins as the two have developed hand-in-hand. A lot of systems theory is associated with and has come out of the whole area of computation.

The areas of computer science and its counterpart information theory have continued to be one of the few major contributors to complexity theory in many different ways, though systems theory is about much more than just computers it is a fully fledged formal language.

Next, we move on to nonlinear systems and chaos theory. Nonlinearity is an inherent feature and major theme that crosses all areas of complex systems. A lot of nonlinear systems theory has its origins in quite dense and obscure mathematics and physics. Out of the study of certain types of equations, weather patterns, fluid dynamics and particular chemical reactions has emerged some very counter intuitive phenomena in the form of the butterfly effect and chaos. Chaos theory, which is the study of nonlinear dynamical systems, was one of the first major challenges to the Newtonian paradigm that was except into the mainstream body of scientific knowledge.

Our modern scientific framework is based upon linear systems theory and this places significant constrains upon it. Linear systems theory is dependent upon the concept of a system having an equilibrium. Although linear systems theory often works as an approximation, the fact is that many of the phenomena we are interested in describing are nonlinear and process of change such as regime shifts within ecosystems and society, happen far-from-equilibrium they are governed by the dynamics of feedback loops and not linear equations.

Trying to model complex systems by using traditional linear systems theory is like trying to put a screw into a piece of wood with a hammer, we are simply using the wrong tool because it is the only one we have. Thus, the areas of nonlinear systems and their dynamics is another major part to the framework of complexity theory that has come largely from physics, mathematics and the study of far-from-equilibrium processes in chemistry.

Then there is network theory. Network theory is another major area to complexity theory. As almost all complex systems can be understood and modeled effectively as networks, like systems theory network theory is another formal language but it is a much more practical tool for analysis and thus has found widespread application in many areas. The study of networks is probably the youngest and most active area of complexity science again driven by the rise of computation and the fundamental role that networks are starting to play in our world with the advent of information technology.

With the theory of networks and the availability of new sources of data we are starting to get a real picture to what some of these complex systems that make up our world actually look like. We can start to see the connections within financial systems through which contagion spreads, the real-time movement of freight around the globe or the socio-political networks that influence our lives. This is a new kind of science driven less by models and equations but more by real-time dense data sets. This means we are no long left staring at models but now have accessible visualizations to give us a much more richer, intuitive and in many ways real sense for what exactly these complex systems are like. The main contributions to this area have come from the area of mathematics called graph theory and again computer science.

The lastly major area to complexity theory that we will discuss is that of complex adaptive systems and self-organization. Complex adaptive systems are classical examples of complex systems and people often use the two words somewhat interchangeably. They consist of many parts acting and reacting to each other’s behavior, like a school of fish swing together, nation states within the international politics environment or businesses in a market, they are highly dynamic and develop through an evolutionary like process. Self-organization theory tackles one of the main themes within complexity theory, which is how things work together, how differentiated components become integrate into a coherent functioning organization without centralized coordination.

Here we are looking at how agents governed by simple rules, synchronize their behavior with the result being a process of self-organization as patterns of organization emerge from the bottom-up. Researchers try to model complex adaptive systems by capturing these local rules and using computational tools like cellular automata and agent based modeling to try and simulate how these systems are shaped by their interactions and evolutionary forces. This is an area that has grown out of cybernetics, computer science and with major contributions also coming from ecology.

Finally, we might discuss a little the context and significance of the area of complexity theory as it plays a somewhat unique role within the framework of contemporary science. The website Scholarpedia describes complexity theory as an emerging post-Newtonian paradigm. There is a lot packed into this short statement so lets try and unravel and make sense of it. The Newtonian framework is based on linear systems theory, this has been a powerful tool for helping us understand the world, through the contributions of millions of researchers over the course of centuries we have built-up a large and sophisticated body of scientific knowledge which is one of humanities greatest achievements.

Throughout the 20th Century though the Newtonian paradigm and linear systems theory has become increasingly called into question as general relativity and then chaos theory proved some of its most basic assumptions to be in fact floored. The fact is that much of the phenomena that we are really interested in are inherently nonlinear such as almost all social-political, ecological and economic phenomena. A core challenge of 21st Century science then is to extend this framework into the world of nonlinear systems and this means going beyond the Newtonian framework, as Scollarpedia puts it developing a post-Newtonian paradigm and this is exactly what complexity theory is doing.

To summarize we have been giving a quick overview to the area of complexity theory, which we defined as a set of theoretical frameworks used for modeling and analyzing complex systems within a variety of domains. We looked at four of the major modeling frameworks that fall under its canopy. Starting with systems theory that is in many ways the foundations and origins of complexity theory, we talked a bit about the theory of nonlinear systems and how it has emerged out of the study of chaotic physical and chemical processes and also mathematic equations.

We then discussed network theory as another major domain, which understands complex systems in terms of connectivity and how things flow through these systems. The last major area we looked at was the theory of complex adaptive systems and self-organization, which tries to understand complex systems in terms of the interaction between agents governed by simple rules and the patterns of organization that can emerge through self-organization. Finally, we tried to provide some insight to the significance and context of complexity theory as a so-called post-Newtonian paradigm as it tries to extend our scientific body of knowledge into the world of nonlinear systems.

**Complex System**

In this section, we will be trying to define what exactly a complex system is, we will firstly talk about systems in general before going on to look at complexity as a product of a number of different parameters, where we will be discussing system’s hierarchy, nonlinearity, connectivity, adaptation and self-organization. There is no formal definition for what a complex system is and thus there remains many different perspective and opinions on the subject. Therefore, it might be of some value to us to start off by taking a quick look at some of these different definitions to get an idea for some of their commonalities.

Firstly, the Advances in Complex Systems Journal, gives us this definition, "A system comprised of a (usually large) number of (usually strongly) interacting entities, processes, or agents, the understanding of which requires the development, or the use of, new scientific tools, nonlinear models, out-of equilibrium descriptions and computer simulations."

Next, the social scientist Herbert Simons gives us this definition, "A system that can be analyzed into many components having relatively many relations among them, so that the behavior of each component depends on the behavior of others." Jerome Singer tells us that a complex system is, "A system that involves numerous interacting agents whose aggregate behaviors are to be understood. Such aggregate activity is nonlinear, hence it cannot simply be derived from summation of individual components behavior."

Then, a complex system is a special class of system. A system is simply a set of parts called elements and a set of connections between these parts called relations. These parts can be ordered or unordered, an unordered system is simply a set of things, because there is not specific structure or order we can describe a set by simply listing all of its elements and their properties. So a pile of stones on the ground is an example of an unordered set, as there is no pattern or order to the system we can only describe it by describing the properties of each element in isolation and then adding them all up, with the whole set being nothing more than the sum of its individual parts.

If in contrast, through the relations, these parts are ordered in a specific way, they can function together as an entirety. If out of these parts working together, we get the emergence of a global pattern of organization that is capable of functioning as a coherent whole. For example, if all the parts in our car are arranged in a specific way, then we will have the global functionality of a vehicle of transportation or, out of the specific arrangement of billions of cells and the different specialized organs that make up our body we get the emergence of a global system that enables us to operate as an entire organism. So, that is the basic model of a system: it consists of elements and relations, when those elements work together we get the emergence of a new level of organization.

Now let’s start adding complexity to this. Probably the only property that will be in all definitions of a complex system is that they consist of many parts, meaning many elements interacting on many different levels. With the phenomenon of emergence that we were previously discussing, a whole new level to the system has developed, which then starts to interact with other systems in its environment. The result is that new patterns of organization develop and once again we get the emergence of another level of organization and so on.

People form part of social groups that form part of broader society which in turn forms part of humanity. The point to take away here is that these systems have a hierarchical structure. This is a pervasive phenomenon in our world, elements are nested inside of subsystems, which in turn form part of larger systems and so on. All complex systems have this multidimensional property to them, they are composed of many elements on many different scales, with all of these levels effecting each other. A business is part of a local economy, which is part of a national economy, which in turn is part of a global economy. Each is interconnect and interdependent with the others, we cannot fully isolate one component or reduce the whole thing to one level and this is a primary source of complexity. If you can place yourself into one of these systems with all of these parts interacting on many different levels then you should have a good sense for why they are called complex. So this is our first property to a complex system, many different elements interacting on many different scales.

Let’s focus on nonlinearity: almost all well-formulated definitions for complex systems involve the term nonlinearity. It is a continuously reoccurring and pervasive theme. Nonlinearity in its most basic and intuitive sense describes how the input and output to a system are not proportional to each other. Nonlinearity arises from the fact that when we put two or more things together the result may not necessarily be a simple addition of each elements properties in isolation, in contrary we can get a combined effect that is greater or less than the simple sum of each part. Examples of this might be two sound waves that are perfectly out of synch canceling each other out through noise interference or the division of labor as can be seen in many human and insect communities, results in synergies that mean the output will be far greater than what the individuals could accomplish in isolation.

Due to what are called feedback loops nonlinear systems may grow or decay at an exponential rate, these periods of rapid change are defined as phase transitions. Thus, complex systems are known to be able to shift or flip into whole new regimes within very brief periods of time. Some small change in input value to the system can through feedback loops trigger a large systemic effect. Examples of this can be seen in financial crisis and the claps of ecosystems such as coral reefs. Nonlinearity is in many ways an expression of the deep interdependent nature to complex systems.

And now let’s talk about connectivity: many definitions for complex systems involve dense or high levels of interconnectivity between components, as we turn up the degree of connectivity it becomes the nature and structure to these connections that define the system as opposed to the properties of its components, how things are connected and what is connected to what becomes the main question. At some critical level of connectivity the system stops being a set of parts and becomes a network of connection and it is now all about how things flow in this network.

Networks are the true geometry of complex systems. These systems do not operate in a three-dimensional Euclidean geometry whether we are talking about the global air transportation system the flow of financial capital or information on the Internet, space is redefined in terms of the topology created by connectivity. What matters is your position in the network structure and you degree of connectivity. Connectivity leads us into the world of complexity as the number of relations between elements can grow at an exponential fashion. If we take just a handful of elements they can be connected in possibly thousands or even millions of different ways.

Lastly, let’s study autonomy and adaptation: Whether we are talking about a flock of birds, the Internet, or our global economy, there is no up-down centralized mechanism for coordinating the whole system. Within complex systems elements have a degree of autonomy often through their capacity to adapt to their local environment according to their own set of instructions.

Without centralized coordination and with a degree of autonym comes the capacity for elements to self-organize, they can synchronize their states or cooperate resulting in the emergence of patterns of organization from the bottom up. With autonomy and adaptation also comes the capacity for a variety of different responses for any given phenomena meaning complex systems are often heterogeneous with high levels of diversity, ecosystems and multi-cultural societies are good example of this.

Without centralized coordination, complex systems develop on the macro scale through a process of evolution. Elements within complex adaptive systems are subject to the evolutionary force of selection where those that are best suited to that environment are selected and replicated while others are not. Products are subjected to selection within a market environment, in democracies politicians are subject to selection by voters and creatures in ecosystems are subjected to natural selection through competition, in such a way the whole macro scale system manages to adapt to its environment without centralized coordination and develop to exhibit higher levels of both differentiation and integration. The greater the autonomy and capacity for adaptation that elements have the more complex the system we are dealing with.

In summary, we have been trying to lay down a basic working definition for a complex system while remembering that there is no formal consensus on the subject. We first talked about how a complex system is a special class of system. We defined a system as a set of elements and the relations between them. We saw how when these parts are arrange in a specific order for them to function as an entirety we get what is called the process of emergence, where by a new level of organization emerges.

We then began to add complexity to our model of a system by defining it as a product of four primary parameters. Firstly talking about the number of elements and different levels to the hierarchy within our system. We then discussed nonlinearity as another dimension to complexity, where non additive interactions and feedback loops over time, can give us exponential relations between the input and output to a system and lead to phase transitions. We also talked about connectivity as another drive of complexity as heightened connectivity within complex systems means they often appear to us as networks. Lastly we discussed how autonomy and adaptation enables self-organization and the process of evolution that shapes complex systems on the macro scale.

**Complex Adaptive Systems**

In this section, we will be giving an overview to complex adaptive systems. We will firstly define what we mean by this term, before briefly covering the main topics in this area as we talk about adaptation, the dynamics of cooperation and competition, self organization and emergence. Finally, we will look at the complex interplay between the micro and macro patterns of organization that is a core feature to these systems.

A complex adaptive system is a special class of complex system that has the capacity for adaptation. Thus, like all complex systems they consist of many elements, what are called agents, with these agents interacting in a nonlinear fashion creating a network of connections within which agents are acting and reacting to each other’s behavior. Through adaptation agents have the capacity to synchronize their states or activities with other agents locally, out of these local interactions the system can self-organize with the emergence of globally coherent patterns of organization developing.

This macro scale organization then feeds back to the micro level, as the system has to perform selection upon the agents based upon their contribution to the whole system’s functioning. And thus there develops a complex dynamic between the bottom up motives of the individual agents and the top down macro scale system of organizations, both of which are often driven by different agendas but are ultimately interdependent. It is this interaction between bottom-up differentiation of agents with different agendas going in different directions and top-down integration in order to maintain the global pattern of organization that creates the core dynamic of complexity within these systems. This is a lot of very dense information so we will now try to flesh it out in greater detail through examples.

There are many examples of complex adaptive systems from ant colonies to financial market to the human immune system, to democracies and all types of ecosystems, but we will start on the micro level by talking about the agents and adaptation. An agent is an actor that has the capacity to adapt their state, meaning that given some change within their environment they can in response adjust their own state, so say our agent is a player within a sports game, well if we throw a ball to the person he or she can catch that ball. They are able to do this because they have what is called a regulatory or control system, a control system of this kind consist of a sensor, controller and an actuator, the person is using their optical sense to input information to their brain, the controller, that is then sending out a response to their mussels, the actuator, and through this process they can adjust to generate the appropriate response to this change in their environment. And it is this same process through which a bird in an ecosystem or a trader within a market is receiving information, processing it and generating a response. Typically these agents can only intercept and process a limited amount of local information, like a snail following a trail on the ground it does not have a global vision of the whole terrain around it and it must simply respond to the local information available to it.

With this capacity of adaptation agents have some degree of autonomy through which they can choose to synchronize or desynchronize their state with that of other agents within their local environment, we might also call this cooperation or competition. They typically do this based upon the costs and pay-offs for choosing one of either option, and this cost benefit ratio varies depending on the scenario or what we might call the game they are engaged in with other agents. Some scenarios such as playing chess have very low incentives for cooperation while favoring competition, these are called zero-sum-games, while other scenarios have a much lower cost and higher pay-off for cooperation such as driving your car on the correct side of the road, these different types of games create attractors that result in default positions for agents to cooperate or compete.

Added to this are feedback loops, where what one agent does influences what another chooses to do. If you owned a certain stock and upon hearing some negative news about that company all of your fellow traders around you started selling it this would create a positive feedback loop attracting you to also sell. If you did that would again amplify the positive feedback placing a stronger attraction on others to also do likewise, in such a fashion some phenomena can cascade through a population synchronizing their states rapidly.

This process previously describe is a form of what is called self-organization, from the interaction of the individual agents arises some kind of global pattern which typically could not have been predicted from the behavior of the agents in isolation. For example, in the brain, consciousness is an emergent phenomenon, which comes from the interaction between the brain cells thus the global property of consciousness results from the aggregate behavior of individual elements. Within this macro scale system that emerges control and regulation is typically distributed out, there is no master neuron or set of neurons that tell the whole brain what to do, no one is in control and no one in the system has complete information of it.

This distributed nature to complex adaptive systems may make them very robust, where the system can adapt to some large disturbance. The Internet might be an example of this, dynamically updated routing tables keep track of how long it takes to send information along any path on the network, if there is a failure in one part of the network packets are rerouted though another channel, control over the flow of I.P. packets is distributed out over many different routers and service providers, with a large amount of redundancy making it robust to failure. But equally complex adaptive systems can self-organize into a critical state where feedback loops can work to amplify some small perturbation into a large systemic effect as witnessed during financial crisis.

This emergent macro scale system of organization then operates within some environment. Whether we are talking about a herd of animals within an ecosystem, the human body, a democracy or a corporation within a market, the whole macro system is periodically subject to perturbations and change within that environment. In order for it to optimize its state there must be some mechanism for performing selection upon the agents within the system. Those creatures within an ecosystem that can best respond to the environment are replicated. Those employees that have proven their value to the company will be promoted while others will be fired. Those products that best fulfill the demand are selected by the consumer, while others go by the wayside. The result being that the whole system evolves to exhibit more of the desired characteristics as they become more prevalent with the system.

In this way, this global pattern of organization will feedback to affect the agents on the local level both enabling them and constraining them. It enables them as it is a mechanism for them to coordinate their activities and thus receive the benefits from forming part of a complex organization, in the form of security, shared knowledge, technology, and so on. But, it will also constrain them as following regulations and being subject to some form of selection is part of maintaining this global organization. Of course agents have their own agendas that may or may not be aligned with those of the whole system, and this is where the real complexity comes into the dynamic, as there is now a core tension between the micro and macro levels.

The system as a whole, which is how it appears within its environment, will be primarily defined by how this core tension is resolved. That is to say, is the system driven by the interests of the agents at the expense of the whole? Or by the interest of the whole at the expense of the interests of the individuals? Or has it manage to find some resolution to this conflict? If we take an example of an economy, we can have a free market economy which is driven primarily by the interest of the agents in a bottom-up fashion, or we might have a communist economy driven by a top down dynamic at the expense of individual motives or we may have some economic system that manages to integrate the two.

In this section, we have tried to present an overview to complex adaptive systems, we have discussed some of their core dynamics by looking at the capacity of adaptation, how feedback loops and attractors can work to synchronize elements, we have talked about how self-organization can give rise to the emergence of distributed global patterns of organization. Lastly, we saw how through the process of evolution this macro system of organization can feedback to effect the agents and how a new dynamic emerges between the motives of the individual agents on the micro level and this new macro level form of organization.

**Adaptive Systems Overview**

In this section, we are going to cover the basics of adaptive systems by firstly defining what they are, we will then illustrate the concept by discussing a number of examples, we will go on to talk about agency and how the capacity for adaptation gives rise to autonomy and diversity. Firstly, we can define adaptation as the capacity for a system to change its state in response to some change within its environment. An adaptive system then is a system that can change given some external perturbation and this is done in order to optimize or maintain its condition within an environment by modifying its state.

One of the simplest examples of an adaptive system might be a mousetrap, it is designed to respond to some perturbation that triggers a mechanical reaction within the system and it is simply executing on a logic that was built into its mechanical structure by design. The growth of a plant or fungus towards a source of light, what is called phototropism, is another example of adaptation, the cells on the plant that are farthest from the light release a chemical causing them to elongated and thus move the plant towards the light source. This adaptability gives the organism some flexibility that improves its performance and chances of survival. By flexibility we simply mean it can generate an optimal response to a limited set of changes within the state of its environment.

In both of these previous examples the capacity for adaptation is simply embedded within the physical mechanics of the system, but entities that are capable of more advanced forms of adaption have specialized subsystems dedicated to regulating this process of adaption, we call these specialized components regulatory or control systems. For example an animal like a cat has a nervous system dedicated to sensing, processing and responding to information it receives from its environment.

With an electrical nervous system the creature is able to respond very rapidly and also capable of a much more complex algorithmic processing of information. With this control system it is able to generate a wide variety of response to deal with a rapidly changing environment where it may be presented with a large number of different scenarios as a creature like our cat might encounter during the dynamic activity of hunting.

Beyond this type of algorithmic logic that governs basic control systems, adaptive systems can have a much more complex conceptual framework for representing their environment and we call this a schema. Schemata, which is plural for schema, are mental frameworks or concepts used to organize and structure information. With schemata an adaptive system has a full model for classifying and correlating different information about its environment, which can then be used to interpret new information, learn and generate novel responses to a very wide verity of input stimulus.

Within complex adaptive systems theory these adaptive systems are called agents and they are so called because they have agency, agency can be defined as an action or intervention designed to produce a particular effect. An agent then is an entity that takes an active role to produce a specific outcome, thus agents do not act in a random fashion but actions are performed in order to produce a particular effect. That is to say all adaptive systems have a goal, whether we are talking about a plant that adapts its state by moving towards the direction of the sunlight or a trader who buys a particular security to diversify her portfolio, these agents are acting according to a set of rules that are specifically formulated or designed to achieve a desired outcome, although these desired outcomes may be very diverse from the plant requiring more sunlight to the trader wanting more capital.

Adaptive systems have a particular internal order or structure that enables them to intercept and transform energy and resources of some kind. We may be talking about our plant intercepting photons, combining and transforming them into sugars through the process of photosynthesis or we may be talking about a business within an economy that takes in some input and transforms it to an out put that generates revenue. The aim or goal then of an adaptive system with agency is to maintain and develop this internal order and capacity to process resources, they can only do this by importing energy or resources and exporting entropy to and from their environment. In other words, these systems are dependent upon their environment to ensure their continued functioning and they adapt in order to maintain and optimize their status within this environment, with this whole process being described as homeostasis.

The capacity for adaptation gives rise to autonym, where as much of our science is focused upon studying deterministic systems where we search for linear cause and effect interactions that govern them and then encode these in mathematical equations as laws of nature. The capacity for adaptation though gives an element a certain capability to act autonomously from these deterministic linear cause and effect laws. We say to a certain extent because the most simple adaptive systems like a thermostat are essentially deterministic. They are determined to respond to some cause in their environment with a given effect, it is only when we have advanced adaptive systems with internal agency that we get autonomy and the capacity for a variety of responses given any cause. The more complex the logic governing the adaptive system is, the more capable it is of producing a variety of responses to any given input and the more it is able to operate sustainably in a broader complex environment.

In this section, we discussed the basics of adaptation, which we defined as the capacity for a system to change its state in response to some change within its environment. We looked at a number of different types of adaptive systems ranging from the simple to the complex. Moreover, we talked about how simple adaptive systems like the phototropic response system of a plant contain their logic embedded in the mechanics of the system but more advanced adaptive systems develop specialized components called control or regulatory systems. Finally, we talked about agency and how the capacity of adaptation can give rise to autonym and variety.

**Control Systems**

In this section, we are going to talk about control systems from the perspective of systems theory and cybernetics. We will define what they are and give some examples before introducing you to the basic components of a control system and its overall workings. Lastly, we will talk about what it means for something to be self-regulated or under control.

Within systems theory, a control system is a specialized sub-system that is designed to monitor and regulate the behavior and operation of the broader system it is a part of in order to maintain its functionality. The primary objective of a control system is to preserve the internal level of order that enables the system to function and develop. This preservation of a stable or equilibrium state to a system’s operation is called homeostasis. On its most basic level, homeostasis is the maintenance of a system within a given set of parameter or environmental conditions that best enable its internal functioning. Homeostasis is untimely at the core of what all types of control systems are designed to do.

Some examples of control systems include: the thermostat that is designed to regulate the temperature of a given system, such as a building within a defined set of parameters. Another example from the human body is the hypothalamus that regulates the autonomic nervous system, as it controls basic body functions such as internal temperature, hunger, sleep and so on. The commander of a military unit is another example: he or she is given the position of supreme control within the organization, responsible for its maintenance and operation. A nation’s government is another example of a control system, designed to maintain and develop the socio-economic system within a given jurisdiction.

All of these very diverse systems share a basic underlining set of relations and components that are common to all regulatory systems. There are essentially just three components to any given control mechanism: first, we need a sensor for feeding information into the system; second, a controller that contains the logic or set of instructions for processing this information; finally, an actuator that executes some action in order to effect the state of the system or its environment. We will go over each of these individually to get a better understanding of them.

A sensor is a component that detects and encodes some stimulus from the system’s environment and transfers it to the controller. Any given sensor can of course only sense a specific stimulus. A sensor has a physical device that is receptive to some change in a parameter that it is measuring, with this change in stimulus then being encoded into information and transferred ultimately to the controller. Examples of this are the visual system within biological organisms where the photons hitting the optical nerves in the retina are encoded into electrical signals to be sent to the brain, or seismometers that sense small movements in the Earth’s surface and encode them into graphical representations.

The controller, the controller is the brains of the operation it contains the critical logic that is governing the whole system and is encoded in some set of instructions. The controller can be modeled as an information-processing unit, taking in some input of information, manipulating this information according to its set of instructions with the result being an output of information that is designed to be acted upon. An example of a controller might be a digital circuit board composed of logic gates that physically manipulate an electrical input according to binary operations to produce some output signal. On this basic level, the set of instructs are what is called an algorithm: by switching gates on and off they can respond to a limited set of input signals through an if/then logic, to create some output response. This set of logic gates is an example of a very simple controller. To take an example of a much more complex controller, we might think about a democratic nation’s public administration system, operating under a set of instructions encoded within a constitution that is designed to take information about the state of the nation. This has been received from a number of different sensors, such as the mass media or statistics gathering, and processed this information according to these set of instructions to produce the policies and regulations required to maintain and develop the socio-economic system of the nation state.

Lastly, the actuator. An actuator is an instrument or set of instruments that act on the instructions produced by the controller. It is designed to physically effect the system that is being regulated in order for it to conform to the instructions produced by the controller. An example of an actuator might be the muscles in the human body: they are controlled by electrical signals sent from the brain, and we can actuate them in order to change the state of our environment by simply moving from one location to another. The brakes on your car are another example of an actuator: they execute or act on your instructions to regulate the speed of the car.

For a system to be regulated or under control means that for any given change in state presented by its environment the system can generate a response so as to maintain functionality. In order for a system to be able to regulate itself, all of these components to a regulatory system need to be working together. Without a sensor, the controller cannot know the state of its environment and thus the appropriate response. If you are driving your car with impaired vision, you are not receiving all the required information about your changing environment that is required to generate the appropriate response. With the result being that sooner or later you will fail to receive critical information that will cause an accident, drastically reducing the systems’ functionality and thus we can say you are not fully in control of the vehicle.

Without a controller, the system cannot alter an input to a required output, thus cannot adapt and will be under the control of external influences within its environment. In order for the system to have control over itself it must have a logical set off instructions that are able to process any input signal to the required output response. There are two key questions to consider here, firstly the basic functioning of the logic unit, are there errors in the instructions and how they are being processed, such as bugs in computer code or random deformations within DNA.

Secondly, does the logic have sufficient variety and complexity to be able to represent all the different states that its environment will present it with? Within cybernetics this is called requisite variety, which simply means that the system is required to have a set of instruction with sufficient variety and complexity to represent all the diverse states within its environment or else it will not be able to regulate itself within that environment. As an example of this we might think of asking a small child to run a multinational corporation: the child simply does not have the conceptual capabilities to represent the complexity of the system it is ask to regulate, thus it is not in control and the system’s functionality will be degraded overtime as it moves outside of some homeostatic parameter that the child is not aware of and thus not able to respond to.

Thirdly, without the functioning of the regulatory system’s actuator, the instructions created by the logic unit cannot be executed upon and thus the system cannot alter its state to respond to the changes in its environment. If a nation’s law enforcement agency refuses to execute on a courts legal decree to disband a popular protest, then the government is essentially out of control, as it no longer has the actuator required to regulate the system.

In this section, we have covered the basics of regulatory systems as understood by systems theory and cybernetics. We started by defining a control mechanism as a specialized sub-system that is designed to monitor and regulate the behavior and operation of the broader system it is a part of in order to maintain homeostasis and functionality in the face of perturbation from its environmental. Next, we talked about the mechanics behind control systems looking at how there are essentially just three components to a regulatory system: a sensor for feeding information into the system; a controller that contains the logic or set of instructions for processing this information; an actuator that executes some action in order to effect the state of the system or its environment. Lastly, we talked about what it is required for a system to be self-regulating, where for any given change in state presented by its environment the system can adapt by generate a response required to maintain itself with its homeostatic parameters.

**Schemata**

In this section, we are going to talk about the internal logic or schema that governs the behavior of agents within complex adaptive systems. This logic can span from the very elementary to the very complex and thus we will break it down into two different types. We will start with the most basic type of logic what are called algorithms and then go on to discuss more advanced conceptual systems, which are called schemata within the language of complex adaptive systems.

The most basic form of logic an agent can have is one that simply responds to a given input signal with an output action that is always the same. For example, if one taps ones knee at the right location it will trigger the nerves to actuate the mussels into generating a sharp reactionary motion, every time we input the same stimulus to this physiological system we will get the same response. This is the most basic algorithm conceivable always mapping the same input to the same output.

More advanced algorithms are able to discern between a given set of inputs and use an if then logic to select an appropriate output. For example, the control system within a chemical processing plant might be able to select from a set of output temperature values based upon any range of input temperature values in order to regulate a chemical process chamber. Another example of this might be the basic algorithm that is thought to govern the flocking of birds, it consists of just three simple rules which are; one separation, meaning always maintain a certain distance from your neighbors; two, alignment, meaning steer towards the average heading of your neighbors; and three cohesion meaning to steer towards average position of neighbors in order to maintain long range attraction. Here the individual bird is continuously inputting a value to these three required parameters, processing this information according to the set of instructions and then selecting from a range of appropriate motion responses in order to maintain its correct positioning.

As advanced as these algorithms may become, they are essentially designed to just generate a response to a given range of stimuli. As such, they capture much of the logic behind mechanical control systems and those governing may biological systems such as in our bird example above, but the advanced cognitive capability of a modern human been far exceed a simple set of algorithms, with this cognitive capacity human agents can create conceptual representation or models of the world and we call these schemata. The word schema comes from the Greek word meaning to shape, or more generally, plan. A schema is a cognitive framework or concept that helps organize and interpret information, as such it is a conceptual template that determines how reality is interpreted and from this what are appropriate responses to a given stimuli.

With a schema, an agent can create a model of what it encounters, identify similarities and differences amongst things in order to create categories and relations between categories. This allows an agent to quickly take in new information and classify it with reference to what it already knows. Every time an agent receives new information, it references it against the information it already has. This process of obtaining new information and filtering it to ensure its validity is often modeled using Bayesian inference. Bayesian inference references any new information received by the agent against prior knowledge in order to ascribe a probability value to the likely hood of its validity. If the information is deemed to have a high probability of validity it is incorporated into the agents’ schema and used as a reference to infer the validity of any future information it receives.

For example, throughout your life you have received constant information endorsing the validity to the existence of the force of gravity. This massive amount of information confirming it, gives it a very high probability of being valid and every day that probability goes up as you receive more confirmation of its existence. As a result, if you are presented with some piece of information that disproves the existence of a gravitational force on planet Earth, your immediate reaction will be to ascribe this new piece of information with a very low probability of being valid. In this way, a schema can develop as it receives new information and incorporates this into the framework, both reinforcing preexisting categories and reducing the overall state of uncertainty as new information confirms or disaffirms the space of unknown possibilities.

With a schema we not only have the basic functioning of a control system that is able to respond to an immediate stimulus, but by being capable of creating a complex model of a situation, we can also understand what is generating this stimulus in the first place. A schema allows the agent to identify the causes that create the effects. Moreover, an agent with an advanced schema is able to also create a model of its own operation, that is how it responses to any given stimulus and can then try to alter this basic behavior. For example, we might be able to identify that every time we get stressed, we start smoking and then try to alter this reaction. This somewhat self-referential capacity for a system to model and analyze its own regulatory system is the subject of what is called second order or new cybernetics.

These advanced schemata of course have many benefits to an agent over a simple algorithmic logic, it is ultimately the foundations that has enabled technology, advanced civilization and human’s capacity to dominate it physical environment but of course it comes at a cost and not only in terms of the physical energy to maintain the system. However, there is now tension between the basic control system that is designed to react to stimulus thus insuring immediate self-preservation, and the schema that creates a broader vision interested in the system’s long term objectives and consequences of its actions, with the possibility of these two levels conflicting and reducing the agents’ capacity for action.

Human agents within complex adaptive systems are not only governed by the need for physical self-preservation, but are also governed by these advanced conceptual frameworks they are required to maintain both conceptual homeostasis as much as physical homeostasis. Through a number of mechanisms, information can be systematically filtered to ensure it doesn’t threaten the basic assumptions that support the schema and that the system is in regular contact with information sources that endorse and preserve this current schema. In fact, it is critical to the functioning of the whole system. Psychology has plenty of examples of this, such as confirmation bias, which is a tendency to search for or interpret information in a way that confirms ones pre-existing schema and placing much higher validation standards on information that threatens it. Thus, in the same way agents actively seek out environments that are conducive to their physical requirements, they will often actively seek out information sources that preserve and maintain the status quo of their schema.

Therefore, we should not expect human agents to be rational or logical, ultimately humans aren’t computers where logic is a precondition to their operation, but there is instead a subjective dimension to humans that is driven by emotions and independent from logical validation. This subjective domain to human agents is played out in what we call culture, often in the form of a story or a set of stories about how the world is that endorse what is considered right and wrong, with people then acting out these stories as rituals in order to validate them and feel a part of them. People buy Nike shoes because advertising agencies have created a story around the brand, people want to be associated with that and they live this story out by wearing the shoes, there is no economic logic as to why people would pay an extra $50 to buy a pair of shoes with a tick on the side of them. Much of human activity only makes sense within the context of the cultural narrative that it is a part of. This may add a whole new level of complexity to our models but we pay a high price when we exclude it in terms of capacity to capture the real world phenomenon exhibited by many complex adaptive systems.

In summary, we have been talking about the internal logic or schema that governs the behavior of agents within complex adaptive systems. We tried to show how this logic can span from the very elementary to the very complex. At the simples end of the spectrum, we looked a algorithms that typically operator through an if then logic that can switch to generate a given set of output states in response to some given set of input states. We then went on to discuss what are called schemata that represent more advanced conceptual frameworks or models capable of categorizing and filtering information to insure its validity, while also being able to develop through a process that may be understood in terms of Bayesian theory. Lastly, we talked about how social agents are not only governed by a basic regulatory system driving them to maintain physical homeostasis but also a more complex set of needs to maintain a conceptual homeostasis.

**Simple Rules - Complex Patterns**

In the coming sections, we will be starting our discussion on the major theme of self-organization by exploring one of the key premises of complex systems theory, which is that global coordination and complex behavior can emerge out of very simple rules governing the interaction between agents on the local level without need for centralized coordination. At the heart of this is the question of how agents synchronize their state or cooperate to create local patterns of organization and this question will be the focus of our attention during the next few lectures.

We see many examples of self-organization within complex adaptive systems that are composed of elements following simple rules. For example, swarms of fireflies who may start out flashing their light in a random fashion with respect to each other, come, through their interaction, to coordinate their behavior into an emergent pattern of the whole swam flashing in synchrony. This type of quite basic self-organization can be modeled using cellular automata where very simple rules are programmed into a computer and out of the interaction between these simple agents we see emerging surprisingly dynamic patterns that are able to stay evolving over prolonged periods of time to produce novel behavior.

Ant colonies are another often cited example of self-organization through simple rules, without a centralized coordinator the colony as a whole exhibits quite sophisticated differentiation and specialization of it functional organs that then work together to maintain the whole system. Individual ants interact and communicate through exchanging chemical scents that induce other ants to do more or less of a given activity. This type of coordination is the product of what we call feedback loops. We will be discussing how through feedback loops some local pattern or behavior can become amplified to create an attractor state that will draw local elements into a particular synchronized configuration, thus arising some pattern of organization without the need for any form of top down control system.

Lastly, in this section we will be discussing cooperation and competition as another lens through which we can try to understand this process of synchronization. As an example we might think about the vast complex adaptive system of our global economy, an organization that is capable of producing things like laptop computers and sports cars that no individual could produce in isolation. They take the coordination of thousands or possibly millions of people in order to complete the full production and distribution process, but no one is in control of this whole operation, no one makes these people coordinate their activities they have done so according to their own local rules and incentives.

The elements in this type of system have agency, that is some kind of choice over their actions and thus we can best understand the coordination of their activities through the concepts of cooperation and competition, where agents choose to synchronize there states in order to maximize their individual payoffs, once again giving rise to local and global patterns of organization.

**Feedback loops & Externalities**

In this section, we will be discussing the role of feedback loops with respect to self organization by looking at how they works to promote or de-promote the synchronization of states between elements within complex adaptive systems. We will start by talking about the different types of feedback loops while also discussing a number of examples. We will then look at positive and negative externalities and finish by talking about the effects of combining both feedback loops and externalities.

A feedback loop defines a relationship of interdependency between two or more components where the change in state of one element affects that of another with this effect then in tern feeding back to alter the source element. This dynamic captured by feedback loops plays a fundamental role in the self-organization of elements within complex systems.

When the state of elements within a system is independent from each other, then we can use statistics to model the synchronization of states between elements For example, say we have a hundred people in a town with just two banks A & B. If all other things are equal, then we can model whether two people are customers of the same bank using simple statistics where approximately fifty percent of the people will be using any one of the banks. But if the usage of each bank is not independent, it is instead interdependent then it will no longer simply be statics governing the dynamics, it will now be these feedback loops of interdependencies.

To illustrate this say more people are using bank A and this leads to overcrowding in the bank. This may then feedback to affect the users as they decide to go to bank B, which is now quicker and easier to use. Likewise if bank B after sometime then becomes overcrowded people may move back to bank A. This is an example of a negative feedback where the state of one element effects the other in the opposite direction, we can see how the net result of this would be a stable system. If we had a hundred banks in this town governed by this rule the result would be a very evenly distributed and stable system where the agents occupy a wide variety of states with respect to the banks that they use.

But imagine one day bank A starts a marketing campaign, putting up a big billboard saying for every customer we have we will give you one percent extra interest on your savings. The result of this would be that for every new customer the bank had it would present its self as a more attractive option for any other prospective customer. This is an example of a positive feedback where the more elements that adopt this state the stronger the attraction placed upon any other element is to also synchronize its state with this pattern of organization.

Something to take away from this banking example is that in both the first and the second example, that is when we had random correlations or negative feedback between the elements. Both of these dynamics led to an overall stable state where the system tended towards equilibrium, systems governed by these dynamics are linear, additive, we can create closed formula solutions to model them and they are the focus of most of our scientific framework. 

In these first two systems, there is a dynamic that is working to maintain a distribution amongst the states between elements that results in equilibrium. But this is not always the case as we saw positive feedback can drive the system far from equilibrium. Stock market crashes, outbreaks of war, political movements, growth and decay of ecosystems, traffic jams and many biological processes are the product of positive feedback that takes place far-from- equilibrium. Take for example a social riot, as the rioting breaks out, your chance of going to jail decreases, and the social benefit of joining in increases. This creates an attractor, attracting more elements to align themselves with this new organization. Positive feedbacks are nonlinear and they are often a signal of a system shifting into a new regime. 

Whereas feedback refers to dependencies between the same actions externalities refer to dependencies between different actions. An example of an externality might be the relationship between the usage of personal transportation and air quality. The more cars, the lower the air quality. This is a negative externality. A positive externality might be one between the temperature on a given day and the sale of ice-creams, the higher the temperature the higher the ice-cream sales are likely to be. In contrast to a positive feedback loop, positive externalities can reinforce desynchronized states and diversity as two or more different states or classes of things are reinforcing and sustaining each other. This is essentially what we call a synergy. If we have more flowers we can have more bees if we have more bees we can have more flowers thus they endorse and sustain the diversity of states between them. 

Positive feedback combined with negative externalities can be a powerful force for synchronizing the state of elements within a system as it both places a strong attraction on elements of the same class to synchronize their states while also depleting a different class. We might think about the rise of the Third Reich in pre-war Germany as an example. Every time a new member adheres and promotes the ideology of a sociopolitical organization like the Nazi party it has a positive feedback effect amplifying this attractor. But also this social system was having a negative externality on other ethnic minority groups. Thus, it was both reducing the variety within the social group and external to it as all elements became aligned in this sociopolitical regime, the net result of this was totalitarianism as the social system moved far from its equilibrium ultimately resulting in a phase transition as it collapsed into a post-war economic and social crisis. 

In contrast to this negative feedback combined with positive externalities will create a strong mechanism for maintaining equilibrium through endorsing a diverse set of desynchronized states within the system, this will clearly add to a system’s robustness and long-term sustainability, with mature ecosystems exemplifying this.

To sum it up, we have been talking about how feedback loops work to synchronize the state of elements within complex adaptive systems. We have looked at how negative feedback works to maintain a desynchronized set of states between components as they interact through an inverse relationship that promotes diversity of states and stability, while positive feedback works to synchronize the different states between elements as it creates an attractor that exerts a force aligning elements into a single regime. Finally, we talked about externalities as interdependencies between different activities and saw how when we combine feedback with externalities they can work to compliment and amplify each other’s effect.

**Cellular Automaton**

In this section, we are going to discuss cellular automata. We will firstly talk about what they are before looking at a classical example; we will then discuss individually the different classes of patterns that cellular automata can generate before wrapping-up with a talk about their significance as a new approach to mathematical modeling. 

Cellular automata are algorithmic models that use computation to iterate on very simple rules, in so doing these very simple rules can create complex emergent phenomena through the interaction between agents as they evolve over time. To illustrate the functioning of a cellular automaton we will take an example from probably the most famous algorithm called the Game of Life devised by the mathematician John Conway. The Game Of Life is played on a grid of square cells. A cell can be alive or dead. A live cell is shown by putting a mark on its square, a dead cell is shown by leaving the square empty, each cell in the grid has a neighborhood consisting of all adjacent cells to it and there are just three rules governing the behavior of an agent. 

1. Any live cell with fewer than two live neighbors dies, as if caused by underpopulation.
2. Any live cell with two or three live neighbors’ lives on to the next generation.
3. Any live cell with more than three live neighbors dies, as if by overcrowding. 
4. Any dead cell with exactly three live neighbors becomes a live cell, as if by reproduction. 

So let’s input a starting condition and run the program to see what we get. The most basic pattern we will get is called still life or called class one, where nearly all of these patterns evolve quickly into a stable, homogeneous state and any randomness in the initial pattern disappears. 

The second class of pattern we may get is where the system evolves into an oscillating structure. The simplest of these being a blinker that has a period two oscillation. We can also have oscillating structures that cycle over prolonged periods of time for example a pulsar has a period three oscillation, but oscillators of many more periods are known to exist.

Class three patterns are random where nearly all initial patterns evolve in a semi-random or chaotic manner. Any stable structures that appear are quickly destroyed by the surrounding noise. Local changes to the initial pattern tend to spread indefinitely. Here we can get what are called gliders where a group of cells appear to glide across the screen and this is a good example of emergence as we no longer see the simple rules that are producing them but instead this emergent structure of an object gliding. 

Lastly, automata can also produce patterns that become complex and endure over a prolonged period of time, with stable local structures. With these more complex patterns cellular automata can simulate a variety of real-world systems, including biological and chemical ones. 

Since the advent of the game of life, new similar cellular automata have been developed that can do all sorts of things, such as create fractal patterns, which are self-similar structures that repeat themselves over various scale of magnitude. Other games create patterns that can reproduce themselves. We might ask, is there anything that these automate can’t do? From the perspective of computation the Game Of Life can do anything that your computer can do: it can count to 100, calculate the volume of a cylinder or if you wanted to figure out the cube root of 1230. You could encode this into a set of cells on the automata and have it compute the value, the Game Of Life as simple as it is has been proven by computer scientist to be capable of universal computation. 

Lastly, we will talk about the significance of cellular automaton as a new approach to mathematical modeling. Von Neumann and Ulam originally introduced the concept in the mid 20th century and then a few decades later the popular Game of Life brought interest to the subject beyond academia. In the ‘80s, Stephen Wolfram engaged in a systematic study of cellular automata after which he published a book called A New Kind of Science claiming that cellular automata could enable a new approach based upon the exploration of these algorithms. But what is behind this big statement about these simple programs creating a new kind of science? 

One assumption within modern science is that simple rules can only create simple phenomena and thus inversely complex phenomena must be the product of complex rules. The advent of chaos theory during the past few decades revealed this to be an invalid assumption as simple systems like a double pendulum proved to be capable of generating complex and chaotic behavior. It is now increasingly excepted that complexity may not be the product of complex rules but in fact emerge out of the interaction of simple rules as they evolve over time, cellular automata are the tools that capture and embody this paradigm within science. 

Secondly, ever since the rise of modern science some four hundred years ago equations have been the dominant form of mathematical models through which we have encoded so called scientific laws of nature. There are many valid applications for equation based modeling but they also have their limitations. They present a somewhat static picture of the world in a state of permanent equilibrium. This is most clearly exhibited within economic models that describe markets as always moving towards an equilibrium state between supply and demand. But, in reality, many complex systems like ecosystems and societies are only at or near equilibrium when they are dead. Things are constantly changing: new technologies are invented, start ups are disrupting the status quo and so on. Non-equilibrium phenomena of this kind are not well modeled by equations and are best describe through the evolutionary dynamics that shape them, with cellular automata again being well designed to capture this. 

We will finish here by saying that cellular automata are an alternative computation modeling method based upon algorithms that iterate on simple rules to try and simulate complex phenomena. The primary classification of cellular automata are numbered one to four in order of the complexity that they can sustain, as they go from stable to periodic oscillation to chaotic and complex patterns of behavior. We have also talked about how they are better suited to modeling phenomena that are the product of evolutionary dynamics and have emerged out of the interaction between their parts, as is typically the case for complex systems.

**Cooperation & Competition**

In this section, we are going to talk about the dynamics of cooperation and competition between agents within complex systems. We will firstly discuss the general concept before looking at zero and positive-sum-games, following this we will be talking about negative externalities as we look at the so-called “tragedy of the commons” and social dilemma. 

A core premise of complexity theory is that global patterns in complex systems emerge out of the synchronization between the states of elements on the local level, whereas the terms synchronization or desynchronization are generic to any type of system when we are dealing with elements that have agency. That is to say some form of choice over their actions. We will refer to this as cooperation and competition, as agents now have some choice as to whether they synchronize their state with other agents locally. What we will call cooperation, or inversely, they may choose to adopt an asynchronous state with respect to other agents, what we will call competition. 

Cooperation and competition between agents don’t occur randomly. Rather, it is the product of both local and global forces as the incentives. For an agent, to choose one of either, incentives often built into the context of the situation they are engaged in. In order to illustrate this we will look at what is called a zero-sum game. In game theory and economic theory, a zero-sum game is a mathematical representation of a situation in which each participant's gain or loss is exactly balanced by the losses or gains of the other participants. If the total gains of the participants are added up and the total losses are subtracted, they will sum to zero. 

Thus cutting a cake, where taking a larger piece reduces the amount of cake available for others, is an example of this. A zero-sum game is also called a strictly competitive game, as the pie cannot be enlarged by good negotiation and cooperation there is no incentive for cooperation between agents in these situations, but in fact a strong attractor state toward competition. War is another example of a zero sum situation, in these games what the other looses you gain thus keeping track and comparing your state to that of you opponent makes sense. Zero-sum games are linear and additive the whole system is simply a summation of its constituent elements, thus they are essentially simple or non-complex. 

Complexity arises when we have a dynamic between competition and cooperation. Situations where participants are interdependent, being able to all gain or suffer together, are referred to as non-zero-sum. For example, all trade is by definition positive sum, because when two parties agree to an exchange each party must consider the goods it is receiving to be more valuable than the goods it is delivering, this type of positive sum game is a strong driver towards cooperation. As the pie gets bigger and everyone gets higher payoffs by simply interacting. There are many scenarios like this where the cost of coordination is relatively low and the payoff is relatively high, everyone driving on the correct side of the road is an example of this, there is little incentive not to do so and very high incentives to coordinate thus making cooperation a very strong attractor state.

But of course not all scenarios are like this, non-zero-sum games often involve an interplay between competition and cooperation. As an example of this, we might think about a game of doubles tennis where you have a zero-sum game of competition with your opposition but a positive sum game with your team member. Problems in the real world are typically non-zero-sum, where there is no single optimal strategy that is preferable to all others, nor is there a predictable outcome. Players engaged in a non-zero sum conflict have both some complementary interests and some interests that are opposed. 

There are a number of variables that can be altered to adjust whether a game will on balance favor cooperation or competition between agents. Ongoing interactions between agents over a prolonged period of time, as well as dense interactions, allows for trust building. Within many traditional societies there is some required resource placed at the heart of the community, such as water well, or mill in order to promote interaction. This interaction between agents helps to develop reputation systems and identification so that people can cooperate with those they know to be trustworthy owning to their cooperation in the past. Bringing people together, giving them information and channels for communication are the basic ingredients for social cooperation and self-organization. Research on lobster fishing off of Maine New England showed that small communities on islands are better able to cooperate in order to maintain their commons than those on the mainland. 

Another key factor governing cooperation and competition within complex adaptive systems is externalities. To illustrate this we might think about a classical example called the tragedy of the commons. Garrett Hardin in 1968 wrote a paper in Science Magazine in which he imagined a grazing pasture that was open to all and he posited that if this was the case everyone would bring their animals on to graze the pasture more and more which would end in the over usage of the commons and its ultimate collapse. 

Any individual will get an immediate gain from over grazing the pasture but the long term cost of this would be spread out over the whole population of people using the resource. As a result, the cost would be externalized from the simple cost benefit equation that each agent is making, meaning the full cost is not subtracted from the benefits to the individual. Therefore, the action is in their individual interests even if it is to everyone’s detriment. The choice by an agent to overgraze the pasture is then considered a rational action; by rational we simply mean that the action is consistent with the logic of the agent’s self-preservation. 

The tragedy of the commons is an instance of the more generic social dilemma, which is a dynamic where individual rational choice leads to a situation where everyone is worse off, we might say, individual rationality leads to collective irrationality. The social dilemma is behind many economic and social challenges from over fishing to traffic jams to air pollution and voting. The basic mechanics driving this system are feedback loops and externalities. A soon as we have interactions between agents within a system what one element does effects another, but the question is does this effect then return to the agent that created it, if it does, then the agent must account for it. I might be too lazy to carry my coat with me when I go out but if I know that this action will have an immediate negative effect on me in the very short term future when it starts raining, then I will take account of it, factoring it into the equation that governs my current actions. 

But if one element does affect another without that effect then returning to its source, then we have externalities. We call it an externality because the effect that one agent is having on its environment becomes external to the equation under which the agent is operating. If as in our first example every action of an agent feeds back to its source, then the system will be self-regulating with every cost and benefit being paid for by the agent that created it and thus everything can be regulated by the agents locally in a distributed fashion. But with negative externalities the cost and benefits of an agent’s actions become decoupled from the local regulatory mechanisms of the agents and there becomes a need for global governance and top down regulation. 

To conclude, we have been discussing some of the main dynamics behind cooperation and competition. We have talked about how different scenarios, what we called games, create certain attractor states that make competition or cooperation the default position. We looked at zero-sum games that create a dynamic of competition by pitting the losses and gains of individuals against each other. Inversely, we saw how positive sum games can create strong attractors for cooperation as coordination makes the total payoffs increase in value. We also talked about more complex scenarios where there are incentives for both cooperation and competition. Finally we discussed the social dilemma and the dynamics of the negative externalities that were behind it.

**Self-Organization Overview**

In this section, we will be talking about the process of self-organization within complex systems and the dynamic interplay between order and entropy that is thought to be required to enable it. We will firstly discuss different theories for the emergence of organization in so doing we will look at the first and second laws of thermodynamics. We will then talk about the rise of self-organization theory during the past century and lay down the basic framework through which this process is understood to take place. 

“Why then, O brawling love! O loving hate! O any thing, of nothing first create!” This short quote from Shakespeare, asks probably the oldest and most fundamental question there is to ask, why and how do we get something instead of nothing, some form of order instead of just randomness. From the formation of galaxies to the human body to the structure of snowflakes or the complex organization within a single biological cell. We live in a world that exhibits extraordinary order of all kind and on all scales. The real question is why or how do we get things to work together, how do we get global level coordination within a system? And there are two fundamentally different approaches to trying to answer this question; firstly, this coordination may be imposed by some external entity or secondly it may be self-generated internally. 

For thousands of years, many different societies came to the former conclusion that this organization we see in the world derives from some external divine entity, religions and spirituality often depict the world in terms of a interplay between super natural forces of order and chaos, but of course modern science has always rejected any form of divine intervention, as core to its foundation is the law to the conservation of energy and matter. 

The first law of thermodynamics is an expression of this fundamental conservation, which states that the total energy of an isolated system remains constant or conserved. Energy and matter can be neither created nor be destroyed, but simply transformed from one form to another. The conservation of energy is a fundamental assumption and keystone of the scientific enterprise. If you tell a physics that you have create a perpetual motion machine, that can essentially create energy out of nothing they will just laugh at you, because you are no longer playing the game of science, you have broken its most fundamental rule. 

The second law of thermodynamics states that the total entropy, which may be understood as disorder, will always increase over time in an isolated system. To understand where this comes from we might think about how if we have some object heated that heat will always try to spread out to become evenly distributed within its environment. However, the reverse never happens. Heat will not spontaneously reverse this process to become concentrated again. Likewise, whenever rooms are cleaned they become messy again in the future, people get older as time passes and not younger all of these are expressions of the second law of thermodynamics meaning that a system cannot spontaneously increase its order without external intervention that decreases order elsewhere in another system. 

For many years, the second law of thermodynamics—that systems tend toward disorder—has generally been accepted. Unfortunately, none of this helps us in answering Shakespeare’s question as to why our universe has in fact developed to produce at least some systems with extra ordinary high levels of organization. In fact, the second law of thermodynamics would predict quite the opposite. 

The term "self-organizing" was introduced to contemporary science in 1947 by the psychiatrist and engineer W. Ross Ashby. Self-organization as a word and concept was used by those associated with general systems theory in the 1960s, but did not become commonplace in the scientific literature until its adoption by physicists and researchers in the field of complex systems in the 1970s and 1980s. In 1977, the work of Nobel Laureate chemist Ilya Prigogine on dissipative structures, was one of the first to show that the second law of thermodynamics may not be true for all systems. 

Prigogine was studying chemical and physical systems far-from-equilibrium and looking at how small fluctuations could be amplified through feedback loops to create new patterns. For example, when water is heated evenly from below, while cooling down evenly at its surface, since warm liquid is lighter than cold liquid, the heated liquid tries to move upwards towards the surface. However, the cool liquid at the surface similarly tries to sink to the bottom. These two opposite movements cannot take place at the same time without some kind of coordination between the two flows of liquid. The liquid tends to self-organize into a pattern of hexagonal cells call convection cells, with an upward flow on one side of the cell and a downward flow on the other side. 

The theory of self-organization has come to explore a new approach to this age old question about the emergence of order. Unlike religion and spirituality that simply ascribes it to exogenous super-natural phenomena or tradition reductionist science that posits that order can only come by transferring it from some other external system. With self-organization theory, the organization is instead traced back to the interaction between components, where nonlinear interactions between elements can become amplified by positive feedback loops to create attractors that can result in new patterns of order emerging. But that this process requires the system to be far-from its equilibrium so as a to have sufficiency entropy or disorder, for new random fluctuations and noises to gain traction and take hold as emergent patterns. When the system is far from its equilibrium it can find a dynamic state between order and chaos that enables it to continue generating novel phenomena and regenerate itself for prolonged periods of time through self-organization. 

Thus, this new set of theories around self-organization recognizes a complex interplay between order and chaos. Whether we use the more scientific terminology of a system being far-from-equilibrium, or the more catchy term of edge-of-chaos. This new vocabulary has built into it a recognition that self organization, evolution and novelty, thrive on a dynamic interplay between order and disorder because it is only when there is a sufficiently high enough level of entropy and disorder within the system that a weak fluctuation can be amplified into a new pattern of order. But when the system settles into an equilibrium or stable configuration this no longer becomes possible. 

Today, the study of self-organizing systems is a hot topic that is central to understanding the complex systems that make up our world, with interest in how to model, design and manage complex systems coming from many areas such as the social sciences, computer science, business management, robotics and engineering. The theory and interest in the process of self-organization has arisen in tandem with computing resources. Just as we can’t study galaxies without out telescopes or cells without microscopes we can’t study complex systems without computers. Whereas before it was very difficult to mathematically model systems with many degrees of freedom, the advent of inexpensive and powerful computers made it possible to construct and explore models composed of many entities. Looking at how out of their local interactions global patterns of organization can emerge and this represents one of the few primary methods which we use to study complex systems. 

In summary, we have been talking about the process of self-organization. We looked at the different theories offered for the emergence of organization and discussed how self organization theory ascribes this process to the nonlinear interactions between components when the system is in a dynamic state far from its equilibrium. With this so called edge-of-chaos state providing it with a sufficient amount of entropy for some small fluctuation to take hold and become amplified into a new pattern of order.

**Self-Organization Far-From-Equilibrium**

In this section, we will be talking about the theory of far from equilibrium self organization. We will firstly discuss the concepts of order and randomness in terms of symmetry and information theory. We will then talk about complex as the product of an in between or phase transition state and finally will discuss the term edge-of-chaos and talk about how self-organization is thought to be dependent upon noise and random fluctuations in order to stay generating variety. 

Far-from-equilibrium self-organization is a model that describes the process of self organization as taking place at a critical phase transitions space between order and chaos when the system is far-from its equilibrium. But let’s start by talking about organization. Organization is an ordered structure to the arrangement of elements within a system that enables them to function, as such we can loosely equate it to the concept of order. Both order and organization are highly abstract concepts neither of which is well defined within the language of mathematics and science but probably the most powerful method we have for formulizing them is through the theory of symmetry. 

The theory of symmetry within mathematics is an ancient area of interest originally coming from classical geometry but within modern mathematics and physics it has been abstracted to the concept of invariance. In this way symmetry describes how two things are the same under some transformation. So, if we have two coins one showing heads the other tails, by simply flipping one of the coins over it will come to have the same state as the other. Thus we don’t need two pieces of information to describe the states within this system, we can describe this system in terms of just one state and a flipping transformation that when we perform it will give us the other state. 

Now say instead of having two coins we had an apple and an orange, well there is no transformation we know of that can map an apple to an orange, they are different things there is no trivial symmetry or order between them and thus we need at least two distinct pieces of information to describe this system. This second system requires more bits of information to describe its state, thus we can say it has higher statistical entropy. The point to take away here is that we can talk about and quantify order and randomness in terms of information theory that ordered systems can be described in terms of these transformations, which we encode in equations. Ordered systems are governed by equations whereas random systems are not. But because there is no correlation between the element’s states in these random systems, they are governed by probability theory the branch of mathematics that analyses random phenomena. 

Complex systems are by any definition nonlinear. Complexity is always a product of an irreducible interaction or interplay between two or more things: if we can just do away with this core dynamic and interplay then we simply have a linear system. If the system is homogeneous and everything can be reduced to one level then it might be a complicated system but it is certainly not a complex system. Thus one of the main ideas or findings of complexity theory is that complexity is found at what is sometimes called the interesting in between. If we take some parameter to a system, say its rate of change or its degree of diversity and turn this parameter fully up: what we often get is randomness, continuous change or total divinity of states without any pattern; or, if we turn it fully down, we get complete stasis and homogeneity with very stable and simple patterns. 

It is often the case that, with too much order, the system becomes governed by a simple set of symmetries. However, too much disorder results in randomness and the system becomes subject to statistical regularities. It is only in-between that we get complexity; on either side of this there is a single dominant regime or attractor that will come to govern the system’s behavior. 

It is only when a system is far from its equilibrium, away from one of these stable attractor regimes that we get a phase transition area representing the interplay between the two regimes, in this space the system is much more sensitive to small fluctuations that can take it into either basin of attraction, this phase transition area is also called the edge of-chaos. The phrase edge-of-chaos was first used to describe a transition phenomenon discovered by computer scientist Christopher Langton. Langton found a small area conducive to producing cellular automata capable of universal computation. At around the same time physicist James Crutchfield and others used the phrase “Onset of chaos” to describe more or less the same concept. In the sciences in general, the phrase has come to refer to a metaphor that some physical, biological and social systems operate in a region between order and either complete randomness or chaos, where the complexity is maximal. 

The edge of chaos concept remains mainly theoretical and somewhat controversial but it is often posited that self-organization and evolution can only really happen in this phase transition space. There may be a number of different interpretations for why this is so but one way of understanding it is that self-organization requires entropy and evolution requires variety. Unlike external intervention where we can take a well ordered system and simply reconfigure it by transferring energy to it from some other external source, in this way we go from one ordered regime to another without the need for entropy to enable the process, we simply need some input of energy. But as we know self organization does not happen in this fashion, it does not resort to a mass import of energy, it is internally generated on the local level and this process requires the presents of entropy and randomness for elements to be available for reconfiguration into a new regime through feedback loops that originate as weak signals or fluctuations. 

A number of different researchers have posited different theories surround this process of self-organization far-from-equilibrium. The principle of "order from noise" was formulated by the cybernetician Heinz von Foerster in 1960. It notes that self organization is facilitated by random perturbations and noise that let the system explore a variety of states in its state space. A similar principle was presented by the Ilya Prigogine as "order through fluctuations" or "order out of chaos". Researcher Per Bak also looked at this phenomenon in terms of what he calls self-organizing criticality the mechanism by which complex systems tend to maintain themselves on this critical edge. Many of these theories talk about both the need for entropy and variety in order for the system to stay adapting and evolving over a prolonged period of time. 

In summary, we have been talking about the process of self-organization talking place far-from-equilibrium at interplay between order and chaos, a theory that may have a lot of conceptual substance to it but remains a long way from having a proper formulization. We talked about how order and randomness can be described in terms of symmetry and information theory, we looked at complexity as phenomena that arise at the interplay between different entities or regimes, finally we discussed the term edge-of-chaos and the role it is thought to plays in the process of self-organization.

**Robustness**

In this section, we will be discussing robustness and resilience within self-organizing systems. We will firstly talk about what we mean by robustness. We will go on to discuss adaptation as a mechanism for resilience and why self-organizing systems are typically considered robust. We will look at the theory of requisite variety and finish by talking about self-organized criticality. Before we start lets briefly recap as to what we mean by self-organization, self-organization is basically the spontaneous creation of a globally coherent pattern out of the local interactions between initially independent components. This collective order is organized in function of its own maintenance, and thus tends to resist perturbations. 

All systems exist within an environment and are to a certain extent dependent upon a specific range of input values from that environment. The system has a set of parameters to these inputs within which it can maintain its structure and functionality but outside of these critical parameters the system will disintegrate, i.e. become degraded to a lower level of integration or functionality. Resilience and robustness are then defined by this set of parameters, the lower the system’s dependency upon its environment and the broader this range of input values that the system can operate within the more robust it can be said to be. 

For example, in computer science, robustness is the ability of a computer to cope with errors during execution, that is to say the ability of an algorithm to continue operating despite abnormalities in input, in this way robustness defines its independence from a specific range of inputs or inversely its capacity to process a wider range input states. To illustrate this farther we might think about a tree with-standing the force of wind blowing against it, the tree has a certain tensile strength through its capacity to bend, within a certain range of input values to the force exerted upon it, it will be able to with-stand this perturbation from its environment. The wider the range to these input values the more robust the tree will be. 

There are fundamentally just two ways for a system to maintain its integrity given some perturbation, it can resist this change or adapt to it. By resist we mean it creates a boundary or filter condition that prevents the external influence from altering the internal configuration to the system and thus preserving its functionality and structure up to some limit. In our tree example this might mean the organism developing a sturdy truck. Inversely the system can adapt by finding or generating the appropriate response required to counter-balance the perturbation, we might think of this as the tree bending over in response to the force exerted upon it. 

Robustness and resilience are general characteristics of self-organizing systems both through their capacity to resist change and their capacity to adapt to it. Firstly, we will talk about their capacity to resist change through distributed control and feedback loops. In centralized systems with up-down control there are specialized components required for regulating the system, these represent largely irreplaceable hubs that will affect the whole system if removed or degraded, within complex systems. In contrary, control is distributed out on the local level meaning there is much less specialization, missing or damaged components can often be replaced by others and this gives them a much lower level of criticality. 

Secondly, self-organizing systems are held within their current configuration by a set of feedback loops that are also distributed out across the system on the local level. A good example of this might be a magnet that consists of many tiny magnetic spins that are all aligned to produce an overall magnetic force. If part of the spins are knocked out of their alignment. The magnetic field produced by the rest of the spins will quickly pull them back. This force maintaining the system within its current configuration is distributed out again giving it a low level of criticality and thus a higher level of robustness. 

Next, we will talk about adaptation as a mechanism for resilience. Here we are talking about the system’s capacity to maintain or generate sufficient diversity of states for it to be able to select the appropriate response when required to counter balance a perturbation from its environment and thus maintain its internal configuration within the required critical parameters to preserve its structure or function. 

To illustrate this, we might think about going hiking on a mountain. In this situation one needs to be aware of the possible states to the weather that this environmental might present and have sufficient variety of clothing to counter balance these different possible perturbations in order to maintain one’s body within its critical temperature parameters that are required for its continued functioning. If I do not have what is called the requisite variety in order to adapt then this environment might present me with a blizzard for which I do not have the thermal clothing to maintain my body and in such a case my body’s functionality may be severely or critically degraded. 

Another reason for this intrinsic robustness to self-organizing systems is that self organization thrives on randomness, fluctuations or “noise”. Without these initial random movements self-organization cannot happen. A certain amount of random perturbations may facilitate rather than hinder self-organization, if the overall pattern that is generating the system remains intact the entropy from the perturbation may be used for regeneration and evolution. For example, forest fires are thought to play an important role in the development of ecosystems, excluding fires from these ecosystems means fire-adapted plants decline in abundance and overstocked forests become more prone to catastrophic fire due to the buildup of woody fuels. Exposing the system to perturbations without destroying it is a core part of the process of evolution and developing resilience. 

But self-organization don’t always lead to robustness it can also lead to what is called self-organized criticality where the system organizes into a state where some small event can have a large systemic effect. This phenomenon is best described with referents to what is called the sand pile model. This model is simulated by simply dropping grains of sand on a surface, as the pile builds up grains role off the side from time to time, typically just one or two at a time, but as we stay adding sand the side of the pile eventually builds up to a critical angle before we get a massive avalanche. At some critical point adding just one more grain of sand triggered a massive effect. 

This sand pile model for self-organization has been used to model everything from the occurrence of earthquakes to neuronal avalanches in the cortex and financial crisis. The positive feedback loops that are an inherent part of the process of self-organization can also be a strong force for reducing diversity in the system as they synchronize it into a single regime where all elements become susceptible to the same perturbation, without diversity to resist the spreading of some phenomena it can cascade into a systemic shock. 

To sum up, we have been talking about robustness and resilience within self-organizing systems. We defined robustness in terms of a system’s dependency upon a set of input values from its environment in order for it to maintain homeostasis. We talked about how a system can maintain its functionality given a perturbation from its environment by either resisting it or adapting to it, we saw how distributed feedback loops and interchangeability of components enables robustness. We also looked at the need for what we called requisite variety in order for a system to be able to adapt and finally talked about self-organizing criticality and noted that although complex systems are often robust they are also susceptible to large systemic shocks.

**The Dynamics of Evolution**

In this section, we will be talking about the dynamics of evolution within complex systems. We will firstly define what we mean by the term, then look at its basic functioning by discussing each stage in the process and finally talk about evolution with respect to the development of complexity. A quick note before we begin, the concept of evolution has a strong association with its application in biology, complexity theory though deals with the concept on a slightly more abstract level as it applies to all complex adaptive systems from the development of civilizations to finical markets, cultures and technologies as such we are trying to understand evolution as a continuous and pervasive phenomena that occurs in all types of natural, social and engineered systems. 

Evolution is essentially the same process as adaptation, except that it operates on a different scale, adaptation is an element’s capacity to generate a response to some change within its environment, it is a micro or local level phenomena. Evolution then is this same process but operating on the macro scale, which is to say on the level of a population of agents and here again it, is the capacity for the system to respond to changes within its environment. Evolution is the adaptive response of a group of entities that occurs over a period of many life cycles. Evolutionary changes reflect the response of the collection of agents to their environment. 

We will now present the basic framework to the theory of evolution as posited by systems theory and complexity theory and then afterwards go over this in more detail with examples. Firstly, a system exists within an environment and that environment changes periodically. For the system to endure it must be able to generate the appropriate response to these environmental perturbations. Secondly, generating the appropriate response means selecting from a variety of different internal states or strategies and thus the system needs to maintain and be able to generate a certain degree of variety. Thirdly, that variety is not for free it costs the system something to maintain and thus it must select from these variants the most appropriate responses for that environment in order to minimize this cost while maximizing the pay-off to the system as a whole. Lastly, these variants that have proven most appropriate for responding to changes within that particular environment will then be selected for replication in order to be more prevalent within the system during its next life cycle, thus achieving the ultimate goal of altering the entire system to make it better suited to that environment. 

Our first statement that a system needs to be able to generate an appropriate response to any change from its environment in order for it to endure, can be described by the theory of homeostasis, that all systems require a certain state to their environment in order to operate and they need to somehow maintain that set of input values from their environment or else they will cease to properly function when it changes. If I am driving my car down the road and someone pulls out in front of me I need to be able to identify this and steer around it, the environment has changed and if I can’t generate the appropriate response then I am in trouble. So this is not so much a statement of how evolution works but more a statement of why we need evolution, put very simply it is because environments change, if we can’t change with them we will get wiped-out and evolution is the only way to prevent this. 

Secondly, that the system needs to maintain a certain level of variety from which it can select the appropriate response given any environmental change. This is the so-called law of requisite variety, that a system needs sufficient variety of states to respond to the variety within its environment. For example, the human immune system needs to have the right type of antibodies to neutralize an invader; the immune system does this by simply produces an astronomical variety of different antibody shapes so that it will have the appropriate response when needed. In general, this requisite variety maybe create by randomness as in the random deformation of DNA or cross mixing such as sexual reproduction but also may be purposefully generated, as would be the case for an R&D lab. 

Thirdly, selection: the most appropriate responses to the given state of the environment are selected, because diversity typically has some cost associated with it and thus we can only maintain a limited amount of it. I may want to be appropriately dress for any given weather condition, but I can’t bring my whole wardrobe with me, each item I take will have a carrying cost associated with it, thus I must perform selection upon the variety of clothing I have based upon an assessment of the environment’s state. Biological evolution by means of natural selection is another example: there is a limited amount of resources within any ecosystem, selection takes place as a result of the competition among the members of a population for resources, and this helps by work to insure that only those that are so called “fit” for that environment will endure. In this way the system can adjust its internal configuration to external perturbations, while minimizing the cost of diversity and changes to its overall organization. 

Lastly, replication: unlike adaptation which is an immediate process operation on the level of an individual, evolution as mentioned works instead on the level of a population of agents, it don’t operate immediately but plays-out over the course of several lifecycles to the population. Elements that have proven to be functional within that environment during their lifecycle are selected for replication, thus increasing the percentage of their representation within the future population in order for the overall system to exhibit more of their desired characteristics. 

With this process of evolution a system can through its iteration over a prolonged period of time go from starting simple to becoming more complex through the retention of functional variants, in so doing expand to become capable of operating within broader more complex environments. Continuing on with our example of the immune system, the immature immune system of a new born child is dependent upon its mother to produce and provide it with antibodies in order to fend of invaders. As it grows and comes in contact with new antigens, naturally or through vaccines, it develops its own antibodies and retains copies that have proven successful for future application. Thus, building up a catalog of successful antibodies that can provide it with the requisite variety to maintain its physiological homeostasis within more threatening environments. 

When a system has requisite variety, then it can be said to have control over itself within that particular environment, but of course there is always a broader environment that will present the system with a wider more complex set of eventualities for it to deal with. As the system evolves it retains the appropriate responses for a given perturbation until it has accumulated the requisite variety for a given environment and then can expand into a broader one, where again it will have to generate more variety in order to deal with a new set of perturbations. 

Now that we have come to understand this dynamic of evolution it is increasingly being used as an optimization algorithm in many areas. For example, computer scientist create programs or formulas that compete against one another to solve a problem, the winners being rewarded with “offspring” in the next generation that then compete again. Over a series of generations one can use this process to evolve optimal solutions to difficult problems. The resulting method, under the name genetic algorithms, has become a widely used optimization method and a tool for complex systems researchers. Genetic algorithms are good at taking a very large search space and looking for optimal solutions through iteration. 

In summary, we have been talking about the basic dynamics to the process of evolution which we defined as essentially the same process as adaptation, the capacity for a system to respond to changes within its environment, but this time operating on the level of a population of agents. We itemized the basic elements to this process as consisting of the generation of variety through random events and cross mixing. Selection upon this set of variants according to their degree of functionality within a particular environment and lastly replication in order to increase the proportion of elements exhibiting this required capability. In this way a system on the macro scale can reconfigure itself to respond to the input of an environmental change and stay adapting for a prolonged period of time.

**Fitness Landscapes**

In this section, we will present the concept of a fitness landscape as it is used to model complex adaptive systems, we will provide you with a basic description of how the model works, talk about the key parameter that effect its topology and finally look at the types of strategies used by agents within these different landscapes. 

A fitness landscape also called an adaptive landscape is a model that comes from biology where it is used to describe the “fitness” of a creature, or more specifically genotypes, within a particular environment, the better suited the creature to that environment the higher its elevation on this fitness landscape will be. As such, it visually represent the dynamics of evolution as a search over a set of possible solutions to a given environmental condition in order to find the optimal strategy which will have the highest elevation on this landscape and receive the highest payoff. 

As evolution is a fundamental process that plays out across may different types of systems, natural, social and engineered, this model has been abstracted and applied to many different areas in particular within computer science, business management and economics, but is equally applicable to all complex adaptive systems. Within this more generic model a location on the landscape is a solution to a given problem, the elevation captures how functional that solution is and solutions that are similar in nature are typically placed close to each other. 

For example, the challenge might be commuting to work in the morning, there are many different strategies we could take from flying to possibly swimming to driving our car or taking the bus. We could then create a fitness landscape to represent this where each one of these solutions would be given a fitness value based upon how well it performs against some measurement of success, such as time or cost. The result being swimming or flying will likely end up at a low elevation relative to taking our car or the bus, we might also note that our car or bus strategy would be located in proximity to each other because they have many similarities, while swimming or flying would be placed at very different location on this landscape. 

Now that we understand what a fitness landscape model is there are two main things we need to consider, firstly the type of landscape we are dealing with and secondly the types of strategies we might use given these different landscapes. 

Firstly to talk about the types of landscapes, what we will call their topology, there are a number of different parameters that will define the overall topology. Starting with how different are the payoffs on the landscape, the lower the range between the heights of the peeks the more equal the payoffs between strategies. An example of a even topology might be a scenario where I role a fair dice and ask you to try and predict the number it will land on, each number is equally likely to turn up and thus each one of your strategies is an equally viable solution. As we turn up this parameter to the unevenness of the topology, there will come to be a greater disparity to the functionality of the different strategies and their payoffs. 

Secondly, how distributed are the optimal solutions? Is there just one dominant strategy that will drastically out perform all others or are there many different viable solutions, for example in terms of intercontinental passenger transportation, air travel drastically out performs all other methods with respect to time. If we create a fitness landscape of the different methods we would see one dominant mountain in the center with lots of other much smaller peeks around it. 

Thirdly, how dynamic is the environment? Are we dealing with some ecology where environmental conditions may remain relatively stable for prolonged periods of time or are we dealing with say some emerging market where the context is changing rapidly resulting in the peaks and valleys to the landscape moving up and down as the whole landscape dances around? 

Lastly, how interdependent are events, does what one agent chooses to do effect the landscape or other agents, a fitness landscape of say a market is created by all the companies, consumers and regulators within that market, every time one of these players moves it effects the whole landscape and thus we have a dynamic landscape that will be defined by these set of interdependencies. 

Now that we have an idea of the different types of topologies we can start to think about the different types of strategies that agents might use, as the degree of functionality to any solution will alter drastically depending upon the type of landscape it is operating on. Agents within complex adaptive systems can typically only respond to local level information, whether we are talking about a trader in a financial market or a herd of deer looking for pasture, these agents do not have complete information of their environment they can only access and thus respond to a limited amount of typically local level information and they need to have a strategy for processing this information and generating an optimal response. This strategy is essentially just an algorithm, we will call this the explore or exploit algorithm because an agent has fundamentally just two options to either exploit their current position within the landscape or invest resources to go exploring for new solutions, that is to say looking for higher peeks. 

We will start with agents on the simplest landscape by turning all our parameters fully down making it, smooth, distributed, static and without any interdependencies. In this scenario you don’t even need a strategy, like our example of trying to predict which number a fair dice will end up on, all options are equally valid, thus your best option is to just stay exploiting your initial position. Now we will turn up the disparity between payoffs so that there is at least one optimal solution that is far superior to others, one big mountain in the middle of the landscape, now all agents need is a simple algorithm that tells them to stay going upwards until they come to a peek and then stay there, as they have now found the global optimal solution, this is called a greedy algorithm and it works well in these very simple environments. 

Next we will turn up the distribution of solutions, here the topology will develop many local peeks of varying height, this landscape corresponds to a problem that involves a set of interacting variables, and there are many different variables and different combinations between them giving us lots of different possible solutions. Designing a car would be an example of this, we might want it to be fast and low cost, but if we put a bigger engine in it to make it go faster this would require a stronger chasse which would add to the cost and there would of course be many more interacting factors involved allowing for many different possible solutions but some would still be better than others. 

Applying our greed algorithm here would result in an agent getting stuck on the first local peek it comes to which is unlikely to be the global optimal solution. What is needed is a much greater initial investment in exploring, allowing the agent to go up and down many times while also over a prolonged period gradually reducing the amount of times the agent is allowed to go downhill, thus gradually closing in on a global optimal solution without getting stuck on local peeks. 

Next as we turn up the volatility to the environment peaks and valleys move up and down overtime, we might think about climate change here what was once an optimal environment for some creature may become a valley as the climate changes and the peek moves to some other part of the landscape. Unlike in static landscapes where it is worth making a big investment in exploring because once you find the optimal solution you will be able to exploit it for a long time, in these dynamic environments this is no longer the case as the goal is changing and the agents need to stay changing with it. If we then turn up the interdependency between the actions that agents take the environment will become even more dynamic as the topology is being continuously shaped and reshaped by the actions and reactions of the agents to each other with agents needing to be continuously adapting. 

In summary, we have be looking at how the model of a fitness landscape can be applied to understanding the environment within which complex adaptive systems operate, as it gives us a visual representation to the dynamics of evolution that is the primary force shaping these systems on the macro scale. We have seen how a few key parameters can fundamentally alter this topology and thus require agents to adopt very different strategies in their pursuit of optimal solutions.

**Conclusion**

We can try to summarize what we covered in this book by saying complex adaptive systems are a special class of system composed of many interconnected, autonomous parts that are capable of adaptation. Examples include flocks of birds, ant colonies, the human immune system, the Internet’s routing system, cities and all forms of social organization, from financial markets to political regimes. 

These systems are primarily defined by the agents’ capacity to adapt through some form of regulatory or control system. The logic under which they do this is often quite simple, but through the nonlinear interactions between many agents we can get the emergence of a global pattern of organization that is qualitatively different from that of any of the parts. With this global pattern then feeding back to both enable and constrain the agents on the local level, giving us a complex dynamic between the macro and micro levels to the system. 

These complex adaptive systems are highly dynamic. They are typically high-energy systems. They import a lot of energy in order to maintain a dynamic state far-from equilibrium. In physics, this is what is called a dissipative system. They import energy and dissipate it in order to maintain this dynamic non-equilibrium state. A flock of birds would be an example of this. 

Because the components have a high degree of autonomy and are capable of adaptation, this means that control is primarily distributed out to the local level. The overall state of the system is a product of the interactions between the agents. Nothing is really written in stone. Agents are just acting and reacting to each other’s behavior, like businesses competing in a market or traders buying and selling in a financial network and we can use the model of a fitness landscape to try and capture this. Out of all these actions and reactions, we get the overall state of the system as it develops through an evolutionary process.