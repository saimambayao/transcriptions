
**Nonlinear Systems**
An Overview

**Nonlinear Systems: An Overview**

**Contents**
1. Systems Overview
2. Linear Systems Theory
3. Nonlinear Systems Overview
4. Synergies & Interference
5. Feedback Loops
6. Exponentials & Power Laws
7. Long Tail Distributions
8. Dynamical Systems
9. Nonlinear Dynamics & Chaos
10. The Butterfly Effect
11. Fractals

**Preface**

This book is a voyage into the extraordinary world of nonlinear systems and their dynamics. The primary focus of the book is to provide you with a coherent understanding of the origins and product of nonlinearity and chaos. The book is designed as an intuitive and non-mathematical introduction, it explores a world of both extraordinary chaos where some small event like a butterfly flapping its wings can be amplified into a tornado, but also a world of extraordinary order in the form of fractals, self-similar structures that repeat themselves at various scales, one of nature’s most ingenious tools for building itself. Like quantum physics the world of nonlinearity is inherently counter-intuitive, it’s a world where our basic assumptions start to break down and we get extraordinary results, once the domain of obscure mathematics, the concepts from nonlinear systems theory are increasingly proving relevant to the world of the 21st century.

This book covers all the key concepts from this domain, starting by looking at the origins of how and why we get nonlinear phenomena, we go on to talk about exponential growth, power laws, chaos theory, the butterfly effect, bifurcation theory, fractals and much more. The book requires no prior specific knowledge of mathematics or science; it is designed as an introduction presenting concepts in a nonmathematical and intuitive form that should be accessible to anyone with an interest in the subject.

**Nonlinear Systems Overview:** In this section we start the book by giving an overview to the model of a system that will form the foundations for future discussion, we talk about linear systems theory based upon what is called the superposition principles of additivity and homogeneity. We will go on to talk about why and how linear systems theory breaks down as soon as we have some set of relations within a system that are non-additive, we also look at how feedback loops over time work to defy the homogeneity principle with the net result being nonlinear behavior.

**Feedback Loops & Relations:** In this section we introduce the key sources of nonlinearity as the type of relations between components within a system where these relations add or subtract some value to the overall system. We will talk about synergies and interference that make the system either greater or less than the simple sum of its components. We will then cover the second source of nonlinearity, what are call feedback loops that allow for both exponential growth and decay.

**Exponentials, Power laws:** In this section we will discuss the dynamics of exponentials and their counterparts power laws that represent an exponential or power relation between two entities, we talk about long tail distributions, sometimes called the fat tail, so called because it results in there being an extraordinary large amount of small occurrences to an event and a very few very large occurrences with there being no real average or normal to the distribution.

**Systems dynamics & Chaos:** For many centuries the idea prevailed that if a system was governed by simple rules that were deterministic then with sufficient information and computation power we would be able to fully describe and predict its future trajectory, the revolution of chaos theory in the latter half of the 20th century put an end to this assumption showing how simple rules could, in fact, lead to complex behavior. In this section we will describe how this is possible when we have the phenomena of what is called sensitivity to initial conditions.

**Fractals:** We will have encountered many extraordinary phenomena by this stage in the book but fractals may top them all, self-similar geometric forms that repeat themselves on various scales, they can both contain infinite detail, as we zoom in and the very counter-intuitive phenomena of infinite length within a finite form with this all being the product of very simple iterative rules.

**Systems Overview**

A nonlinear system is a type of system, so before we can talk about them we need to first have some understanding of the more general concept of a system. There are many definitions for systems, we will just quickly present a few of them for you to get an idea of where they overlap. Your Dictionary defines a system as: An arrangement of things, or a group of related things that work toward a common goal. The Merriam-Webster dictionary defines a system as: a group of related parts that move or work together. Webopedia tells us a system is: A group of interdependent items that interact regularly to perform a task.

We will then loosely define a system as a set of parts that are interconnected in effecting some joint come. A system then consists of parts, what we call elements, and a set of connections between them, what we call relations. An example of a system might be a farm, a farm consist of a set of components such as, fields, seeds, machinery, workers etc. and the set of relations between these things that is how they are interconnected in order to perform some overall function.

A bicycle is also an example of a mechanical system with a number of elements that have been designed to interrelate in a specific fashion in order to collectively function as a unit of transportation. A plant cell is another example of a system, this time it is what we would call a biological system, consisting of a set of organelles that are interconnected and interdepend in performing the metabolic processes that enable the cell as an entire system to function. There are of course many more examples of systems from ecosystems to transportations systems to social systems. In the world of science and mathematics there are fundamentally two different types of systems, what are called linear and nonlinear systems and in the next two sections we will be discussing each and the distinctions between them.

**Linear Systems Theory**

Before we talk about nonlinear systems we need to first have a basic understand of what a linear system is. Linear systems are defined by their adherence to what is called the superposition principles. There are just two superposition principles and they are called homogeneity and additively. Firstly, additively, which states that we can add the effect or output of two systems together and the resulting combined system will be nothing more that the simple addition of each system’s output in isolation. So for example, if I have two horses that can each pull a hundred kilograms of weight on a cart in isolation, well if I then combine these two horses to tow a single larger cart they will be able to pull twice as much weight. Another way of stating the additivity principle is that for all linear systems, the net response caused by two or more stimuli is the sum of the response, which would have been caused by each stimulus individually.

Our second superposition principle, homogeneity, states that the output to a linear system is always directly proportional to the input, so if we put twice as much into the system we will, in turn, get out twice as much. For example, if I pay $50 for a hotel room, for which I will get a certain quality of service, this principle states that if I pay twice as much I will then get an accommodation service that is twice a good. When we plot this on a graph we will see why linear systems are called linear because the result will always be a straight line.

These principles are of course deeply intuitive to us and will appear very simple, but behind them are a basic set of assumptions about how the world works, so let’s take a closer look at these assumptions that support the theory of linear systems. Essentially what these principles are saying is that it is the properties of the system in isolation that really matter and not the way these things are put together or the nature of the relationships between them, this is of cause very abstract so let’s illustrate it with some examples.

Imagine you have an ailment and you have two drugs that you know are meant to cure this problem, so you take them both at the same time. The result of this, or we might say, the output to this system, will depend on whether the two drugs have any effect on each other when taken in combination. If the drugs have no effect on each other, then it will be the properties of each drug in isolation that will define the overall output to the system, and because of this lack of interaction between the components our linear model will be able to fully capture and describe these phenomena. But, if the drugs do have some effect on each other, then it will be the relationship between them that will define the system and our linear model that does not account for this will fail as it is based upon the principle of additively that assumes a simple adaptive relationship that is not the case in this situation.

So we can then see the basic reasoning behind adaptively, that we can simply add things without any regard of how they will interact when we put them together. Now behind the principle of homogeneity is the assumption that scale doesn't matter. So let’s think about this: say I have a business producing a million widgets a year and then scale this up to producing two million the next year. Well, maybe everything will simply scale in a linear fashion that will of cause be captured by our linear model, but also it may not. If I can leverage economies of scale then cost will not grow in a linear fashion and if by producing twice as much, I saturate the widget market, then this will feedback to reduce my revenue resulting in a scaling that is not linear in nature.

Linear systems models fail to capture feedback, they do not take into account the effect that an action of a system will have on its environment and how that will in turn feedback to effect the system again, not just in space but also in time, that is to say how past actions will feed in to effect the current state of the system. The model of a linear system essentially exists in a static time vacuum.

So, why do we use linear systems models at all, if they fail to capture so much of the real phenomena that we see in the world around us? There are a number of reasons why we do. Firstly, linear models are deeply intuitive to us. The static properties of real tangible things that linear systems theory captures is much easier for us to see, touch and quantify as opposed to the intangible world of the relations between these things and over time.

Secondly, as we have noted linear models do capture the behavior of some if not many systems, such as the simple interactions between particles of matter or simple dynamics of cause and effect that we might sometimes see in social and economic behavior.

Lastly, and probably most significantly, linear models are inherently simple, as you may have noticed, they also remove any qualitative questions surrounding the nature of the relations between elements in the system. This makes them particularly amenable to the rigorous quantitative methods of mathematics and the reductionist approach to science, where we can approach complex problems by breaking them down into their constituent parts and then tack these simpler problems in isolation.

**Nonlinear Systems: Overview**

Although it is often said that nonlinear systems describe the vast majority of phenomena in our world, they have unfortunately been designated as alternatives, being defined by what they are not, it might be of value to start our discussion by asking why is this so? The real world we live in is inherently complex and nonlinear, but from a scientific perspective all we have is our models to try and understand it, these models have inevitably started simple and developed to become more complex and sophisticated representations. When we say simple in this case, we mean things that are the product of direct cause and effect interactions, with these simple interactions we can draw a direct line between cause and effect and thus defined a linear relation.

For centuries, science and mathematics have been focused upon these simple linear interactions and orderly geometric forms, that can be describe in beautifully compact equations, not so much because this is how the world is, but more because they are by far the easiest phenomena for us to encode in our language of mathematics and science. It is only in the past few decades that scientists have begun to approach the world of systems that are not linear, thus their late arrival on the scene and our lack of understanding of what they really are has lumped them with being defined by what they are not.

In the previous section, we discussed the key characteristics of linear systems, what is called the superposition principles. We can then defined nonlinear systems as those the defy the superposition principles, meaning with nonlinear phenomena the principles of homogeneity and additively break down. But let’s take a closer look at why this is so.

Starting with additively, as we have already discussed additively states that when we put two or more components together, the resulting combined system will be nothing more than a simple addition of each component’s properties in isolation. The additively principle, as attractively simple as it is, breaks down in nonlinear systems, because the way we put things together and the type of things we put together effect the interactions that make the overall product of the components combination more or less than a simple additive function and thus defies our additively principle and we call it nonlinear.

There are many examples of this such as putting two creatures together, depending which type of creatures we choose, we will get qualitatively different types of interaction between them, that may well make the combination non-additive. Bees and flowers create synergistic interactions or lions and deer interacting through relations of predator and prey, both of these represent either super or sub-linear interactions.

Of course, this is all very intuitive to us, learning about these nonlinear interactions between things is all part of growing up and learning to have a normal common sense, but the problem is actually formulating this in the language of science and mathematics. Whereas we can easily and rigorously study the properties of elements in isolation by taking them into a laboratory or some other isolated environment, it is more difficult though for us to know why or when or if elements will have some special interaction and not only this but these interactions often create novel and surprising new phenomena through the process of emergence.

Imagine you have to play the role of a match maker between two people you know, you may know very well what they are like separate but it would be a lot more difficult for you to tell if they would hit it off when you introduce them to each other. To take another example, who could have imagined that when we put hydrogen and oxygen together we would get water and not only that but because of the weak hydrogen bonds between them that this new substance would, in fact, have the property of wetness. Hopefully, these illustrations will be sufficed to make clear how nonlinearity arises from the non-additive nature to the interactions between things when we combine them.

Next the principle of homogeneity, that if we remember, essentially states that the output to the system is always directly proportional to the input, twice as much into the system, twice as much out, four times as much in four times as much out and so on. The direct implication of this homogeneity principle is that things scale in a linear fashion, which clearly fails to account for the effect that the output of the previous state of the system will have on its current or future state. Put simply our linear model does not deal with feedback loops, inputs and outputs simply appear and disappear without any relation between them.

The homogeneity principle may often work as an approximation, but the underlining fact is that as soon as we put our system into the real world, that is to say into an environment where it operates within both space and time, there will inevitably be feedback loops, as the actions it takes effect its environment with those effects in turn feeding back to effect the future state to the system. This means as soon as we start to deal with the real world, things start to get nonlinear and the more interactions we incorporate into our models, thus making them more robust and realistic, the more nonlinear things are likely to become.

Of course the immediate analogy that springs to mind of this is the so-called “limits to growth model.” Within the industrial age paradigm that was heavily influenced by linear systems thinking, there was or still is the belief in continuous progress without regard for the effect that the current actions of the system will have on its natural environment and how these will feedback to effect the future input variables to the system. But as soon as we begin to conceive of this economic system within its environment, we quickly come to the conclusion that this infinite scaling is not possible because the environmental effect will inevitably feedback to constrain the future state of the system at a certain limit to growth, thus we can see how the homogeneity principle breaks down and we get nonlinear behavior.

In this section we have illustrated how and why the super superposition principles’ breakdown and nonlinearity arises whenever we take into account the nature of the interactions within a system, both between constituent components and over time through feedback loops.

**Synergies & Interference**

As we have previously discussed the nature of the interactions between components within a system is a key source of nonlinearity, in this short section we are going to give a quick overview to the two fundamentally different types of relations that result in non-additive combinations, what we call synergies and interference. Starting with synergies, the term synergy comes from a Greek word that means "working together". A synergy is a positive interaction between two elements derived from some synchronization between their states.

As an example, we could cite the division of labor within many insect and human communities, such as ant colonies and market economies where synergistic relations create a net result that is greater than the product of the individual elements actions in isolation. Very simple ants can through the division of their labor and collaboration create ant colonies that appear to far exceed the capabilities of simply summing up the capability of each ant in isolation.

From this we should note that in order to achieve synergies, components need to be both different and synchronized, if all the ants in our ant colony or the people in a business performed exactly the same function then the result would simply be additive, or if they all performed different functions but did not coordinate their behavior then again we would be dealing with an additive linear system. It is only when we get differentiation that is when components become different not in some random fashion but in a specific way with respect to each other and they also coordinate their activities, then we get synergies and the system become nonlinear.

To illustrate this further, say person A alone is too short to reach an apple on a tree and person B is too short as well. Once person B sits on the shoulders of person A, they are tall enough to reach the apple. The point here is that they had to both differentiate their activities with respect to each other and then coordinate them again in order to achieve this synergy, with both on the ground and they got nothing, but when one stood on the ground and the other on his shoulders then something different happened. This phenomenon of synergy is ubiquitous being encountered throughout the natural, social and engineered world.

Whereas synergistic relations are constructive relations we can of course also have destructive relations between components, which we might call interference. Destructive relations result in a combined system that is less than the sum of its constituent components in isolation. An example of this could be the interference between two wave functions, where they cancel each other out due to their asynchronous interaction.

Synergistic relations occur through differentiation and synchronization, interference often involves a decisive lack of differentiation between components within the same environment, resulting in them trying to all access or occupy a single state with the inevitable result being destructive relations of competition and crowding out. We might think about rush hour traffic jams here, many people trying to access the same resource at the same time, with every new component added to the system resulting in an increased overall loss for everyone else, due to a lack of differentiation between their activities.

Synergies arise from feedback loops between components during their development, enabling them to adapt and synchronize their behavior with other components within the environment. Destructive relations, in contrast, involve the lack of feedback between components meaning they can’t or don’t synchronize their behavior and the overall system remains sub-linear.

For example, the human body as an entirety is a complex system that emerges out of differentiation and synchronization on both the cellular level and the level of the individual organs as they grow and are regulated through a network of feedback loops. The disease of cancer then represent a set of cells which have broken free from these feedback control mechanisms that the body exerts on all cell to regulate their proliferation, these cells grow into a malignant tumor that is then in a destructive relation with all other elements in the system.

In this section, we have discussed the two fundamentally different types of relations that give rise to super or sub-linear phenomena, either due to the positive effects that emerge out of synergies or due to the deleterious effects that result from interference.

**Feedback loops & Equilibrium**

If we were to draw a model of a linear system it would look something like this, there would be an input to the system, some process and an output, as we can see the input and the output to the system are independent from each other. The value that we input to the system now, is not in any way affected by the previous output. There are of cause phenomena where this holds true, such as the flipping of a coin, the value I will get from flipping a coin now will not be dependent in any way on the value I got the last time I flipped it, in mathematics, this is called the markov property.

But the fact is that many things in our world don’t behave like this, meaning that the current input variables to the system are dependent on previous outputs and current outputs will affect future inputs. The state of the weather yesterday will effect the state of the weather today, the amount of money I have in my account today will through interest effect the amount I have tomorrow and so on. This phenomenon where the output of a system is "fed back" to become inputs as part of a chain of cause-and-effect that forms a circuit or loop is called a feedback loop.

A feedback loop could be defined as a channel or pathway formed by an 'effect' returning to its 'cause,' and generating either more or less of the same effect. An example of this might be a dialogue between two people, what you say now will effect what the other person will say and that will in turn feedback as the input to what you will say in the future. A full discussion of the dynamics of feedback loops is beyond the scope of this section; our aim here is to merely touch on how they affect the behavior of a system with respect to nonlinearity.

Feedback loops are divided into two qualitatively different types, what are called positive and negative feedback. A negative feedback loop represents a relationship of constraint and balance between two or more variables, when one variable in the system changes in a positive direction the other changes in the opposite, negative direction, thus always working to maintain the original overall combined value to the system.

For example, the feedback loops that regulate the temperature of the human body. Different body organs work to maintain a constant temperature within the body by either conserving or releasing more heat, through sweating and capillary dilation they counter balance the fluctuations in the external environment’s temperature. Another example of negative feedback loop might be between the supply and demand of a product. The more demand there is for a product the more the price may go up which will in turn feedback to reduce the demand.

We can note the direct additive relationship here when one component goes up the other goes down in a somewhat proportional fashion with the end result being a linear system that tends toward equilibrium. We haven’t yet had the chance to discuss the significance of equilibrium but it plays a very important role in linear systems theory. When we have these additive negative feedback loops the net result is a zero sum game, the total gains and losses combined are zero and we can then define this as the system’s equilibrium or “normal” state, with our models then being built on this assumption of there being an equilibrium. This concept of equilibrium holds well for isolated systems and systems in negative feedback loops, but as we will see this assumption about there being an equilibrium point breaks down in nonlinear systems and thus we describe them a being “far-from-equilibrium”.

A positive feedback in contrast to negative feedback is a self-reinforcing process, the increase in the values associated with one element in the relation are correlated with an increase in the values associated with another, in other words both elements either grow or decay together. Examples of this are numerous, such as compound interest where last years increases result in an increase in this year's input, or chain reactions such as cattle stamps are another example. But the result is always a self reinforcing process that leads to exponential outcomes of growth or decay. The total gains and loses are non-additive and do not sum up to zero, thus there is no equilibrium and these nonlinear systems are said to exist far-from-equilibrium.

These positive feedback loops are of course unsustainable requiring the input of energy from their environment, the exponential growth in human industrial activity over the past few centuries could be sited here. The more developed our industrial technologies are, the better we are able to process and access petroleum which again feeds back to result in more energy and more industrial development and so on, but the point is that this is all the product of some input of energy from the system’s environment that will eventually reach some limit.

Positive and negative feedback loops are key to the dynamics of nonlinear systems and we will be discussing them further later on in the book, but for the moment we will summarize with a few key take away points. That as soon as we put our system into its environment its output will in some way effect that environment and this will in turn effect the future input to the system through what is called a time delay feedback loop. If the new input produces a result in the opposite direction to previous results, then it is a negative feedback and their effects will stabilize the system towards some equilibrium point. If these new inputs facilitate and accelerate the development of the system in the same direction as the preceding results, they are positive feedback resulting in nonlinear exponential growth or decay. Lastly, whereas negative feedback will lead to the system converging around some equilibrium state. Positive feedback leads to divergent behavior as it rapidly moves away from the equilibrium and we describe them as being “far-from-equilibrium”.

**Exponentials & Power Laws**

Part of our definition for linear systems is that the relationship between input and output is related in a linear fashion, the ratio of input to output might be 2 times as much, 10 times as much or even a thousand times it is not important this is merely a quantitative difference, what is important is that this ratio between input and output is itself not increasing or decreasing. But, as we have seen, when we have feedback loops the previous state to the system feeds back to effect the current state thus enabling the current ratio between input and output to be greater or less than its ratio previously and this is qualitatively different.

This phenomenon is captured within mathematical notation as an exponential. The exponential symbol describes how we take a variable and we times it by another not just once but we, in fact, iterate on this process, meaning we take that output and feed it back in to compute the next output, thus the amount we are increasing by each time itself increases. So let’s take a quick example of exponential grow to give you an intuition for it.

Say I wish to create a test tube of bacteria. Knowing that the bacteria will double every second, I start out in the morning with only two bacteria hoping to have my tube full by noon. As we know the bacteria will grow exponentially as the population at any time will feed into effect the population at the next second, like a snowball rolling down a hill. It will take a number of hours before our tube is just 3% full but within the next five seconds as it approaches noon it will increase to 100% percent of the tube.

This type of disproportional change with respect to time, is very much counter to our intuition where we often create a vision of the future as a linear progression of the present and past, we will be discussing farther the implication of this type of grow later when we get into the dynamics of nonlinear systems but for the moment the important thing to note here is that in exponential growth the rate of growth itself is growing and this only happens in nonlinear systems, where they can both grow and decay at an exponential rate.

Exponentials are also called powers and the term power law describes a functional relationship between two quantities, where one quantity varies as a power of another. There are lots of example of the power law in action but maybe the simplest is the scaling relationship of an object like a cube, a cube with sides of length $a$ will have a volume of $a^3$ and thus the actual number that we are multiplying the volume by grows each time, this would not be the case if there was a simple linear scaling such as the volume being 2 times the side length.

Another example from biology is the nonlinear scaling within the metabolic rate vs. size of mammals. The metabolic rate is basically how much energy one needs per day to stay alive and it scales relative to the animal's mass in a sub-linear fashion, if you double the size of the organism then you actually only need 75% more energy. One last example of the power law will help to illustrate how it is the relationships between components within a system that is a key source of this nonlinearity.

The so-called Metcalfe's Law comes from the world of I.T. and it derives from the simple observation that every time we add a new computer to the network we have the possibility of adding as many more links as there are computers in the network. As each person who joins the network makes it more valuable, Metcalfe’s law leads to the value or power of a network increasing in proportion to the square of the number of nodes on the network, this is of cause not restricted to just computer networks but is a feature of all networks and thus is given the more general name of the network effect. The network effect is a key driver of positive feedback as every time someone links to a particular node on a network it makes it that bit more likely someone else will also. This example helps to illustrate the dynamics behind positive feedback and how through these positive feedback loops the system can move or develop in a particular direction very rapidly.

Many-real world networks such as the World Wide Web have proven to have this power law relationship between the size and quantity, where there are just a very few sites with a very large size and very many of a very small size. We should note here again that with the network effect as with all nonlinear systems, things can go both ways, it may have helped to grow the Internet to its vast size in a very short period of time which we might site as a positive thing, but also the network effect is in operation when some negative news about your company goes viral and behind the creation of herd mentalities.

The key take away point from this section on exponentials is to get a sense of the qualitatively different nature of growth within linear and nonlinear systems. Exponential growth means that the system is not just growing or decreasing but that due to the feedback loops and synergies within the system and over time there is also another meta level to the systems development that is itself increasing this rate of growth or decay to enable very rapid change.

**Long Tail Distributions**

In this section, we will be continuing with our discussion on power laws from the slightly different perspective of probability. One product of this exponential power law feature to nonlinear systems is that they are non-normal, meaning there is statistically no normal average or typical state within the system, to understand this we need to first talk a bit about what we mean by average and normal.

When we say "normal" we mean that there is some kind of mean state to the system, given enough samples of the different states within the system we will be able to compute some average that we can use as a representative of the whole system. For example, the distribution of people's heights, if we were to plot them, would follow what is called a normal distribution meaning there will be very many people around the average of say 5 to 6 feet, some a bit larger and some a bit smaller than this, say between 3 and 8 feet, but virtually no one outside of this range of states. These normal distributions then have a well-defined center and then drop off exponentially fast meaning there is an extraordinary low probability of getting extreme states.

Another feature to normal distributions is the so-called “law of large numbers” which means that the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed. This so-called “law” is important because it "guarantees" stable long-term results for the averages of some random events. For example, while a casino may lose money in a single spin of the roulette wheel, its earnings will tend towards a predictable percentage over a large number of spins. Any winning streak by a player will eventually be overcome by the law of large numbers that will bring the net gains and losses back to an average.

The normal distribution holds for many linear systems, physical and chemical but in the world of nonlinearity the idea that there is such a thing as normal and we should expect this normal is no longer applicable. The power law nature to nonlinear systems creates what is called a long tail distribution meaning extraordinary events that are virtually impossible within a normal distribution are possible within nonlinear systems, these extraordinary events are referred to as “black swans”.

For example, on October 19th, 1987 on Black Monday the Dow Jones stock market index dropped by 22% in a single day, compared to the typical fluctuation of less than 1% this was a shift in more than 20 standard deviations from the norm. Within a normal distribution, this would be virtually impossible, with something like a 10 to the power of -50 chance of it happening.

Although in normal distributions extraordinary events like this are virtually impossible they are much more common in power law distribution resulting in a very different graph that has a long tail sometimes called fat-tail. These extraordinary events can then have a dramatic effect on the systems average behavior, essentially rendering this concept of average or normal nonsensical, because if we take a random sample we will get a certain average but then if we add just one more node to this it might be a black swan that will radically alter the average again.

So, if we go back to our example of measuring people’s height, if we have a room of people and then the tallest person in the world walks in the average height will only change by a few feet, but say we are measuring people’s income and now the richest person in the world walked in with an income of many billions this would so radically alter the average income for it to become nonsensical.

In a power law distribution as we increase the number of samples we take, values will not converge to an average they will in fact diver, with some exceptions. Asking for an average is like asking how big is a stone or how long is an average piece of string? Within probability, power laws describe an exponential relation between the size of an event and the frequency of it occurring. Many types of systems follow this power law distribution resulting in a long tail graph, from traffic on the Internet to the occurrence of earthquakes and stock market crashes. This power law makes extreme events much more likely and renders our traditional conception of average and normal no longer applicable.

Again, this ties back to our main theme of nonlinearity being a product of the nature of interactions between components and overtime, our normal distribution is largely derived from the fact that we are taking random samples from components that have no correlation between them. If I flip a coin now it will not effect what I get on the next flip and will follow a normal distribution. If you win on one casino table it will not effect whether I will win on another and so on. But in these nonlinear systems, things are arranged in a particular way, large websites are large because of the network effect and because people have specifically chosen to connect to them there is nothing random about this, financial crashes are also similar in nature. In a few sections from now, we will be coming back to illustrate how power laws are in fact closely associated with fractal geometry.

**Dynamical Systems Overview**

Within science and mathematics, dynamics is the study of how things change with respect to time, as opposed to describing things simply in terms of their static properties the patterns we observe all around us in how the state of things change over time is an alternative ways through which we can describe the phenomena we see in our world.

A state space—also called phase space—is a model used within dynamic systems to capture this change in a system’s state over time. A state space of a dynamical system is a two or possibly three-dimensional graph in which all possible states of a system are represented, with each possible state of the system corresponding to one unique point in the state space. Now we can model the change in a system’s state in two ways, as continuous or discrete.

Firstly, as continues where the time interval between our measurements is negligibly small making it appear as one long continuum and this is done through the language of calculus. Calculus and differential equations have formed a key part of the language of modern science since the days of Newton and Leibniz. Differential equations are great for few elements they give us lots of information but they also become very complicated very quickly.

On the other hand, we can measure time as discrete meaning there is a discernible time interval between each measurement and we use what are called iterative maps to do this. Iterative maps give us less information but are much simpler and better suited to dealing with very many entities, where feedback is important. Whereas differential equations are central to modern science iterative maps are central to the study of nonlinear systems and their dynamics as they allow us to take the output to the previous state of the system and feed it back into the next iteration, thus making them well designed to capture the feedback characteristic of nonlinear systems.

The first type of motion we might encounter is simple transient motion, that is to say some system that gravitates towards a stable equilibrium and then stays there, such as putting a ball in a bowl it will roll around for a short period before it settles at the point of least potential gravity, its so-called equilibrium and then will just stay there until perturbed by some external force.

Next, we might see periodic motion, for example the motion of the planets around the sun is periodic. This type of periodic motion is of cause very predictable we can predict far out into the future and way back into the past when eclipses happen. In these systems small disturbances are often rectified and do not increase to alter the system's trajectory very much in the long run. The rising and receding motion of the tides or the change in traffic lights are also examples of periodic motion. Whereas in our first type of motion the system simply moves towards its equilibrium point, in this second periodic motion it is more like it is cycling around some equilibrium.

All dynamic systems require some input of energy to drive them, in physics, they are referred to as dissipative systems as they are constantly dissipating the energy being inputted to the system in the form of motion or change. A system in this periodic motion is bound to its source of energy and its trajectory follows some periodic motion around it or towards and away from it. In our example of the planet’s orbit, it is following a periodic motion because of the gravitational force the sun exerts on it, if it were not for this driving force, the motion would cease to exist.

Another example would be the human body that requires the input of food on a periodic basis. We consume food then dissipate it through some activity and then consume more and dissipate it again in a somewhat periodic fashion, like other biological systems we are bound to cycle through this set of states, the same it true for our car or a business that are constrained by the inputs of fuel or finance. The dissipation and the driving force tend to balance, settling the system into its typical behavior; this typical set of states the system follows around its point of equilibrium is called an attractor.

In the field of dynamical systems, an attractor is a set of values or states toward which a system tends to evolve for a wide variety of starting conditions to the system. System values that get close enough to the attractor remain close even if slightly disturbed. There are many examples of attractors such as the use of addictive substances, while being subject to the addiction our body cycles in and out of its physiological influence but continuously comes back to it in a somewhat periodic and predictable fashion, which is until it is able to break free from it.

A so-called basin of attraction then, describes all the points within our state space that will move towards a particular attractor. So we could think of a planet’s gravitational field as a basin of attraction, if we place some matter that is large enough into the gravitational field, it will be drawn into its orbit irrespective of its starting condition.

In this section we have started our discussion on the dynamics of nonlinear systems by talking about the two simplest forms of motion and change, firstly those that simply gravity towards some equilibrium and then remain there, such as a ball in a bowl that is determined to find the point of lowest potential gravity and then remain in this equilibrium until perturbed. We then discussed a second type of change, periodic motion, a very common type of motion that followed a regular periodic pattern in state space. Under this regime the system receives some input of energy and then dissipates this to maintain a periodic state of change around its source of energy, in this way being constrained within a so-called attractor. As you may have noticed we have been dealing with relatively simple systems with a single point equilibrium characteristic of linear systems. In the coming sections we will be discussing the dynamics of nonlinear systems that may have multiple equilibria.

**Nonlinear Dynamics & Chaos**

Isolated systems tend to evolve towards a single equilibrium, a special state that has been the focus of many-body research for centuries. But when we look around us we don’t see simple periodic patterns everywhere, the world is a bit more complex than this and behind this complexity is the fact that the dynamics of a system may be the product of multiple different interacting forces, have multiple attractor states and be able to change between different attractors over time.

Before we get into the theory let’s take a few examples to try and illustrate the nature of nonlinear dynamic systems. A classical example given of this is a double pendulum; a simple pendulum without a joint will follow the periodic and deterministic motion characteristic of linear systems with a single equilibrium that we discussed in the previous section. Now if we take this pendulum and put a joint in the middle of its arm so that it has two limbs instead of one, now the dynamical state of the system will be a product of these two parts interaction over time and we will get a nonlinear dynamic system.

To take a second example: in the previous section we looked at the dynamics of a planet orbiting another in a state of single equilibrium and attractor, but what would happen if we added another planet into this equation, physicists puzzled over this for a long time, we now have two equilibrium points creating a nonlinear dynamic system as our planet would be under the influence of two different gravitational fields of attraction.

Whereas with our simple periodic motion it was not important where the system started out, there was only one basin of attraction and it would simply gravitate towards this equilibrium point and then continue in a periodic fashion. But when we have multiple interacting parts and basins of attraction, small changes in the initial state to the system can lead to very different long-term trajectories and this is what is called chaos.

Wikipedia has a good definition for chaos theory so let’s take a quote from it. “Chaos theory studies the behavior of dynamical systems that are highly sensitive to initial conditions—a response popularly referred to as the butterfly effect. Small differences in initial conditions yield widely diverging outcomes for such dynamical systems, rendering long-term prediction impossible in general”.

We should note that chaos theory really deals with deterministic systems and moreover it is primarily focused on simple systems, in that it often deals with systems that have only a very few elements, as opposed to complex systems where we have very many components that are non-deterministic. In these complex systems, we would of cause expect all sorts of random, complex and chaotic behavior, but it is not something we would expect in simple deterministic systems.

This chaotic and unpredictable behavior happens even though these systems are deterministic, meaning that their future behavior is fully determined by their initial conditions, with no random elements involved. In other words, the deterministic nature of these systems does not make them predictable, which is deeply counter-intuitive to us. A double pendulum essentially consists of only two interacting components, that is, each limb and these limbs are both strictly deterministic when taken in isolation, but when we join them this very simple system can and does exhibit nonlinear and chaotic behavior. This once again reveals to us the source of nonlinearity as the product of the interactions between components within the system. In these chaotic systems chance and their nondeterministic nature emerges out of the interaction between the components.

Although chaos theory deals with simple nonlinear systems the phenomena of sensitivity to initial conditions is also a part of complex systems as we might expect. For example: Say I am walking to the subway station on my way to work but as I pass the bus stop I notice a bus just pulling in that I recognize as one that will take me near to where I want to go, so I jump on the bus and it takes me off into a different basin of attraction than if I had arrived just 30 seconds later.

In complex systems, this sensitivity to initial conditions can be very acute during particular stages in their development, what are called phase transitions when they are far-from-equilibrium. Small fluctuations can push them into new basins of attraction, we will be covering phase transitions in a later section but in the next section we will continue our discussion on chaos by talking about the butterfly effect.

**The Butterfly Effect**

Science is dependent upon isolating a system so as to study it, but this is literally impossible, everything has an effect on everything else in our universe. Every single particle of matter has some gravitation effect on every other particle of matter, irrespective of how much we try to isolate something the concept of an isolated system that has zero interaction with its environment is really just a theoretical one that does not exist in reality. This simple insight literally destroys the entire scientific enterprise. Science is dependent upon this capacity to isolate systems, because if we want to say A causes B then we need to be able to control for all other variables, that is remove them from the equation. As we have stated this is not possible. So how do we get around this stumbling block? As it appears the scientific endeavor goes on without this causing too much of a problem.

What we do, because we can’t fully isolate any system, is essentially define what significant effects are, what negligible effect are, and simply forget about the negligible effects. For example, if I am doing some research in my lab on the interaction of two particles of matter. Under this premise I do not need to take into account the gravitation effect that some planet the other side of the universe is having on these particles because it is deemed negligible.

This basic premise that small causes can only cause relatively small effects is one of the basic assumptions and principles that gives our world some order, we depend upon it almost all day every day. I feel confident in the fact that if I forget to pay my bank overdraft this week it is not going to bring the whole global economy into meltdown, or that a teeny little pin prick is not going to kill me. We find order in the world in the fact that the chances of these phenomena happening are so small, that they are negligible and we can thus forget about them, without this being the case within linear systems our world would be extraordinary random and chaotic.

But as we have previously discussed, nonlinear systems through feedback loops, can grow exponentially. This means that negligible effects or differences, within nonlinear systems can themselves grow in an exponential fashion where small effects and errors are fed back into the system at each stage of its development to compound the size of the errors as it grows in an exponential fashion.

As was the case in the famous Edward Lorenz computer experiment, where when he fed values into the computer that he thought were exactly the same, the output results the computer give him were widely divergent. He eventually traced this back to small differences in rounding errors, that made the values only very slightly different. However, through iteration these very small errors would grow not in a linear fashion but exponentially making the resulting output widely divergent within a relatively short period of time and thus giving us the phenomenon that is called sensitivity to initial conditions.

Sensitivity to initial conditions is popularly known as the "butterfly effect", thought to be so called because of the title of a talk given by Edward Lorenz in 1972 called *Does the Flap of a Butterfly’s Wings in Brazil set off a Tornado in Texas?* The flapping wing represents a small change in the initial condition of the system, which causes a chain of events leading to some large-scale phenomena. Had the butterfly not flapped its wings, the future trajectory of the system might have been vastly different. Something to note is that the butterfly does not directly cause the tornado, this is of cause impossible, the flap of the butterfly’s wings simply defines some initial condition; it is then the set of chain reactions through feedback loops that enable a small change in the initial conditions of the system to have a significant effect on its output rendering long-term predictions virtually impossible.

With respect to the unpredictable nature of the butterfly effect you might say, well if our initial measurement is wrong then obviously our prediction of its future state is also going to be wrong! But this is missing the point, which is firstly that this inaccuracy is growing exponentially as the system develops it is not just staying the same and secondly that in these nonlinear systems we can never know exactly the starting condition as our accuracy of measurement must grow in an exponential fashion in order to achieve just a linear growth in our horizon of predictability.

Chaos and the butterfly effect after being shunned by the scientific community for many decades are today accepted as scientific facts. A fundamental and inescapable part of the dynamics to nonlinear systems, they show again how when things can grow exponentially we can get extraordinary and counter-intuitive results.

**Bifurcations & Phase Transitions**

As we have previously discussed, the qualitative dynamic behavior of nonlinear systems is largely defined by the positive and negative feedback loops that regulate their development, with negative feedback working to dampen down or constrain change to a linear progression, while positive feedback works to amplify change typically in a super-linear fashion. As opposed to negative feedback where we get a gradual and often stable development over a prolonged period of time, what we might call a normal or equilibrium state of development, positive feedback is characteristic of a system in a state of non-equilibrium. Positive feedback development is fundamentally unsustainable because all systems, in reality, exist in an environment that will ultimately place a limit on this grown.

From this we can see how the exponential grow enabled by positive feedback loops is what we might say special, it can only exist for a relatively brief period of time. When we look around us, we see the vast majority of things are in a stable configuration constrained by some negative feedback loop whether this is the law of gravity, predator-prey dynamics or the economic laws of having to get out of bed and go to work every day. These special periods of positive feedback development are characteristic and a key driver of what we call phase transitions.

A phase transition may be defined as some smooth, small change in a quantitative input variable that results in a qualitative change in the system’s state. The transition of ice to steam is one example of a phase transition. At some critical temperature a small change in the systems input temperature value results in a systemic change in the substance after which it is governed by a new set of parameters and properties. For example, we can talk about cracking ice but not water, or we can talk about the viscosity of a liquid but not a gas as these are in different phases under different physical regimes and thus we describe them with respect to different parameters.

Another example of a phase transition we could cite is the changes within a colony of bacteria that occur when we change the heat and nutrient input to the system. When we do this we change the local interactions between the bacteria and get a new emergent structure to the colony, although this change in input value may only be a linear progression it resulted in a qualitatively different pattern emerging on the macro level of the colony. It is not simply that a new order or structure has emerged but the actual rules that govern the system change and thus we use the word regime and talk about it as a regime shift, as some small changes in a parameter that effected the system on the local level leads to different emergent structures that then feedback to define a different regime that the elements now have to operate under.

Another way of talking about this is in the language of bifurcation theory, whereas with phase transitions we are talking about qualitative changes in the properties of the system, bifurcation theory really talks about how a small change in a parameter can cause a topological change in a system’s environment resulting in new attractor states emerging. A bifurcation means a branching, in this case we are talking about a point where the future trajectory of an element in the system divides or branches out, as new attractor states emerge, from this critical point it can go in two different trajectories which are the product of these attractors, each branch represents a trajectory into a new basin of attraction with a new regime and equilibrium.

Let’s take a real-world example of a bifurcation: say you have been studying Fine Art as an undergraduate, this subject has for the past few years represented your basin of attraction, that is to say, your studies have cycled through its many different domains but never moved off into another totally different subject. But now that you have graduated you have the option to continue your studies in either sculpture or painting, you have now reached a bifurcation point as two new attractors have opened up in front of you, some small event at this point could define your long-term trajectory into one of these two different basins of attraction.

Lastly, as opposed to linear systems that may develop in an overall incremental fashion, the exponential grow that nonlinear systems are capable of through feedback loops and phase transitions leads to a different overall pattern to their development what we might call punctuate equilibrium. Within this model of punctuated equilibrium the development of a nonlinear system is marked by a dynamics between positive and negative feedback, with negative feedback holding the system within a basin of attraction that represent periods of stable development. These stable periods are then punctuated by periods of positive feedback which takes the system far from its equilibrium and into a phase transitions as the fundamental topology of its attractor states change and bifurcate.

Examples of this punctuated equilibrium might be the development of economies that go through periods of stable growth then rapid change through economic crisis and recovery, or ecosystems as they collapse due to some environmental change and then an ensuing period of rapid re-grow towards a new equilibrium of stable development again. The same punctuated development may be seen within the development of a human been as they go from childhood to adulthood to old age, each period representing a stable basin of attraction with changes between each being marked by periods of rapid and defining change.

To sum it up, we have discussed phase transitions and bifurcations as periods of qualitative and often rapid change in the dynamics of a nonlinear system’s state. We have tried to present an intergrade picture of this that incorporates our previous discussions on feedback loops to show how nonlinear systems often don’t develop in an incremental fashion but their development is marked by a model of punctuated equilibrium with contrasting periods of stability and rapid growth.

**Fractals**

Fractals are sometimes called the pictures of nature and if chaos theory, as the name implies, is the chaotic and unpredictable dimension to nonlinear systems then we might say fractals represent their orderly side. We see lots of order in our world, the earth goes round the sun today and it will do the same tomorrow and the next day for millions of years, if we take a butterfly we see that one side of it is almost exactly mirrored in the other, we see the same in the regular geometric forms of snowflakes. One way of understanding this order, is though, the concept of symmetry.

In mathematics, "symmetry" can be defined as an object or process that is invariant to a transformation, in more familiar terms; this means something that stays the same despite a change. So with our example of the butterfly, its external physiology had a reflection symmetry meaning we can simply flip one side over and we will get the other, the same was true of our snowflake and we could imagine some symmetry in the pattern of earth’s orbit.

Without these symmetries to the world the scientific endeavor would be very difficult, because science is about the creation of compact representations of the world, it is like we are creating maps of the world and it is only through finding these symmetries and encoding them in models that we can describe a wide variety of phenomena with simple equations, without these symmetries our scientific map of the world would have to be the same size as the world itself and thus useless. Symmetry and asymmetry are then two of the most powerful concepts in mathematics and science for talking about order and chaos; they help us to understand these abstract concepts in a distinctly geometric and visual form.

So with the phenomenon of chaos, we saw this breaking of symmetry two things that started out similar became increasing dissimilar the symmetry between them became broken and the result was after a short period of time complete asymmetry. Fractals have what is called scale invariance, that is they have a symmetry with respect to scale, meaning the scale can change but the structure will repeat itself over various levels of magnitude.

This scale invariance is also called self-similarity and it is this type of symmetry under magnification, which gives fractals an amazing type of structure and order. Fractals are both mathematical constructs that derive out of iterative functions and real world phenomena, in this section we will present them as mathematical models and then in the next look at their real world counterparts.

As we have seen, with most of nonlinear systems the core ideas behind fractals is of feedback and iteration. The creation of most fractals involves applying some simple rule to a set of geometric shapes or numbers and then repeating the process on the result. For example, the most famous fractal called the Mandelbrot set, so named after the discovery of the concept of fractals, is a product of a simple iterative map on complex numbers. We will not go into the details of complex numbers but the iterative map itself is quite simple. There are many extraordinary things about fractals but the first thing we will note is the infinite variety that these simple iterative functions can produce.

Whereas most Euclidian shapes—what we might call normal shapes—tend towards a bland featurelessness as we scale them up. If we take something like a circle the more we zoom in on it the more it will start to appear like a bland straight line and this would also be the case for other regular shapes such as squares, triangles and so on. The iterative functions behind fractals, in contrast, give us an infinite amount of structure and detail within their form, irrespective of if we divide the shape into two or divide it into a million each of these million little parts will itself have an infinite amount of detail and a form that resembles the whole.

Just as there can exist an infinite amount of form in a finite object there can with fractals also exist an infinite length within any finite length. This is best demonstrated by a fractal called the Koch curve, that can be obtained by iterating a simple process of dividing a line and placing a triangle in its center and then iteration on this to divide the lines on the triangle in a similar fashion. What we get when we do this is a path that will, in fact, have an infinite length to it, if we were to try and trace a path along this line its infinite detail would prevent us from ever reaching the end thus this finite length contains an infinite length within it.

The last thing we will note about fractals is that their scale-free property means that no scale is the “proper” frame of reference. Most linear systems that represent regular forms have features and structures within a limited range of scales thus if we plot them we get a normal distribution, with the majority of features tending towards a mean, giving it some kind of “normal” frame of reference with respect to scale.

These nonlinear fractals as we have noted, have scale invariance meaning we will find features on all levels, resulting in there being no normal distribution to their occurrence and thus no normal frame of reference. The occurrence of features is instead distributed out as a power law, big features exist but are very rare whilst many small features also exist and this long tail does not drop off because it is a fractal it will go on infinitely. Thus we can say that the power laws we encountered in an earlier section are the algebraic expression of the scale free structure to fractals. And just as there is no “normal” scale in fractals there is no normal average to power law distributions.

In this section, we have begun our exploration into the amazing world of fractals that has once again exhibited the extraordinary results we can get from simple nonlinear feedback loops, this time in their capacity to produce structures that have a scale invariant property, giving them the very counter-intuitive capacity to contain an infinite level of variety and length within a finite form.

**Conclusion**

This book has been a very high-level overview of the extraordinary world of nonlinear systems, a set of ideas that shakes our most deeply held assumptions about how the world around us works. In deed, we might note at this stage how deeply intuitive the ideas of linear systems theory are to us, and how counter-intuitive nonlinear systems are. In fact, there is a quote from the systems thinker Peter Senge who describes this very succinctly when he says that “the world is made of circles and we think in straight lines.”

Nonlinear systems and their dynamics is often shrouded in dense mathematics and appears very complicated, but as we have tried to emphasize in this book it is important to grasp the basic conceptual origins to nonlinear first before digging into the more technical dimension to the subject or else we can get lost in the trees without being able to see the forest. The main theme of this book has been in trying to show how nonlinearity is really the product of a special kind of interaction between components in space or over time.

With linear systems the components interaction is largely neutral—that is to say additive—when components interact, that interaction does not add or subtract any value to the system, it is simply a summation of the parts. But with nonlinear systems components interact in a specific fashion that makes the combined output greater (synergies) or less (interference) than the sum of the parts giving rise to emergent properties to the whole system.

Equally this type of nonlinear interaction can be over time where what happens now effects what happens in the future as events feedback on themselves creating a compounding effect. This iterative process with nonlinear interactions over time can create a compounding effect giving us exponential growth, the butterfly effect and chaos.